{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Spoofax Language Workbench \u00b6 Spoofax is a platform for developing textual (domain-specific) programming languages. The platform provides the following ingredients: Meta-languages for high-level declarative language definition An interactive environment for developing languages using these meta-languages Code generators that produces parsers, type checkers, compilers, interpreters, and other tools from language definitions Generation of full-featured Eclipse editor plugins from language definitions An API for programmatically combining the components of a language implementation With Spoofax you can focus on the essence of language definition and ignore irrelevant implementation details. Get started by downloading and installing Spoofax or build it from source . Looking for Spoofax 3? Visit the Spoofax 3 documentation website .","title":"Spoofax: The Language Designer's Workbench"},{"location":"#the-spoofax-language-workbench","text":"Spoofax is a platform for developing textual (domain-specific) programming languages. The platform provides the following ingredients: Meta-languages for high-level declarative language definition An interactive environment for developing languages using these meta-languages Code generators that produces parsers, type checkers, compilers, interpreters, and other tools from language definitions Generation of full-featured Eclipse editor plugins from language definitions An API for programmatically combining the components of a language implementation With Spoofax you can focus on the essence of language definition and ignore irrelevant implementation details. Get started by downloading and installing Spoofax or build it from source . Looking for Spoofax 3? Visit the Spoofax 3 documentation website .","title":"The Spoofax Language Workbench"},{"location":"getting-started/","text":"Getting started \u00b6 The quickest way to get started with Spoofax by downloading an instance of Eclipse with the latest release. Alternatively, you can install the Spoofax plugin into an existing Eclipse instance, use Homebrew on macOS, or download and build Spoofax from source. Installation \u00b6 The recommended way to get started with Spoofax is to download an Eclipse instance with the latest Spoofax plugin. The plugin also includes the Spoofax meta-languages. Alternatively, you can install the Spoofax plugin into an existing Eclipse instance, or download and build Spoofax from source. Choose the Eclipse Bundle installation (recommended) or the Eclipse Plugin installation: Eclipse Bundle Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Download Eclipse with Spoofax without an embedded JRE . Development releases . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ Installation instructions . Homebrew ( macOS) On macOS Spoofax can be installed easily using Homebrew . Install the latest release of Spoofax Eclipse as follows: brew tap metaborg/metaborg brew install --cask spoofax The optional command-line tools are installed with: brew install strategoxt Upgrading the Spoofax cask is not recommended Upgrading the Spoofax cask using brew cask upgrade --greedy will lose all manually installed plugins. It is recommended to use Eclipse update sites to keep Spoofax up-to-date. Quick Start \u00b6 Once installed, create a new Spoofax project: Right-click the Package Explorer , choose New \u2192 Project , and select Spoofax Language project from the Spoofax category. Provide a name for your new language and click Finish . Select the created language project and press Ctrl + Alt + B ( Cmd + Alt + B on macOS) to build the project. Create a new file with the extension registered to your language to test it. Follow one of the tutorials to learn more. Finding the filename extension of your language If you didn't explicitly specify a filename extension for your language, it is derived from the language name. You can find the filename extension for your language in editor/Main.esv at the extensions property.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"The quickest way to get started with Spoofax by downloading an instance of Eclipse with the latest release. Alternatively, you can install the Spoofax plugin into an existing Eclipse instance, use Homebrew on macOS, or download and build Spoofax from source.","title":"Getting started"},{"location":"getting-started/#installation","text":"The recommended way to get started with Spoofax is to download an Eclipse instance with the latest Spoofax plugin. The plugin also includes the Spoofax meta-languages. Alternatively, you can install the Spoofax plugin into an existing Eclipse instance, or download and build Spoofax from source. Choose the Eclipse Bundle installation (recommended) or the Eclipse Plugin installation: Eclipse Bundle Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Download Eclipse with Spoofax without an embedded JRE . Development releases . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ Installation instructions . Homebrew ( macOS) On macOS Spoofax can be installed easily using Homebrew . Install the latest release of Spoofax Eclipse as follows: brew tap metaborg/metaborg brew install --cask spoofax The optional command-line tools are installed with: brew install strategoxt Upgrading the Spoofax cask is not recommended Upgrading the Spoofax cask using brew cask upgrade --greedy will lose all manually installed plugins. It is recommended to use Eclipse update sites to keep Spoofax up-to-date.","title":"Installation"},{"location":"getting-started/#quick-start","text":"Once installed, create a new Spoofax project: Right-click the Package Explorer , choose New \u2192 Project , and select Spoofax Language project from the Spoofax category. Provide a name for your new language and click Finish . Select the created language project and press Ctrl + Alt + B ( Cmd + Alt + B on macOS) to build the project. Create a new file with the extension registered to your language to test it. Follow one of the tutorials to learn more. Finding the filename extension of your language If you didn't explicitly specify a filename extension for your language, it is derived from the language name. You can find the filename extension for your language in editor/Main.esv at the extensions property.","title":"Quick Start"},{"location":"background/","text":"Background \u00b6 This section contains information on the ideas, architecture, and design decisions behind Spoofax. For the Spoofax language reference, see the References section. Publications : References to papers about the design and implementation of various aspects of Spoofax Statix : More about the Statix language for Static semantics definition Stratego : Motivation for the design of the Stratego transformation language Documentation : Explanation of how this documentation works","title":"Background"},{"location":"background/#background","text":"This section contains information on the ideas, architecture, and design decisions behind Spoofax. For the Spoofax language reference, see the References section. Publications : References to papers about the design and implementation of various aspects of Spoofax Statix : More about the Statix language for Static semantics definition Stratego : Motivation for the design of the Stratego transformation language Documentation : Explanation of how this documentation works","title":"Background"},{"location":"background/bibliography/","text":"Spoofax Bibliography \u00b6 Spoofax and its meta-languages have been described and motivated extensively in the academic literature. All references to Spoofax-related papers should have been collected in a bibliography on researchr from where the complete collection of bibtex entries can be obtained. (Let us know if we are missing publications in that collection.) This section provides references to that literature. Spoofax \u00b6 The first version of Spoofax was described in an award winning paper OOPSLA 2010 1 . (The paper won the best (student) paper award at OOPSLA 2010 and the Most Influential Paper Award at OOPSLA 2020.) The paper provides motivation for textual language workbenches and discusses the architecture of a language workbench based on declarative meta-languages. An introduction to Spoofax was included along with introductions to MPS and Xtext in V\u00f6lter's DSL Engineering book 2 . A paper in IEEE Software 3 considers Spoofax from a user's perspective. Spoofax was also one of the tools featured in a survey of language workbenches which was first published in SLE'13 4 and later extended in the Computer Languages journal 5 . Bibliographies \u00b6 SDF3 : papers about the syntax definition formalism and its predecessors Statix : papers about the static semantics meta-language and its predecessors Stratego : papers about the transformation meta-language References \u00b6 Lennart C. L. Kats and Eelco Visser. The Spoofax language workbench: rules for declarative specification of languages and IDEs. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 444\u2013463. Reno/Tahoe, Nevada, 2010. ACM. URL: https://doi.org/10.1145/1869459.1869497 , doi:10.1145/1869459.1869497 . \u21a9 Markus V\u00f6lter, Sebastian Benz, Christian Dietrich, Birgit Engelmann, Mats Helander, Lennart C. L. Kats, Eelco Visser, and Guido Wachsmuth. DSL Engineering - Designing, Implementing and Using Domain-Specific Languages . dslbook.org, 2013. ISBN 978-1-4812-1858-0. URL: http://www.dslbook.org . \u21a9 Guido Wachsmuth, Gabri\u00ebl Konat, and Eelco Visser. Language design with the spoofax language workbench. IEEE Software , 31 \\(5\\) :35\u201343, 2014. URL: http://dx.doi.org/10.1109/MS.2014.100 , doi:10.1109/MS.2014.100 . \u21a9 Sebastian Erdweg, Tijs van der Storm, Markus V\u00f6lter, Meinte Boersma, Remi Bosman, William R. Cook, Albert Gerritsen, Angelo Hulshout, Steven Kelly, Alex Loh, Gabri\u00ebl Konat, Pedro J. Molina, Martin Palatnik, Risto Pohjonen, Eugen Schindler, Klemens Schindler, Riccardo Solmi, Vlad A. Vergu, Eelco Visser, Kevin van der Vlist, Guido Wachsmuth, and Jimi van der Woning. The state of the art in language workbenches - conclusions from the language workbench challenge. In Martin Erwig, Richard F. Paige, and Eric Van Wyk, editors, Software Language Engineering - 6 th International Conference, SLE 2013, Indianapolis, IN, USA, October 26-28, 2013. Proceedings , volume 8225 of Lecture Notes in Computer Science, 197\u2013217. Springer, 2013. URL: http://dx.doi.org/10.1007/978-3-319-02654-1_11 , doi:10.1007/978-3-319-02654-1_11 . \u21a9 Sebastian Erdweg, Tijs van der Storm, Markus V\u00f6lter, Laurence Tratt, Remi Bosman, William R. Cook, Albert Gerritsen, Angelo Hulshout, Steven Kelly, Alex Loh, Gabri\u00ebl Konat, Pedro J. Molina, Martin Palatnik, Risto Pohjonen, Eugen Schindler, Klemens Schindler, Riccardo Solmi, Vlad A. Vergu, Eelco Visser, Kevin van der Vlist, Guido Wachsmuth, and Jimi van der Woning. Evaluating and comparing language workbenches: existing results and benchmarks for the future. Computer Languages, Systems & Structures , 44:24\u201347, 2015. URL: http://dx.doi.org/10.1016/j.cl.2015.08.007 , doi:10.1016/j.cl.2015.08.007 . \u21a9","title":"Spoofax Bibliography"},{"location":"background/bibliography/#spoofax-bibliography","text":"Spoofax and its meta-languages have been described and motivated extensively in the academic literature. All references to Spoofax-related papers should have been collected in a bibliography on researchr from where the complete collection of bibtex entries can be obtained. (Let us know if we are missing publications in that collection.) This section provides references to that literature.","title":"Spoofax Bibliography"},{"location":"background/bibliography/#spoofax","text":"The first version of Spoofax was described in an award winning paper OOPSLA 2010 1 . (The paper won the best (student) paper award at OOPSLA 2010 and the Most Influential Paper Award at OOPSLA 2020.) The paper provides motivation for textual language workbenches and discusses the architecture of a language workbench based on declarative meta-languages. An introduction to Spoofax was included along with introductions to MPS and Xtext in V\u00f6lter's DSL Engineering book 2 . A paper in IEEE Software 3 considers Spoofax from a user's perspective. Spoofax was also one of the tools featured in a survey of language workbenches which was first published in SLE'13 4 and later extended in the Computer Languages journal 5 .","title":"Spoofax"},{"location":"background/bibliography/#bibliographies","text":"SDF3 : papers about the syntax definition formalism and its predecessors Statix : papers about the static semantics meta-language and its predecessors Stratego : papers about the transformation meta-language","title":"Bibliographies"},{"location":"background/bibliography/#references","text":"Lennart C. L. Kats and Eelco Visser. The Spoofax language workbench: rules for declarative specification of languages and IDEs. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 444\u2013463. Reno/Tahoe, Nevada, 2010. ACM. URL: https://doi.org/10.1145/1869459.1869497 , doi:10.1145/1869459.1869497 . \u21a9 Markus V\u00f6lter, Sebastian Benz, Christian Dietrich, Birgit Engelmann, Mats Helander, Lennart C. L. Kats, Eelco Visser, and Guido Wachsmuth. DSL Engineering - Designing, Implementing and Using Domain-Specific Languages . dslbook.org, 2013. ISBN 978-1-4812-1858-0. URL: http://www.dslbook.org . \u21a9 Guido Wachsmuth, Gabri\u00ebl Konat, and Eelco Visser. Language design with the spoofax language workbench. IEEE Software , 31 \\(5\\) :35\u201343, 2014. URL: http://dx.doi.org/10.1109/MS.2014.100 , doi:10.1109/MS.2014.100 . \u21a9 Sebastian Erdweg, Tijs van der Storm, Markus V\u00f6lter, Meinte Boersma, Remi Bosman, William R. Cook, Albert Gerritsen, Angelo Hulshout, Steven Kelly, Alex Loh, Gabri\u00ebl Konat, Pedro J. Molina, Martin Palatnik, Risto Pohjonen, Eugen Schindler, Klemens Schindler, Riccardo Solmi, Vlad A. Vergu, Eelco Visser, Kevin van der Vlist, Guido Wachsmuth, and Jimi van der Woning. The state of the art in language workbenches - conclusions from the language workbench challenge. In Martin Erwig, Richard F. Paige, and Eric Van Wyk, editors, Software Language Engineering - 6 th International Conference, SLE 2013, Indianapolis, IN, USA, October 26-28, 2013. Proceedings , volume 8225 of Lecture Notes in Computer Science, 197\u2013217. Springer, 2013. URL: http://dx.doi.org/10.1007/978-3-319-02654-1_11 , doi:10.1007/978-3-319-02654-1_11 . \u21a9 Sebastian Erdweg, Tijs van der Storm, Markus V\u00f6lter, Laurence Tratt, Remi Bosman, William R. Cook, Albert Gerritsen, Angelo Hulshout, Steven Kelly, Alex Loh, Gabri\u00ebl Konat, Pedro J. Molina, Martin Palatnik, Risto Pohjonen, Eugen Schindler, Klemens Schindler, Riccardo Solmi, Vlad A. Vergu, Eelco Visser, Kevin van der Vlist, Guido Wachsmuth, and Jimi van der Woning. Evaluating and comparing language workbenches: existing results and benchmarks for the future. Computer Languages, Systems & Structures , 44:24\u201347, 2015. URL: http://dx.doi.org/10.1016/j.cl.2015.08.007 , doi:10.1016/j.cl.2015.08.007 . \u21a9","title":"References"},{"location":"background/bibliography/references/","text":"References \u00b6 The full Spoofax bibliography. 1: Lennart C. L. Kats and Eelco Visser. The Spoofax language workbench: rules for declarative specification of languages and IDEs. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 444\u2013463. Reno/Tahoe, Nevada, 2010. ACM. URL: https://doi.org/10.1145/1869459.1869497 , doi:10.1145/1869459.1869497 . 2: Markus V\u00f6lter, Sebastian Benz, Christian Dietrich, Birgit Engelmann, Mats Helander, Lennart C. L. Kats, Eelco Visser, and Guido Wachsmuth. DSL Engineering - Designing, Implementing and Using Domain-Specific Languages . dslbook.org, 2013. ISBN 978-1-4812-1858-0. URL: http://www.dslbook.org . 3: Guido Wachsmuth, Gabri\u00ebl Konat, and Eelco Visser. Language design with the spoofax language workbench. IEEE Software , 31 \\(5\\) :35\u201343, 2014. URL: http://dx.doi.org/10.1109/MS.2014.100 , doi:10.1109/MS.2014.100 . 4: Sebastian Erdweg, Tijs van der Storm, Markus V\u00f6lter, Meinte Boersma, Remi Bosman, William R. Cook, Albert Gerritsen, Angelo Hulshout, Steven Kelly, Alex Loh, Gabri\u00ebl Konat, Pedro J. Molina, Martin Palatnik, Risto Pohjonen, Eugen Schindler, Klemens Schindler, Riccardo Solmi, Vlad A. Vergu, Eelco Visser, Kevin van der Vlist, Guido Wachsmuth, and Jimi van der Woning. The state of the art in language workbenches - conclusions from the language workbench challenge. In Martin Erwig, Richard F. Paige, and Eric Van Wyk, editors, Software Language Engineering - 6 th International Conference, SLE 2013, Indianapolis, IN, USA, October 26-28, 2013. Proceedings , volume 8225 of Lecture Notes in Computer Science, 197\u2013217. Springer, 2013. URL: http://dx.doi.org/10.1007/978-3-319-02654-1_11 , doi:10.1007/978-3-319-02654-1_11 . 5: Sebastian Erdweg, Tijs van der Storm, Markus V\u00f6lter, Laurence Tratt, Remi Bosman, William R. Cook, Albert Gerritsen, Angelo Hulshout, Steven Kelly, Alex Loh, Gabri\u00ebl Konat, Pedro J. Molina, Martin Palatnik, Risto Pohjonen, Eugen Schindler, Klemens Schindler, Riccardo Solmi, Vlad A. Vergu, Eelco Visser, Kevin van der Vlist, Guido Wachsmuth, and Jimi van der Woning. Evaluating and comparing language workbenches: existing results and benchmarks for the future. Computer Languages, Systems & Structures , 44:24\u201347, 2015. URL: http://dx.doi.org/10.1016/j.cl.2015.08.007 , doi:10.1016/j.cl.2015.08.007 .","title":"References"},{"location":"background/bibliography/references/#references","text":"The full Spoofax bibliography. 1: Lennart C. L. Kats and Eelco Visser. The Spoofax language workbench: rules for declarative specification of languages and IDEs. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 444\u2013463. Reno/Tahoe, Nevada, 2010. ACM. URL: https://doi.org/10.1145/1869459.1869497 , doi:10.1145/1869459.1869497 . 2: Markus V\u00f6lter, Sebastian Benz, Christian Dietrich, Birgit Engelmann, Mats Helander, Lennart C. L. Kats, Eelco Visser, and Guido Wachsmuth. DSL Engineering - Designing, Implementing and Using Domain-Specific Languages . dslbook.org, 2013. ISBN 978-1-4812-1858-0. URL: http://www.dslbook.org . 3: Guido Wachsmuth, Gabri\u00ebl Konat, and Eelco Visser. Language design with the spoofax language workbench. IEEE Software , 31 \\(5\\) :35\u201343, 2014. URL: http://dx.doi.org/10.1109/MS.2014.100 , doi:10.1109/MS.2014.100 . 4: Sebastian Erdweg, Tijs van der Storm, Markus V\u00f6lter, Meinte Boersma, Remi Bosman, William R. Cook, Albert Gerritsen, Angelo Hulshout, Steven Kelly, Alex Loh, Gabri\u00ebl Konat, Pedro J. Molina, Martin Palatnik, Risto Pohjonen, Eugen Schindler, Klemens Schindler, Riccardo Solmi, Vlad A. Vergu, Eelco Visser, Kevin van der Vlist, Guido Wachsmuth, and Jimi van der Woning. The state of the art in language workbenches - conclusions from the language workbench challenge. In Martin Erwig, Richard F. Paige, and Eric Van Wyk, editors, Software Language Engineering - 6 th International Conference, SLE 2013, Indianapolis, IN, USA, October 26-28, 2013. Proceedings , volume 8225 of Lecture Notes in Computer Science, 197\u2013217. Springer, 2013. URL: http://dx.doi.org/10.1007/978-3-319-02654-1_11 , doi:10.1007/978-3-319-02654-1_11 . 5: Sebastian Erdweg, Tijs van der Storm, Markus V\u00f6lter, Laurence Tratt, Remi Bosman, William R. Cook, Albert Gerritsen, Angelo Hulshout, Steven Kelly, Alex Loh, Gabri\u00ebl Konat, Pedro J. Molina, Martin Palatnik, Risto Pohjonen, Eugen Schindler, Klemens Schindler, Riccardo Solmi, Vlad A. Vergu, Eelco Visser, Kevin van der Vlist, Guido Wachsmuth, and Jimi van der Woning. Evaluating and comparing language workbenches: existing results and benchmarks for the future. Computer Languages, Systems & Structures , 44:24\u201347, 2015. URL: http://dx.doi.org/10.1016/j.cl.2015.08.007 , doi:10.1016/j.cl.2015.08.007 .","title":"References"},{"location":"background/bibliography/sdf3/","text":"An SDF3 Bibliography \u00b6 SDF3 1 is the third generation in the SDF family of syntax definition formalisms, which were developed in the context of the ASF+SDF 2 , Stratego/XT 3 , and Spoofax 4 language workbenches. Kats et al. decribe the motivation for declarative syntax definition 5 . SDF \u00b6 The first SDF 6 supported modular composition of syntax definition, a direct correspondence between concrete and abstract syntax, and parsing with the full class of context-free grammars enabled by the Generalized-LR (GLR) parsing algorithm 7 8 . Its programming environment, as part of the ASF+SDF MetaEnvironment 9 , focused on live development of syntax definitions through incremental and modular scanner and parser generation 10 11 12 in order to provide fast turnaround times during language development. SDF2 \u00b6 The second generation, SDF2 encompassed a redesign of the internals of SDF without changing the surface syntax. The front-end of the implementation consisted of a transformation pipeline from the rich surface syntax to a minimal core (kernel) language 13 that served as input for parser generation. The key change of SDF2 was its integration of lexical and context-free syntax, supported by Scannerless GLR (SGLR) parsing 14 15 , enabling composition of languages with different lexical syntax 16 17 . SDF3 \u00b6 SDF3 is the latest member of the family and inherits many features of its predecessors. The most recognizable change is to the syntax of productions that should make it more familiar to users of other grammar formalisms. Further, it introduces new features in order to support multi-purpose interpretations of syntax definitions. The goals of the design of SDF3 are (1) to support the definition of the concrete and abstract syntax of formal languages (with an emphasis on programming languages), (2) to support declarative syntax definition so that there is no need to understand parsing algorithms in order to understand definitions 5 , (3) to make syntax definitions readable and understandable so that they can be used as reference documentation, and (4) to support execution of syntax definitions as parsers, but also for other syntactic operations, i.e to support multi-purpose interpretation based on a single source. The focus on multipurpose interpretation is driven by the role of SDF3 in the Spoofax language workbench 4 . Key features of SDF3 include Template productions 18 Error recovery 19 Layout constraints for layout-sensitive syntax 20 21 Safe and complete disambiguation of expression grammars 22 Placeholders and syntactic code completion 23 Future Work \u00b6 Parse table composition 24 , while implemented in a prototype, hasn't made into the production implementation yet. References \u00b6 Luis Eduardo de Souza Amorim and Eelco Visser. Multi-purpose syntax definition with SDF3. In Frank S. de Boer and Antonio Cerone, editors, Software Engineering and Formal Methods - 18 th International Conference, SEFM 2020, Amsterdam, The Netherlands, September 14-18, 2020, Proceedings , volume 12310 of Lecture Notes in Computer Science, 1\u201323. Springer, 2020. URL: https://doi.org/10.1007/978-3-030-58768-0_1 , doi:10.1007/978-3-030-58768-0_1 . \u21a9 Mark G. J. van den Brand, Arie van Deursen, Jan Heering, H. A. de Jong, Merijn de Jonge, Tobias Kuipers, Paul Klint, Leon Moonen, Pieter A. Olivier, Jeroen Scheerder, Jurgen J. Vinju, Eelco Visser, and Joost Visser. The ASF+SDF meta-environment: a component-based language development environment. In Reinhard Wilhelm, editor, Compiler Construction, 10 th International Conference, CC 2001 Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2001 Genova, Italy, April 2-6, 2001, Proceedings , volume 2027 of Lecture Notes in Computer Science, 365\u2013370. Springer, 2001. URL: https://doi.org/10.1016/S1571-0661 \\(04\\) 80917-4 , doi:10.1016/S1571-0661 \\(04\\) 80917-4 . \u21a9 Martin Bravenboer, Karl Trygve Kalleberg, Rob Vermaas, and Eelco Visser. Stratego/XT 0.17. A language and toolset for program transformation. Science of Computer Programming , 72 \\(1\\-2\\) :52\u201370, 2008. URL: http://dx.doi.org/10.1016/j.scico.2007.11.003 , doi:10.1016/j.scico.2007.11.003 . \u21a9 Lennart C. L. Kats and Eelco Visser. The Spoofax language workbench: rules for declarative specification of languages and IDEs. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 444\u2013463. Reno/Tahoe, Nevada, 2010. ACM. URL: https://doi.org/10.1145/1869459.1869497 , doi:10.1145/1869459.1869497 . \u21a9 \u21a9 Lennart C. L. Kats, Eelco Visser, and Guido Wachsmuth. Pure and declarative syntax definition: paradise lost and regained. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 918\u2013932. Reno/Tahoe, Nevada, 2010. ACM. URL: http://doi.acm.org/10.1145/1869459.1869535 , doi:10.1145/1869459.1869535 . \u21a9 \u21a9 Jan Heering, P. R. H. Hendriks, Paul Klint, and Jan Rekers. The syntax definition formalism SDF - reference manual. SIGPLAN Notices , 24 \\(11\\) :43\u201375, 1989. doi:10.1145/71605.71607 . \u21a9 Masaru Tomita. An efficient context-free parsing algorithm for natural languages. In IJCAI , 756\u2013764. 1985. \u21a9 Jan Rekers. Parser Generation for Interactive Environments . PhD thesis, University of Amsterdam, Amsterdam, The Netherlands, January 1992. \u21a9 Paul Klint. A meta-environment for generating programming environments. ACM Transactions on Software Engineering Methodology , 2 \\(2\\) :176\u2013201, 1993. doi:10.1145/151257.151260 . \u21a9 Jan Heering, Paul Klint, and Jan Rekers. Incremental generation of lexical scanners. ACM Transactions on Programming Languages and Systems , 14 \\(4\\) :490\u2013520, 1992. doi:10.1145/133233.133240 . \u21a9 Jan Heering, Paul Klint, and Jan Rekers. Incremental generation of parsers. IEEE Trans. Software Eng. , 16 \\(12\\) :1344\u20131351, 1990. \u21a9 Jan Heering, Paul Klint, and Jan Rekers. Lazy and incremental program generation. ACM Transactions on Programming Languages and Systems , 16 \\(3\\) :1010\u20131023, 1994. doi:10.1145/177492.177750 . \u21a9 Eelco Visser. A family of syntax definition formalisms. In Mark G. J. van den Brand and Vania Vieira Estrela, editors, ASF+SDF 1995. A Workshop on Generating Tools from Algebraic Specifications . Technical Report P9504, Programming Research Group, University of Amsterdam, May 1995. \u21a9 Eelco Visser. Syntax Definition for Language Prototyping . PhD thesis, University of Amsterdam, September 1997. \u21a9 Eelco Visser. Scannerless generalized-LR parsing. Technical Report P9707, Programming Research Group, University of Amsterdam, July 1997. \u21a9 Eelco Visser. Meta-programming with concrete object syntax. In Don S. Batory, Charles Consel, and Walid Taha, editors, Generative Programming and Component Engineering, ACM SIGPLAN/SIGSOFT Conference, GPCE 2002, Pittsburgh, PA, USA, October 6-8, 2002, Proceedings , volume 2487 of Lecture Notes in Computer Science, 299\u2013315. Springer, 2002. URL: https://doi.org/10.1007/3-540-45821-2_19 , doi:10.1007/3-540-45821-2_19 . \u21a9 Martin Bravenboer and Eelco Visser. Concrete syntax for objects: domain-specific language embedding and assimilation without restrictions. In John M. Vlissides and Douglas C. Schmidt, editors, Proceedings of the 19 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2004 , 365\u2013383. Vancouver, BC, Canada, 2004. ACM. URL: http://doi.acm.org/10.1145/1028976.1029007 , doi:10.1145/1028976.1029007 . \u21a9 Tobi Vollebregt, Lennart C. L. Kats, and Eelco Visser. Declarative specification of template-based textual editors. In Anthony Sloane and Suzana Andova, editors, International Workshop on Language Descriptions, Tools, and Applications, LDTA '12, Tallinn, Estonia, March 31 - April 1, 2012 , 1\u20137. ACM, 2012. URL: http://doi.acm.org/10.1145/2427048.2427056 , doi:10.1145/2427048.2427056 . \u21a9 Maartje de Jonge, Lennart C. L. Kats, Eelco Visser, and Emma S\u00f6derberg. Natural and flexible error recovery for generated modular language environments. ACM Transactions on Programming Languages and Systems , 34 \\(4\\) :15, 2012. URL: http://doi.acm.org/10.1145/2400676.2400678 , doi:10.1145/2400676.2400678 . \u21a9 Sebastian Erdweg, Tillmann Rendel, Christian K\u00e4stner, and Klaus Ostermann. Layout-sensitive generalized parsing. In Krzysztof Czarnecki and G\u00f6rel Hedin, editors, Software Language Engineering, 5 th International Conference, SLE 2012, Dresden, Germany, September 26-28, 2012, Revised Selected Papers , volume 7745 of Lecture Notes in Computer Science, 244\u2013263. Springer, 2012. doi:10.1007/978-3-642-36089-3\\_14 . \u21a9 Luis Eduardo de Souza Amorim, Michael J. Steindorfer, Sebastian Erdweg, and Eelco Visser. Declarative specification of indentation rules: a tooling perspective on parsing and pretty-printing layout-sensitive languages. In David Pearce 0005, Tanja Mayerhofer, and Friedrich Steimann, editors, Proceedings of the 11 th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2018, Boston, MA, USA, November 05-06, 2018 , 3\u201315. ACM, 2018. URL: https://doi.org/10.1145/3276604.3276607 , doi:10.1145/3276604.3276607 . \u21a9 Luis Eduardo de Souza Amorim. Declarative Syntax Definition for Modern Language Workbenches . PhD thesis, Delft University of Technology, Netherlands, 2019. base-search.net \\(fttudelft:oai:tudelft\\.nl:uuid:43d7992a\\-7077\\-47ba\\-b38f\\-113f5011d07f\\) . URL: https://www.base-search.net/Record/261b6c9463c1d4fe309e3c6104cd4d80fbc9d3cc8fbc66006f34130f481b506f . \u21a9 Luis Eduardo de Souza Amorim, Sebastian Erdweg, Guido Wachsmuth, and Eelco Visser. Principled syntactic code completion using placeholders. In Tijs van der Storm, Emilie Balland, and D\u00e1niel Varr\u00f3, editors, Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering, Amsterdam, The Netherlands, October 31 - November 1, 2016 , 163\u2013175. ACM, 2016. URL: http://dx.doi.org/10.1145/2997364.2997374 , doi:10.1145/2997364.2997374 . \u21a9 Martin Bravenboer and Eelco Visser. Parse table composition. In Dragan Gasevic, Ralf L\u00e4mmel, and Eric Van Wyk, editors, Software Language Engineering, First International Conference, SLE 2008, Toulouse, France, September 29-30, 2008. Revised Selected Papers , volume 5452 of Lecture Notes in Computer Science, 74\u201394. Springer, 2009. URL: http://dx.doi.org/10.1007/978-3-642-00434-6_6 , doi:10.1007/978-3-642-00434-6_6 . \u21a9","title":"An SDF3 Bibliography"},{"location":"background/bibliography/sdf3/#an-sdf3-bibliography","text":"SDF3 1 is the third generation in the SDF family of syntax definition formalisms, which were developed in the context of the ASF+SDF 2 , Stratego/XT 3 , and Spoofax 4 language workbenches. Kats et al. decribe the motivation for declarative syntax definition 5 .","title":"An SDF3 Bibliography"},{"location":"background/bibliography/sdf3/#sdf","text":"The first SDF 6 supported modular composition of syntax definition, a direct correspondence between concrete and abstract syntax, and parsing with the full class of context-free grammars enabled by the Generalized-LR (GLR) parsing algorithm 7 8 . Its programming environment, as part of the ASF+SDF MetaEnvironment 9 , focused on live development of syntax definitions through incremental and modular scanner and parser generation 10 11 12 in order to provide fast turnaround times during language development.","title":"SDF"},{"location":"background/bibliography/sdf3/#sdf2","text":"The second generation, SDF2 encompassed a redesign of the internals of SDF without changing the surface syntax. The front-end of the implementation consisted of a transformation pipeline from the rich surface syntax to a minimal core (kernel) language 13 that served as input for parser generation. The key change of SDF2 was its integration of lexical and context-free syntax, supported by Scannerless GLR (SGLR) parsing 14 15 , enabling composition of languages with different lexical syntax 16 17 .","title":"SDF2"},{"location":"background/bibliography/sdf3/#sdf3","text":"SDF3 is the latest member of the family and inherits many features of its predecessors. The most recognizable change is to the syntax of productions that should make it more familiar to users of other grammar formalisms. Further, it introduces new features in order to support multi-purpose interpretations of syntax definitions. The goals of the design of SDF3 are (1) to support the definition of the concrete and abstract syntax of formal languages (with an emphasis on programming languages), (2) to support declarative syntax definition so that there is no need to understand parsing algorithms in order to understand definitions 5 , (3) to make syntax definitions readable and understandable so that they can be used as reference documentation, and (4) to support execution of syntax definitions as parsers, but also for other syntactic operations, i.e to support multi-purpose interpretation based on a single source. The focus on multipurpose interpretation is driven by the role of SDF3 in the Spoofax language workbench 4 . Key features of SDF3 include Template productions 18 Error recovery 19 Layout constraints for layout-sensitive syntax 20 21 Safe and complete disambiguation of expression grammars 22 Placeholders and syntactic code completion 23","title":"SDF3"},{"location":"background/bibliography/sdf3/#future-work","text":"Parse table composition 24 , while implemented in a prototype, hasn't made into the production implementation yet.","title":"Future Work"},{"location":"background/bibliography/sdf3/#references","text":"Luis Eduardo de Souza Amorim and Eelco Visser. Multi-purpose syntax definition with SDF3. In Frank S. de Boer and Antonio Cerone, editors, Software Engineering and Formal Methods - 18 th International Conference, SEFM 2020, Amsterdam, The Netherlands, September 14-18, 2020, Proceedings , volume 12310 of Lecture Notes in Computer Science, 1\u201323. Springer, 2020. URL: https://doi.org/10.1007/978-3-030-58768-0_1 , doi:10.1007/978-3-030-58768-0_1 . \u21a9 Mark G. J. van den Brand, Arie van Deursen, Jan Heering, H. A. de Jong, Merijn de Jonge, Tobias Kuipers, Paul Klint, Leon Moonen, Pieter A. Olivier, Jeroen Scheerder, Jurgen J. Vinju, Eelco Visser, and Joost Visser. The ASF+SDF meta-environment: a component-based language development environment. In Reinhard Wilhelm, editor, Compiler Construction, 10 th International Conference, CC 2001 Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2001 Genova, Italy, April 2-6, 2001, Proceedings , volume 2027 of Lecture Notes in Computer Science, 365\u2013370. Springer, 2001. URL: https://doi.org/10.1016/S1571-0661 \\(04\\) 80917-4 , doi:10.1016/S1571-0661 \\(04\\) 80917-4 . \u21a9 Martin Bravenboer, Karl Trygve Kalleberg, Rob Vermaas, and Eelco Visser. Stratego/XT 0.17. A language and toolset for program transformation. Science of Computer Programming , 72 \\(1\\-2\\) :52\u201370, 2008. URL: http://dx.doi.org/10.1016/j.scico.2007.11.003 , doi:10.1016/j.scico.2007.11.003 . \u21a9 Lennart C. L. Kats and Eelco Visser. The Spoofax language workbench: rules for declarative specification of languages and IDEs. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 444\u2013463. Reno/Tahoe, Nevada, 2010. ACM. URL: https://doi.org/10.1145/1869459.1869497 , doi:10.1145/1869459.1869497 . \u21a9 \u21a9 Lennart C. L. Kats, Eelco Visser, and Guido Wachsmuth. Pure and declarative syntax definition: paradise lost and regained. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 918\u2013932. Reno/Tahoe, Nevada, 2010. ACM. URL: http://doi.acm.org/10.1145/1869459.1869535 , doi:10.1145/1869459.1869535 . \u21a9 \u21a9 Jan Heering, P. R. H. Hendriks, Paul Klint, and Jan Rekers. The syntax definition formalism SDF - reference manual. SIGPLAN Notices , 24 \\(11\\) :43\u201375, 1989. doi:10.1145/71605.71607 . \u21a9 Masaru Tomita. An efficient context-free parsing algorithm for natural languages. In IJCAI , 756\u2013764. 1985. \u21a9 Jan Rekers. Parser Generation for Interactive Environments . PhD thesis, University of Amsterdam, Amsterdam, The Netherlands, January 1992. \u21a9 Paul Klint. A meta-environment for generating programming environments. ACM Transactions on Software Engineering Methodology , 2 \\(2\\) :176\u2013201, 1993. doi:10.1145/151257.151260 . \u21a9 Jan Heering, Paul Klint, and Jan Rekers. Incremental generation of lexical scanners. ACM Transactions on Programming Languages and Systems , 14 \\(4\\) :490\u2013520, 1992. doi:10.1145/133233.133240 . \u21a9 Jan Heering, Paul Klint, and Jan Rekers. Incremental generation of parsers. IEEE Trans. Software Eng. , 16 \\(12\\) :1344\u20131351, 1990. \u21a9 Jan Heering, Paul Klint, and Jan Rekers. Lazy and incremental program generation. ACM Transactions on Programming Languages and Systems , 16 \\(3\\) :1010\u20131023, 1994. doi:10.1145/177492.177750 . \u21a9 Eelco Visser. A family of syntax definition formalisms. In Mark G. J. van den Brand and Vania Vieira Estrela, editors, ASF+SDF 1995. A Workshop on Generating Tools from Algebraic Specifications . Technical Report P9504, Programming Research Group, University of Amsterdam, May 1995. \u21a9 Eelco Visser. Syntax Definition for Language Prototyping . PhD thesis, University of Amsterdam, September 1997. \u21a9 Eelco Visser. Scannerless generalized-LR parsing. Technical Report P9707, Programming Research Group, University of Amsterdam, July 1997. \u21a9 Eelco Visser. Meta-programming with concrete object syntax. In Don S. Batory, Charles Consel, and Walid Taha, editors, Generative Programming and Component Engineering, ACM SIGPLAN/SIGSOFT Conference, GPCE 2002, Pittsburgh, PA, USA, October 6-8, 2002, Proceedings , volume 2487 of Lecture Notes in Computer Science, 299\u2013315. Springer, 2002. URL: https://doi.org/10.1007/3-540-45821-2_19 , doi:10.1007/3-540-45821-2_19 . \u21a9 Martin Bravenboer and Eelco Visser. Concrete syntax for objects: domain-specific language embedding and assimilation without restrictions. In John M. Vlissides and Douglas C. Schmidt, editors, Proceedings of the 19 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2004 , 365\u2013383. Vancouver, BC, Canada, 2004. ACM. URL: http://doi.acm.org/10.1145/1028976.1029007 , doi:10.1145/1028976.1029007 . \u21a9 Tobi Vollebregt, Lennart C. L. Kats, and Eelco Visser. Declarative specification of template-based textual editors. In Anthony Sloane and Suzana Andova, editors, International Workshop on Language Descriptions, Tools, and Applications, LDTA '12, Tallinn, Estonia, March 31 - April 1, 2012 , 1\u20137. ACM, 2012. URL: http://doi.acm.org/10.1145/2427048.2427056 , doi:10.1145/2427048.2427056 . \u21a9 Maartje de Jonge, Lennart C. L. Kats, Eelco Visser, and Emma S\u00f6derberg. Natural and flexible error recovery for generated modular language environments. ACM Transactions on Programming Languages and Systems , 34 \\(4\\) :15, 2012. URL: http://doi.acm.org/10.1145/2400676.2400678 , doi:10.1145/2400676.2400678 . \u21a9 Sebastian Erdweg, Tillmann Rendel, Christian K\u00e4stner, and Klaus Ostermann. Layout-sensitive generalized parsing. In Krzysztof Czarnecki and G\u00f6rel Hedin, editors, Software Language Engineering, 5 th International Conference, SLE 2012, Dresden, Germany, September 26-28, 2012, Revised Selected Papers , volume 7745 of Lecture Notes in Computer Science, 244\u2013263. Springer, 2012. doi:10.1007/978-3-642-36089-3\\_14 . \u21a9 Luis Eduardo de Souza Amorim, Michael J. Steindorfer, Sebastian Erdweg, and Eelco Visser. Declarative specification of indentation rules: a tooling perspective on parsing and pretty-printing layout-sensitive languages. In David Pearce 0005, Tanja Mayerhofer, and Friedrich Steimann, editors, Proceedings of the 11 th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2018, Boston, MA, USA, November 05-06, 2018 , 3\u201315. ACM, 2018. URL: https://doi.org/10.1145/3276604.3276607 , doi:10.1145/3276604.3276607 . \u21a9 Luis Eduardo de Souza Amorim. Declarative Syntax Definition for Modern Language Workbenches . PhD thesis, Delft University of Technology, Netherlands, 2019. base-search.net \\(fttudelft:oai:tudelft\\.nl:uuid:43d7992a\\-7077\\-47ba\\-b38f\\-113f5011d07f\\) . URL: https://www.base-search.net/Record/261b6c9463c1d4fe309e3c6104cd4d80fbc9d3cc8fbc66006f34130f481b506f . \u21a9 Luis Eduardo de Souza Amorim, Sebastian Erdweg, Guido Wachsmuth, and Eelco Visser. Principled syntactic code completion using placeholders. In Tijs van der Storm, Emilie Balland, and D\u00e1niel Varr\u00f3, editors, Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering, Amsterdam, The Netherlands, October 31 - November 1, 2016 , 163\u2013175. ACM, 2016. URL: http://dx.doi.org/10.1145/2997364.2997374 , doi:10.1145/2997364.2997374 . \u21a9 Martin Bravenboer and Eelco Visser. Parse table composition. In Dragan Gasevic, Ralf L\u00e4mmel, and Eric Van Wyk, editors, Software Language Engineering, First International Conference, SLE 2008, Toulouse, France, September 29-30, 2008. Revised Selected Papers , volume 5452 of Lecture Notes in Computer Science, 74\u201394. Springer, 2009. URL: http://dx.doi.org/10.1007/978-3-642-00434-6_6 , doi:10.1007/978-3-642-00434-6_6 . \u21a9","title":"References"},{"location":"background/bibliography/statix/","text":"A Statix Bibliography \u00b6 The Statix 1 2 meta-language provides support for the declarative definition of the static semantics of programming languages in terms of unification constraints and scope graph constraints for name resolution 1 guaranteeing query stability 2 . Here we trace the development of the Statix language. The NaBL Name Binding Language \u00b6 The NaBL 3 language provides support for the declaration of the name binding rules of programming languages in terms of definitions , references , and scoping . The NaBL task engine supports incremental execution of type checkers based on NaBL 4 . While the paper used the WebDSL language as example, the NaBL analysis was only applied in production in the SDF3 language. The NaBL language is very declarative, but binding patterns such as sequential let and 'subsequent scope' are difficult to express in it. Scope Graphs \u00b6 The study of the semantics of NaBL (and its limits in expressiveness) led to the formulation of a general theory of name resolution based on scope graphs 5 . The vertices of scope graphs are scopes and the edges model reachability. Declarations are associated with scopes. Name resolution by means of a declarative resolution calculus is defined as finding a path from the scope of a reference to the scope of a declaration taking into account the structure of the scope graph extended with visibility rules formulated in terms of path well-formedness and path specificity. Constraint Language \u00b6 Based on the theory of name resolution, a constraint language was defined with a declarative and operational semantics 6 . The language was designed for a two stage type checking process. In the first phase unification and scope constraints are generated, in the second phase these constraints are solved. A distinctive feature of this approach with respect to other constraint-based approaches to type checking, is the fact that name resolution is deferred until constraint resolution. This makes the definition of type-dependent name resolution, e.g. for computing the types of record fields, straightforward. The NaBL2 language was a concrete implementation of this design and was integrated into Spoofax. It featured concrete syntax for unification and scope graph constraints, and rules for mapping AST nodes to constraints. The two stage type checking process entailed limitations for the type systems that could be expressed in NaBL2. In particular, it was not possible to generate constraints based on information computed during the second stage. For example, a subtyping rule operating on types computed during constraint resolution Furthermore, the NaBL2 language itself was untyped, making it easy to make errors in specifications. Statix Language \u00b6 The Statix language 1 was designed to overcome the limitations of NaBL2. The language is typed, with signatures describing the types of ASTs, and typing rules declaring the types of predicates. The type system of Statix is expressed in NaBL2, making the specification of rules statically checked and much less prone to errors. This also provided a useful testbed of the ideas of scope graphs and constraints. The generation and resolution of constraints is intertwined, in order to allow computing constraints over inferred information. Furthermore, in order to generalize the notions of visibility supported by the NaBL2 language, Statix features query constraints, in order to relate references to declarations, but also to compute sets of names based on broader criteria. For example, the definition of structural record types can be expressed by a query that produces all fields of a record. Necessarily these changes entail that queries need to be executed in a scope graph that is not in its final form. This necessitate a theory of query stability. Name resolution queries such be scheduled such that they produce stable results, i.e. results that would also be produced at the end of the process. The this end a theory of critical edges was developed that asserts when it is safe to perform a query in a certain scope 2 . The Statix solver implements the operational semantics on the language in order to automatically derive type checkers from specifications. Optimizations of this solver can be based on the generaly underlying theory and be applied to all languages for which Statix specifications have been written. One such optimization is the derivation of implicitly parallel type checkers from Statix specifications 7 . Editor Services \u00b6 A next step in the evolution of Statix is the derivation of semantic editor services such as renaming and code completion from specifications 8 . References \u00b6 Hendrik van Antwerpen, Casper Bach Poulsen, Arjen Rouvoet, and Eelco Visser. Scopes as types. Proceedings of the ACM on Programming Languages , 2018. URL: https://doi.org/10.1145/3276484 , doi:10.1145/3276484 . \u21a9 \u21a9 \u21a9 Arjen Rouvoet, Hendrik van Antwerpen, Casper Bach Poulsen, Robbert Krebbers, and Eelco Visser. Knowing when to ask: sound scheduling of name resolution in type checkers derived from declarative specifications. Proceedings of the ACM on Programming Languages , 2020. URL: https://doi.org/10.1145/3428248 , doi:10.1145/3428248 . \u21a9 \u21a9 \u21a9 Gabri\u00ebl Konat, Lennart C. L. Kats, Guido Wachsmuth, and Eelco Visser. Declarative name binding and scope rules. In Krzysztof Czarnecki and G\u00f6rel Hedin, editors, Software Language Engineering, 5 th International Conference, SLE 2012, Dresden, Germany, September 26-28, 2012, Revised Selected Papers , volume 7745 of Lecture Notes in Computer Science, 311\u2013331. Springer, 2012. URL: http://dx.doi.org/10.1007/978-3-642-36089-3_18 , doi:10.1007/978-3-642-36089-3_18 . \u21a9 Guido Wachsmuth, Gabri\u00ebl Konat, Vlad A. Vergu, Danny M. Groenewegen, and Eelco Visser. A language independent task engine for incremental name and type analysis. In Martin Erwig, Richard F. Paige, and Eric Van Wyk, editors, Software Language Engineering - 6 th International Conference, SLE 2013, Indianapolis, IN, USA, October 26-28, 2013. Proceedings , volume 8225 of Lecture Notes in Computer Science, 260\u2013280. Springer, 2013. URL: http://dx.doi.org/10.1007/978-3-319-02654-1_15 , doi:10.1007/978-3-319-02654-1_15 . \u21a9 Pierre N\u00e9ron, Andrew P. Tolmach, Eelco Visser, and Guido Wachsmuth. A theory of name resolution. In Jan Vitek, editor, Programming Languages and Systems - 24 th European Symposium on Programming, ESOP 2015, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2015, London, UK, April 11-18, 2015. Proceedings , volume 9032 of Lecture Notes in Computer Science, 205\u2013231. Springer, 2015. URL: http://dx.doi.org/10.1007/978-3-662-46669-8_9 , doi:10.1007/978-3-662-46669-8_9 . \u21a9 Hendrik van Antwerpen, Pierre N\u00e9ron, Andrew P. Tolmach, Eelco Visser, and Guido Wachsmuth. A constraint language for static semantic analysis based on scope graphs. In Martin Erwig and Tiark Rompf, editors, Proceedings of the 2016 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation, PEPM 2016, St. Petersburg, FL, USA, January 20 - 22, 2016 , 49\u201360. ACM, 2016. URL: http://doi.acm.org/10.1145/2847538.2847543 , doi:10.1145/2847538.2847543 . \u21a9 Hendrik van Antwerpen and Eelco Visser. Scope states: guarding safety of name resolution in parallel type checkers. In ECOOP . 2021. To appear. \u21a9 Dani\u00ebl A. A. Pelsmaeker, Hendrik van Antwerpen, and Eelco Visser. Towards language-parametric semantic editor services based on declarative type system specifications \\(brave new idea paper\\) . In Alastair F. Donaldson, editor, 33 rd European Conference on Object-Oriented Programming, ECOOP 2019, July 15-19, 2019, London, United Kingdom , volume 134 of LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2019. URL: https://doi.org/10.4230/LIPIcs.ECOOP.2019.26 , doi:10.4230/LIPIcs.ECOOP.2019.26 . \u21a9","title":"A Statix Bibliography"},{"location":"background/bibliography/statix/#a-statix-bibliography","text":"The Statix 1 2 meta-language provides support for the declarative definition of the static semantics of programming languages in terms of unification constraints and scope graph constraints for name resolution 1 guaranteeing query stability 2 . Here we trace the development of the Statix language.","title":"A Statix Bibliography"},{"location":"background/bibliography/statix/#the-nabl-name-binding-language","text":"The NaBL 3 language provides support for the declaration of the name binding rules of programming languages in terms of definitions , references , and scoping . The NaBL task engine supports incremental execution of type checkers based on NaBL 4 . While the paper used the WebDSL language as example, the NaBL analysis was only applied in production in the SDF3 language. The NaBL language is very declarative, but binding patterns such as sequential let and 'subsequent scope' are difficult to express in it.","title":"The NaBL Name Binding Language"},{"location":"background/bibliography/statix/#scope-graphs","text":"The study of the semantics of NaBL (and its limits in expressiveness) led to the formulation of a general theory of name resolution based on scope graphs 5 . The vertices of scope graphs are scopes and the edges model reachability. Declarations are associated with scopes. Name resolution by means of a declarative resolution calculus is defined as finding a path from the scope of a reference to the scope of a declaration taking into account the structure of the scope graph extended with visibility rules formulated in terms of path well-formedness and path specificity.","title":"Scope Graphs"},{"location":"background/bibliography/statix/#constraint-language","text":"Based on the theory of name resolution, a constraint language was defined with a declarative and operational semantics 6 . The language was designed for a two stage type checking process. In the first phase unification and scope constraints are generated, in the second phase these constraints are solved. A distinctive feature of this approach with respect to other constraint-based approaches to type checking, is the fact that name resolution is deferred until constraint resolution. This makes the definition of type-dependent name resolution, e.g. for computing the types of record fields, straightforward. The NaBL2 language was a concrete implementation of this design and was integrated into Spoofax. It featured concrete syntax for unification and scope graph constraints, and rules for mapping AST nodes to constraints. The two stage type checking process entailed limitations for the type systems that could be expressed in NaBL2. In particular, it was not possible to generate constraints based on information computed during the second stage. For example, a subtyping rule operating on types computed during constraint resolution Furthermore, the NaBL2 language itself was untyped, making it easy to make errors in specifications.","title":"Constraint Language"},{"location":"background/bibliography/statix/#statix-language","text":"The Statix language 1 was designed to overcome the limitations of NaBL2. The language is typed, with signatures describing the types of ASTs, and typing rules declaring the types of predicates. The type system of Statix is expressed in NaBL2, making the specification of rules statically checked and much less prone to errors. This also provided a useful testbed of the ideas of scope graphs and constraints. The generation and resolution of constraints is intertwined, in order to allow computing constraints over inferred information. Furthermore, in order to generalize the notions of visibility supported by the NaBL2 language, Statix features query constraints, in order to relate references to declarations, but also to compute sets of names based on broader criteria. For example, the definition of structural record types can be expressed by a query that produces all fields of a record. Necessarily these changes entail that queries need to be executed in a scope graph that is not in its final form. This necessitate a theory of query stability. Name resolution queries such be scheduled such that they produce stable results, i.e. results that would also be produced at the end of the process. The this end a theory of critical edges was developed that asserts when it is safe to perform a query in a certain scope 2 . The Statix solver implements the operational semantics on the language in order to automatically derive type checkers from specifications. Optimizations of this solver can be based on the generaly underlying theory and be applied to all languages for which Statix specifications have been written. One such optimization is the derivation of implicitly parallel type checkers from Statix specifications 7 .","title":"Statix Language"},{"location":"background/bibliography/statix/#editor-services","text":"A next step in the evolution of Statix is the derivation of semantic editor services such as renaming and code completion from specifications 8 .","title":"Editor Services"},{"location":"background/bibliography/statix/#references","text":"Hendrik van Antwerpen, Casper Bach Poulsen, Arjen Rouvoet, and Eelco Visser. Scopes as types. Proceedings of the ACM on Programming Languages , 2018. URL: https://doi.org/10.1145/3276484 , doi:10.1145/3276484 . \u21a9 \u21a9 \u21a9 Arjen Rouvoet, Hendrik van Antwerpen, Casper Bach Poulsen, Robbert Krebbers, and Eelco Visser. Knowing when to ask: sound scheduling of name resolution in type checkers derived from declarative specifications. Proceedings of the ACM on Programming Languages , 2020. URL: https://doi.org/10.1145/3428248 , doi:10.1145/3428248 . \u21a9 \u21a9 \u21a9 Gabri\u00ebl Konat, Lennart C. L. Kats, Guido Wachsmuth, and Eelco Visser. Declarative name binding and scope rules. In Krzysztof Czarnecki and G\u00f6rel Hedin, editors, Software Language Engineering, 5 th International Conference, SLE 2012, Dresden, Germany, September 26-28, 2012, Revised Selected Papers , volume 7745 of Lecture Notes in Computer Science, 311\u2013331. Springer, 2012. URL: http://dx.doi.org/10.1007/978-3-642-36089-3_18 , doi:10.1007/978-3-642-36089-3_18 . \u21a9 Guido Wachsmuth, Gabri\u00ebl Konat, Vlad A. Vergu, Danny M. Groenewegen, and Eelco Visser. A language independent task engine for incremental name and type analysis. In Martin Erwig, Richard F. Paige, and Eric Van Wyk, editors, Software Language Engineering - 6 th International Conference, SLE 2013, Indianapolis, IN, USA, October 26-28, 2013. Proceedings , volume 8225 of Lecture Notes in Computer Science, 260\u2013280. Springer, 2013. URL: http://dx.doi.org/10.1007/978-3-319-02654-1_15 , doi:10.1007/978-3-319-02654-1_15 . \u21a9 Pierre N\u00e9ron, Andrew P. Tolmach, Eelco Visser, and Guido Wachsmuth. A theory of name resolution. In Jan Vitek, editor, Programming Languages and Systems - 24 th European Symposium on Programming, ESOP 2015, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2015, London, UK, April 11-18, 2015. Proceedings , volume 9032 of Lecture Notes in Computer Science, 205\u2013231. Springer, 2015. URL: http://dx.doi.org/10.1007/978-3-662-46669-8_9 , doi:10.1007/978-3-662-46669-8_9 . \u21a9 Hendrik van Antwerpen, Pierre N\u00e9ron, Andrew P. Tolmach, Eelco Visser, and Guido Wachsmuth. A constraint language for static semantic analysis based on scope graphs. In Martin Erwig and Tiark Rompf, editors, Proceedings of the 2016 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation, PEPM 2016, St. Petersburg, FL, USA, January 20 - 22, 2016 , 49\u201360. ACM, 2016. URL: http://doi.acm.org/10.1145/2847538.2847543 , doi:10.1145/2847538.2847543 . \u21a9 Hendrik van Antwerpen and Eelco Visser. Scope states: guarding safety of name resolution in parallel type checkers. In ECOOP . 2021. To appear. \u21a9 Dani\u00ebl A. A. Pelsmaeker, Hendrik van Antwerpen, and Eelco Visser. Towards language-parametric semantic editor services based on declarative type system specifications \\(brave new idea paper\\) . In Alastair F. Donaldson, editor, 33 rd European Conference on Object-Oriented Programming, ECOOP 2019, July 15-19, 2019, London, United Kingdom , volume 134 of LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2019. URL: https://doi.org/10.4230/LIPIcs.ECOOP.2019.26 , doi:10.4230/LIPIcs.ECOOP.2019.26 . \u21a9","title":"References"},{"location":"background/bibliography/stratego/","text":"A Stratego Bibliography \u00b6 The original publication on Stratego appeared in ICFP'98 1 and introduced named rewrite rules and a language of strategy combinators with an operational semantics. An important aspect of the design of Stratego is the separation of the language in a core language 2 of strategy combinators and a high-level 'sugar' language of rewrite rules that can be desugared to the core language. This is still the way that the Stratego compiler is organized. The original also introduced contextual terms. These where eventually replaced by dynamic rewrite rules 3 . The paper about dynamic rules 3 also provides a comprehensive overview of Stratego and its operational semantics. Stratego and its ecosystem are described in a number of system description papers, including Stratego 0.5 4 , Stratego/XT 0.16 5 , Stratego/XT 0.17 6 Recently, a gradual type system was designed for Stratego 7 . This design and its incremental compiler 8 are the basis for the Stratego2 version of the language. References \u00b6 Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661 \\(05\\) 80027-1 , doi:10.1016/S1571-0661 \\(05\\) 80027-1 . \u21a9 Martin Bravenboer, Arthur van Dam, Karina Olmos, and Eelco Visser. Program transformation with scoped dynamic rewrite rules. Fundamenta Informaticae , 69 \\(1\\-2\\) :123\u2013178, 2006. URL: https://content.iospress.com/articles/fundamenta-informaticae/fi69-1-2-06 . \u21a9 \u21a9 Eelco Visser. Stratego: a language for program transformation based on rewriting strategies. In Aart Middeldorp, editor, Rewriting Techniques and Applications, 12 th International Conference, RTA 2001, Utrecht, The Netherlands, May 22-24, 2001, Proceedings , volume 2051 of Lecture Notes in Computer Science, 357\u2013362. Springer, 2001. URL: https://doi.org/10.1007/3-540-45127-7_27 , doi:10.1007/3-540-45127-7_27 . \u21a9 Martin Bravenboer, Karl Trygve Kalleberg, Rob Vermaas, and Eelco Visser. Stratego/XT 0.16: components for transformation systems. In John Hatcliff and Frank Tip, editors, Proceedings of the 2006 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-based Program Manipulation, 2006, Charleston, South Carolina, USA, January 9-10, 2006 , 95\u201399. ACM, 2006. URL: http://doi.acm.org/10.1145/1111542.1111558 , doi:10.1145/1111542.1111558 . \u21a9 Martin Bravenboer, Karl Trygve Kalleberg, Rob Vermaas, and Eelco Visser. Stratego/XT 0.17. A language and toolset for program transformation. Science of Computer Programming , 72 \\(1\\-2\\) :52\u201370, 2008. URL: http://dx.doi.org/10.1016/j.scico.2007.11.003 , doi:10.1016/j.scico.2007.11.003 . \u21a9 Jeff Smits and Eelco Visser. Gradually typing strategies. In Ralf L\u00e4mmel, Laurence Tratt, and Juan de Lara, editors, Proceedings of the 13 th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2020, Virtual Event, USA, November 16-17, 2020 , 1\u201315. ACM, 2020. URL: https://doi.org/10.1145/3426425.3426928 , doi:10.1145/3426425.3426928 . \u21a9 Jeff Smits, Gabri\u00ebl Konat, and Eelco Visser. Constructing hybrid incremental compilers for cross-module extensibility with an internal build system. Programming Journal , 4 \\(3\\) :16, 2020. URL: https://doi.org/10.22152/programming-journal.org/2020/4/16 , doi:10.22152/programming-journal.org/2020/4/16 . \u21a9","title":"A Stratego Bibliography"},{"location":"background/bibliography/stratego/#a-stratego-bibliography","text":"The original publication on Stratego appeared in ICFP'98 1 and introduced named rewrite rules and a language of strategy combinators with an operational semantics. An important aspect of the design of Stratego is the separation of the language in a core language 2 of strategy combinators and a high-level 'sugar' language of rewrite rules that can be desugared to the core language. This is still the way that the Stratego compiler is organized. The original also introduced contextual terms. These where eventually replaced by dynamic rewrite rules 3 . The paper about dynamic rules 3 also provides a comprehensive overview of Stratego and its operational semantics. Stratego and its ecosystem are described in a number of system description papers, including Stratego 0.5 4 , Stratego/XT 0.16 5 , Stratego/XT 0.17 6 Recently, a gradual type system was designed for Stratego 7 . This design and its incremental compiler 8 are the basis for the Stratego2 version of the language.","title":"A Stratego Bibliography"},{"location":"background/bibliography/stratego/#references","text":"Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661 \\(05\\) 80027-1 , doi:10.1016/S1571-0661 \\(05\\) 80027-1 . \u21a9 Martin Bravenboer, Arthur van Dam, Karina Olmos, and Eelco Visser. Program transformation with scoped dynamic rewrite rules. Fundamenta Informaticae , 69 \\(1\\-2\\) :123\u2013178, 2006. URL: https://content.iospress.com/articles/fundamenta-informaticae/fi69-1-2-06 . \u21a9 \u21a9 Eelco Visser. Stratego: a language for program transformation based on rewriting strategies. In Aart Middeldorp, editor, Rewriting Techniques and Applications, 12 th International Conference, RTA 2001, Utrecht, The Netherlands, May 22-24, 2001, Proceedings , volume 2051 of Lecture Notes in Computer Science, 357\u2013362. Springer, 2001. URL: https://doi.org/10.1007/3-540-45127-7_27 , doi:10.1007/3-540-45127-7_27 . \u21a9 Martin Bravenboer, Karl Trygve Kalleberg, Rob Vermaas, and Eelco Visser. Stratego/XT 0.16: components for transformation systems. In John Hatcliff and Frank Tip, editors, Proceedings of the 2006 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-based Program Manipulation, 2006, Charleston, South Carolina, USA, January 9-10, 2006 , 95\u201399. ACM, 2006. URL: http://doi.acm.org/10.1145/1111542.1111558 , doi:10.1145/1111542.1111558 . \u21a9 Martin Bravenboer, Karl Trygve Kalleberg, Rob Vermaas, and Eelco Visser. Stratego/XT 0.17. A language and toolset for program transformation. Science of Computer Programming , 72 \\(1\\-2\\) :52\u201370, 2008. URL: http://dx.doi.org/10.1016/j.scico.2007.11.003 , doi:10.1016/j.scico.2007.11.003 . \u21a9 Jeff Smits and Eelco Visser. Gradually typing strategies. In Ralf L\u00e4mmel, Laurence Tratt, and Juan de Lara, editors, Proceedings of the 13 th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2020, Virtual Event, USA, November 16-17, 2020 , 1\u201315. ACM, 2020. URL: https://doi.org/10.1145/3426425.3426928 , doi:10.1145/3426425.3426928 . \u21a9 Jeff Smits, Gabri\u00ebl Konat, and Eelco Visser. Constructing hybrid incremental compilers for cross-module extensibility with an internal build system. Programming Journal , 4 \\(3\\) :16, 2020. URL: https://doi.org/10.22152/programming-journal.org/2020/4/16 , doi:10.22152/programming-journal.org/2020/4/16 . \u21a9","title":"References"},{"location":"background/documentation/","text":"Documentation \u00b6 This section explains the documentation's technology and structure, and how you can contribute. Technology \u00b6 This documentation uses MkDocs , a fast and simple static site generated that's geared towards building project documentation from Markdown files. In particular, this website uses MkDocs Material , which provides a clean look, easy customization, and many features for technical documentation. Structure \u00b6 The structure of this documentation follows the Grand Unified Theory of Documentation where documentation is split into four categories: Tutorials : oriented to learning , enabling newcomers to get started through a lesson, analogous to teaching a child how to cook. How-Tos : oriented to a particular goal , showing how to solve a specific problem through a series of steps, analogous to a recipe in a cookbook. Reference : oriented to information , describing the machinery through dry description, analogous to an encyclopaedia article. Background : oriented to understanding , explaining through discursive explanation, analogous to an article on culinary social history. Contributing \u00b6 Contributing to the documentation is easy. Quick changes and fixing typos can be done by clicking the button in the top-right corner of a page, and editing and saving the underlying Markdown file. More considerable contributions can be made by cloning this repository locally, and editing the Markdown files there. The easiest way to get a live preview (automatically reloading) of your changes, is by installing Docker and executing make from the root directory. This will serve the latest changes to localhost:8000 . MkDocs Reference Extensions Reference","title":"Documentation"},{"location":"background/documentation/#documentation","text":"This section explains the documentation's technology and structure, and how you can contribute.","title":"Documentation"},{"location":"background/documentation/#technology","text":"This documentation uses MkDocs , a fast and simple static site generated that's geared towards building project documentation from Markdown files. In particular, this website uses MkDocs Material , which provides a clean look, easy customization, and many features for technical documentation.","title":"Technology"},{"location":"background/documentation/#structure","text":"The structure of this documentation follows the Grand Unified Theory of Documentation where documentation is split into four categories: Tutorials : oriented to learning , enabling newcomers to get started through a lesson, analogous to teaching a child how to cook. How-Tos : oriented to a particular goal , showing how to solve a specific problem through a series of steps, analogous to a recipe in a cookbook. Reference : oriented to information , describing the machinery through dry description, analogous to an encyclopaedia article. Background : oriented to understanding , explaining through discursive explanation, analogous to an article on culinary social history.","title":"Structure"},{"location":"background/documentation/#contributing","text":"Contributing to the documentation is easy. Quick changes and fixing typos can be done by clicking the button in the top-right corner of a page, and editing and saving the underlying Markdown file. More considerable contributions can be made by cloning this repository locally, and editing the Markdown files there. The easiest way to get a live preview (automatically reloading) of your changes, is by installing Docker and executing make from the root directory. This will serve the latest changes to localhost:8000 . MkDocs Reference Extensions Reference","title":"Contributing"},{"location":"background/documentation/citations/","text":"Documentation Citations \u00b6 To cite a paper or work, first ensure the citation is in a bibliography ( .bib ) file in the /bibliographies/ directory. For example, in the bibliographies/spoofax.bib file, we find: @inproceedings { KatsV10 , title = {The {Spoofax} language workbench: rules for declarative specification of languages and {IDEs}} , author = {Lennart C. L. Kats and Eelco Visser} , year = {2010} , doi = {10.1145/1869459.1869497} , url = {https://doi.org/10.1145/1869459.1869497} , pages = {444-463} , booktitle = {Proceedings of the 25th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010} , } Adding References To add a reference, add it on Researchr to the Spoofax bibliography . Then on the command-line, invoke the following to regenerate the spoofax.bib file: make bib Do not change the spoofax.bib file manually, it is generated and updated through Researchr . Then reference the work like this: The Spoofax language workbench[@KatsV10] is vital to declarative language development. Finally, add a place for the bibliography footnotes to be added (usually at the end of the file) by adding the following line to the file: \\bibliography The line will be rendered as: The Spoofax language workbench 1 is vital to declarative language development. And the references will be at the bottom of this page. If the citation appears rendered as Spoofax language workbench[^1] , then you might have forgotten to add a place for the bibliography. Lennart C. L. Kats and Eelco Visser. The Spoofax language workbench: rules for declarative specification of languages and IDEs. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 444\u2013463. Reno/Tahoe, Nevada, 2010. ACM. URL: https://doi.org/10.1145/1869459.1869497 , doi:10.1145/1869459.1869497 . \u21a9","title":"Citations"},{"location":"background/documentation/citations/#documentation-citations","text":"To cite a paper or work, first ensure the citation is in a bibliography ( .bib ) file in the /bibliographies/ directory. For example, in the bibliographies/spoofax.bib file, we find: @inproceedings { KatsV10 , title = {The {Spoofax} language workbench: rules for declarative specification of languages and {IDEs}} , author = {Lennart C. L. Kats and Eelco Visser} , year = {2010} , doi = {10.1145/1869459.1869497} , url = {https://doi.org/10.1145/1869459.1869497} , pages = {444-463} , booktitle = {Proceedings of the 25th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010} , } Adding References To add a reference, add it on Researchr to the Spoofax bibliography . Then on the command-line, invoke the following to regenerate the spoofax.bib file: make bib Do not change the spoofax.bib file manually, it is generated and updated through Researchr . Then reference the work like this: The Spoofax language workbench[@KatsV10] is vital to declarative language development. Finally, add a place for the bibliography footnotes to be added (usually at the end of the file) by adding the following line to the file: \\bibliography The line will be rendered as: The Spoofax language workbench 1 is vital to declarative language development. And the references will be at the bottom of this page. If the citation appears rendered as Spoofax language workbench[^1] , then you might have forgotten to add a place for the bibliography. Lennart C. L. Kats and Eelco Visser. The Spoofax language workbench: rules for declarative specification of languages and IDEs. In William R. Cook, Siobh\u00e1n Clarke, and Martin C. Rinard, editors, Proceedings of the 25 th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010 , 444\u2013463. Reno/Tahoe, Nevada, 2010. ACM. URL: https://doi.org/10.1145/1869459.1869497 , doi:10.1145/1869459.1869497 . \u21a9","title":"Documentation Citations"},{"location":"background/documentation/code-highlighting/","text":"Documentation Code Highlighting \u00b6 Block Highlighting \u00b6 To apply code highlighting to a block of code, surround it with triple backticks ( ``` ) and write the name of the language after the starting backticks, like this: Markdown ```python def foo(): pass ``` Output def foo (): pass Inline Highlighting \u00b6 To apply code highlighting to inline code, surround the code with single backticks ( ` ) and add #! after the initial backtick, followed by the language name: Markdown Call the `#!python def foo()` function. Output Call the def foo () function. Languages \u00b6 The highlighter supports all languages supported by Pygments . The Pygments project has a list of all supported languages . To use a language, refer to it through one of its short names . Additionally, in this Spoofax documentation these Spoofax languages are supported: Short name Language aterm ATerms . dynsem DynSem. esv ESV . flowspec FlowSpec . nabl NaBL. nabl2 NaBL2. sdf3 SDF3 . statix Statix . stratego Stratego .","title":"Code Highlighting"},{"location":"background/documentation/code-highlighting/#documentation-code-highlighting","text":"","title":"Documentation Code Highlighting"},{"location":"background/documentation/code-highlighting/#block-highlighting","text":"To apply code highlighting to a block of code, surround it with triple backticks ( ``` ) and write the name of the language after the starting backticks, like this: Markdown ```python def foo(): pass ``` Output def foo (): pass","title":"Block Highlighting"},{"location":"background/documentation/code-highlighting/#inline-highlighting","text":"To apply code highlighting to inline code, surround the code with single backticks ( ` ) and add #! after the initial backtick, followed by the language name: Markdown Call the `#!python def foo()` function. Output Call the def foo () function.","title":"Inline Highlighting"},{"location":"background/documentation/code-highlighting/#languages","text":"The highlighter supports all languages supported by Pygments . The Pygments project has a list of all supported languages . To use a language, refer to it through one of its short names . Additionally, in this Spoofax documentation these Spoofax languages are supported: Short name Language aterm ATerms . dynsem DynSem. esv ESV . flowspec FlowSpec . nabl NaBL. nabl2 NaBL2. sdf3 SDF3 . statix Statix . stratego Stratego .","title":"Languages"},{"location":"background/documentation/guis-and-menus/","text":"Documentation GUIs and Menus \u00b6 To describe (a sequence of) menu items or GUI elements, you can format them using the gui inline language, and separate them with > . For example, this describes a preferences sequence in Eclipse: Open the `#!gui General > Startup and Shutdown` settings page. It is rendered as: Open the General \u2023 Startup and Shutdown settings page.","title":"GUIs and Menus"},{"location":"background/documentation/guis-and-menus/#documentation-guis-and-menus","text":"To describe (a sequence of) menu items or GUI elements, you can format them using the gui inline language, and separate them with > . For example, this describes a preferences sequence in Eclipse: Open the `#!gui General > Startup and Shutdown` settings page. It is rendered as: Open the General \u2023 Startup and Shutdown settings page.","title":"Documentation GUIs and Menus"},{"location":"background/documentation/links/","text":"Documentation Links \u00b6 Links to other parts of the documentation should be written as relative links and point to specific Markdown files. For example, to add a link to tutorials from the background/index.md page, the link should read: [ Tutorials ]( ../tutorials/index.md ) To link to the index page of a section, link to the index.md file. Avoid internal links without .md Even if it might seem to work, do not link to an internal page without specifying the .md file to link to. For example, do not use [Background](../background/) . These links will not work once the documentation has been deployed. Additionally, you won't see any warnings about these (possibly broken) links in the console when running mkdocs or its Docker image. Absolute Links are Not Supported Even if it might seem to work, do not link to an internal page using an absolute link. For example, do not use [Background](/background/index.md) . These links are not properly converted and will break once the documentation has been deployed.","title":"Links"},{"location":"background/documentation/links/#documentation-links","text":"Links to other parts of the documentation should be written as relative links and point to specific Markdown files. For example, to add a link to tutorials from the background/index.md page, the link should read: [ Tutorials ]( ../tutorials/index.md ) To link to the index page of a section, link to the index.md file. Avoid internal links without .md Even if it might seem to work, do not link to an internal page without specifying the .md file to link to. For example, do not use [Background](../background/) . These links will not work once the documentation has been deployed. Additionally, you won't see any warnings about these (possibly broken) links in the console when running mkdocs or its Docker image. Absolute Links are Not Supported Even if it might seem to work, do not link to an internal page using an absolute link. For example, do not use [Background](/background/index.md) . These links are not properly converted and will break once the documentation has been deployed.","title":"Documentation Links"},{"location":"background/documentation/pages/","text":"Documentation Pages \u00b6 To add a new page to the documentation: Create a Markdown ( .md ) file in an appropriate location in the docs/ folder; Add the page to the nav element in the mkdocs.yml file in the root of the repository. Overriding the title By default, the title shown in the Table of Contents is the title of the page. To override this, specify a title in the nav element explicitly. For example: nav : - Home : - index.md - Installation : getting-started.md By convention, the first page mentioned in nav under a section should be some index.md (without a title), and will be used as the index page (home page) for that section.","title":"Adding Pages"},{"location":"background/documentation/pages/#documentation-pages","text":"To add a new page to the documentation: Create a Markdown ( .md ) file in an appropriate location in the docs/ folder; Add the page to the nav element in the mkdocs.yml file in the root of the repository. Overriding the title By default, the title shown in the Table of Contents is the title of the page. To override this, specify a title in the nav element explicitly. For example: nav : - Home : - index.md - Installation : getting-started.md By convention, the first page mentioned in nav under a section should be some index.md (without a title), and will be used as the index page (home page) for that section.","title":"Documentation Pages"},{"location":"background/documentation/structure/","text":"Documentation Structure \u00b6 The structure of the documentation repository is as follows (hover over any of the files to see its description): \ud83d\udce6 / \u2523 \ud83d\udcc1 .github \u2523 \ud83d\udcc2 bibliographies \u2523 \ud83d\udcc2 docs \u2503 \u2523 \ud83d\udcc2 assets \u2503 \u2503 \u2523 \ud83d\udcdc favicon.png \u2503 \u2503 \u2523 \ud83d\udcdc hero-border-dark.svg \u2503 \u2503 \u2523 \ud83d\udcdc hero-border-light.svg \u2503 \u2503 \u2523 \ud83d\udcdc hero.svg \u2503 \u2503 \u2523 \ud83d\udcdc logo.svg \u2503 \u2503 \u2517 \ud83d\udcdc styles.css \u2503 \u2523 \ud83d\udcc2 background \u2503 \u2523 \ud83d\udcc2 howtos \u2503 \u2523 \ud83d\udcc2 reference \u2503 \u2523 \ud83d\udcc2 release \u2503 \u2523 \ud83d\udcc2 support \u2503 \u2523 \ud83d\udcc2 tutorials \u2503 \u2517 \ud83d\udcdc index.md \u2523 \ud83d\udcc1 overrides \u2503 \u2523 \ud83d\udcdc index.html \u2503 \u2517 \ud83d\udcdc main.html \u2523 \ud83d\udcdc .gitignore \u2523 \ud83d\udcdc Dockerfile \u2523 \ud83d\udcdc LICENSE \u2523 \ud83d\udcdc Makefile \u2523 \ud83d\udcdc mkdocs_requirements.txt \u2523 \ud83d\udcdc mkdocs.yml \u2517 \ud83d\udcdc README.md","title":"Structure"},{"location":"background/documentation/structure/#documentation-structure","text":"The structure of the documentation repository is as follows (hover over any of the files to see its description): \ud83d\udce6 / \u2523 \ud83d\udcc1 .github \u2523 \ud83d\udcc2 bibliographies \u2523 \ud83d\udcc2 docs \u2503 \u2523 \ud83d\udcc2 assets \u2503 \u2503 \u2523 \ud83d\udcdc favicon.png \u2503 \u2503 \u2523 \ud83d\udcdc hero-border-dark.svg \u2503 \u2503 \u2523 \ud83d\udcdc hero-border-light.svg \u2503 \u2503 \u2523 \ud83d\udcdc hero.svg \u2503 \u2503 \u2523 \ud83d\udcdc logo.svg \u2503 \u2503 \u2517 \ud83d\udcdc styles.css \u2503 \u2523 \ud83d\udcc2 background \u2503 \u2523 \ud83d\udcc2 howtos \u2503 \u2523 \ud83d\udcc2 reference \u2503 \u2523 \ud83d\udcc2 release \u2503 \u2523 \ud83d\udcc2 support \u2503 \u2523 \ud83d\udcc2 tutorials \u2503 \u2517 \ud83d\udcdc index.md \u2523 \ud83d\udcc1 overrides \u2503 \u2523 \ud83d\udcdc index.html \u2503 \u2517 \ud83d\udcdc main.html \u2523 \ud83d\udcdc .gitignore \u2523 \ud83d\udcdc Dockerfile \u2523 \ud83d\udcdc LICENSE \u2523 \ud83d\udcdc Makefile \u2523 \ud83d\udcdc mkdocs_requirements.txt \u2523 \ud83d\udcdc mkdocs.yml \u2517 \ud83d\udcdc README.md","title":"Documentation Structure"},{"location":"background/documentation/troubleshooting/","text":"Documentation Troubleshooting \u00b6 Macro Syntax Error: Missing end of comment tag \u00b6 The macros in the documentation, including in code blocks, is expanded through the mkdocs-macros plugin which uses the Jinja2 template processor. Its template syntax is very advanced, allowing not only to replace simple references, but also more complex statements and comments. In this case, Jinja sees {# and interprets it as a comment. Of course, then it cannot find the end of the comment and produces an error. INFO - [macros] - ERROR # _Macro Syntax Error_ _Line 11 in Markdown file:_ **Missing end of comment tag** ```python {# This is not a Jinja comment } ``` To work around this, wrap the block in a {% raw %} and {% endraw %} tag. For example: {% raw %} ``` {# This is not a Jinja comment } ``` {% endraw %} Alternatively, if you still want to use other macros in the code, wrap the offending character sequence in a template: {{'{#'}} For example: ``` {{'{#'}} This is not a Jinja comment } ``` In both cases the example gets rendered as: {# This is not a Jinja comment }","title":"Troubleshooting"},{"location":"background/documentation/troubleshooting/#documentation-troubleshooting","text":"","title":"Documentation Troubleshooting"},{"location":"background/documentation/troubleshooting/#macro-syntax-error-missing-end-of-comment-tag","text":"The macros in the documentation, including in code blocks, is expanded through the mkdocs-macros plugin which uses the Jinja2 template processor. Its template syntax is very advanced, allowing not only to replace simple references, but also more complex statements and comments. In this case, Jinja sees {# and interprets it as a comment. Of course, then it cannot find the end of the comment and produces an error. INFO - [macros] - ERROR # _Macro Syntax Error_ _Line 11 in Markdown file:_ **Missing end of comment tag** ```python {# This is not a Jinja comment } ``` To work around this, wrap the block in a {% raw %} and {% endraw %} tag. For example: {% raw %} ``` {# This is not a Jinja comment } ``` {% endraw %} Alternatively, if you still want to use other macros in the code, wrap the offending character sequence in a template: {{'{#'}} For example: ``` {{'{#'}} This is not a Jinja comment } ``` In both cases the example gets rendered as: {# This is not a Jinja comment }","title":"Macro Syntax Error: Missing end of comment tag"},{"location":"background/statix/","text":"Statix Background \u00b6 rule selection open/closed world reasoning ( try /DWF/DLeq) desugaring of functional rules Internal representation of scope graphs Query Scheduling/Permission to Extend","title":"Statix Background"},{"location":"background/statix/#statix-background","text":"rule selection open/closed world reasoning ( try /DWF/DLeq) desugaring of functional rules Internal representation of scope graphs Query Scheduling/Permission to Extend","title":"Statix Background"},{"location":"background/stratego/","text":"Stratego \u00b6 The Stratego transformation was born from a pure rewriting approach to program transformation by the introduction of traversal combinators 1 and programmable rewriting strategies 2 . Strategic Rewriting \u00b6 This section reviews the classic definition of term rewriting and motivates the transition to strategic rewriting. Term rewriting Limitations of term rewriting Factoring out Traversal Strategic Rewriting Strategy Combinators \u00b6 Rather than defining high-level strategies as primitives, Stratego provides basic strategies combinators for composing strategies. The section in the reference manual provides a definition of all the combinators. Here we expand that description with many examples. Sequential combinators Term combinators Traveral combinators Type unifying traversals Origin Tracking \u00b6 Origin tracking is a term rewriting feature that has been put into Stratego to track connections between terms through a transformation. If you for example parse a file into an abstract syntax tree (AST) and the parser leaves some information about what term refers to what part of the file, you can keep track of that even after transformations. However, origin tracking in Stratego is rather limited, and people are often confused about how it works. On the page for Origin Tracking you can find an explanation of origin tracking, how it works in Stratego, and plans on future improvements. References \u00b6 Bas Luttik and Eelco Visser. Specification of rewriting strategies. In M. P. A. Sellink, editor, 2 nd International Workshop on the Theory and Practice of Algebraic Specifications \\(ASF\\+SDF 1997\\) , Electronic Workshops in Computing. Berlin, November 1997. Springer-Verlag. \u21a9 Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9","title":"Stratego"},{"location":"background/stratego/#stratego","text":"The Stratego transformation was born from a pure rewriting approach to program transformation by the introduction of traversal combinators 1 and programmable rewriting strategies 2 .","title":"Stratego"},{"location":"background/stratego/#strategic-rewriting","text":"This section reviews the classic definition of term rewriting and motivates the transition to strategic rewriting. Term rewriting Limitations of term rewriting Factoring out Traversal Strategic Rewriting","title":"Strategic Rewriting"},{"location":"background/stratego/#strategy-combinators","text":"Rather than defining high-level strategies as primitives, Stratego provides basic strategies combinators for composing strategies. The section in the reference manual provides a definition of all the combinators. Here we expand that description with many examples. Sequential combinators Term combinators Traveral combinators Type unifying traversals","title":"Strategy Combinators"},{"location":"background/stratego/#origin-tracking","text":"Origin tracking is a term rewriting feature that has been put into Stratego to track connections between terms through a transformation. If you for example parse a file into an abstract syntax tree (AST) and the parser leaves some information about what term refers to what part of the file, you can keep track of that even after transformations. However, origin tracking in Stratego is rather limited, and people are often confused about how it works. On the page for Origin Tracking you can find an explanation of origin tracking, how it works in Stratego, and plans on future improvements.","title":"Origin Tracking"},{"location":"background/stratego/#references","text":"Bas Luttik and Eelco Visser. Specification of rewriting strategies. In M. P. A. Sellink, editor, 2 nd International Workshop on the Theory and Practice of Algebraic Specifications \\(ASF\\+SDF 1997\\) , Electronic Workshops in Computing. Berlin, November 1997. Springer-Verlag. \u21a9 Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9","title":"References"},{"location":"background/stratego/origin-tracking/","text":"Origin Tracking \u00b6 Origin tracking is the idea to build an origin relation between input and output terms of a transformation in a term transformation system (TRS). It was first proposed by Van Deursen et al. in 1993 in the eponymous article 1 . The origin relation can be used to traverse from a result term, to the term before the final transformation happened. This relation can be followed further, all the way back to the original input term before all transformations. Examples of uses for this relation (from the paper) are: constructing language-specific debuggers, visualising program execution, and associating positional information with messages in error reports. The first two examples relate to using the TRS to implement evaluation (interpretation) of a programming language, where the abstract syntax tree (AST) of a program is a term that is transformed to execute it. The origin relation allows a debugger or visualiser to show the execution one step at a time. The third example uses the origin relation transitively to reach the original AST of a program and extract positional information from it which was left by the process that created the AST (e.g. a parser or projectional editor). Spoofax, in particular Stratego inside of Spoofax, can also do some origin tracking. This origin relation is used as a one-to-one relation to the original AST only, as it is mostly used for accessing the positional information on the original AST, left there by the parser. Stratego origin tracking as it is implemented today is rather limited and confusing to users. Therefore this page describes how it works today, what the original paper described, and how origin tracking might be improved in Stratego 2. Origin Tracking in ASF \u00b6 Caveat Lector This is a summary based on the first and last sections of the paper. The high-level explanation was intuitive so this is based only on that and not the formal definition in the middle. The TRS used in the paper was the ASF+SDF meta-environment, of which the ASF part, the algebraic specification formalism, was a conditional term transformation system (CRTS). The origin tracking system was a static analysis applied to rewrite rules to find the origin relations between the left-hand side terms and the right-hand side terms. Through a number of rules this system would track relations between contexts, common variables, common subterms, and the redex. Let's quickly go over Common Variables \u00b6 Because of non-linear rewrite rules, e.g. plus(X,X) -> mul(2, X) , an origin relation could be from a term captured by variable X and both the subterms of plus on the left. That's the only special case, otherwise it's just a matter of the thing captured in variables and used on both sides of the rule have the obvious relation. Common Subterms \u00b6 Common subterms are related. This is not only the case of variables, but also for constants (e.g. empty-list in append(E,empty-list) -> cons(E,empty-list) ), and larger combinations (e.g. while(Ezp,S-list) in ev-stat(while(Ezp,S-list) , Env) -> ev-stat (while (Ezp, S-list), ev-list (S-list, Env) ) when ev-exp(Exp, Env) = true ). (Capitalized things are variables in ASF). Redex-Contractum \u00b6 If not related by a previous rule, the left-hand topmost term and righ-hand topmost term are related. For example, the rule real-const(Char-list) -> real-type which returns a type for an expression, relates the type real-type on the right to the expression real-const(...) on the left. Conditional Rewriting \u00b6 Because ASF is a conditional rewrite system, there can be when clauses that bind more variables and deep pattern matches etc. This means that certain deeper terms on the right-hand side may not get an origin, such as the second seq and add in trans(plus(E1,E2)) -> seq(trans(El), seq(trans(E2), add)) . Origin Tracking in Stratego \u00b6 Origin tracking in Stratego is implemented in the runtime. Calling it a dynamic analysis would be generous, it looks more like an afterthought that's hacked in quickly. The place where origin tracking is implemented is the runtime of the all , some and one language constructs. These will make an origin relation between the origin of the term on which the construct is applied, and the result term. This is so the origin relation is always directly to the original AST. This works for tuples and constructor applications, where children are actually changed by the given strategy to apply to the children. The construct also work on lists. There is an extra hack where the origin tracking goes one level deeper in lists (directly implemented in all , implemented in the OriginTermFactory for some and one ). Therefore the Stratego strategy origin-track-forced(s) = ![<id>]; map(s); ?[<id>] adds the \"redex-contractum\" style origin tracking to a strategy s ( map is implemented in terms of all in a performance override for the Java implementation of the Stratego runtime). So, the reason why origin tracking in Stratego seems to be broken in edge-cases that you can never quite remember, is because it is only applied by generic traversals, mapping over lists, and other strategies that internally use all , some , or one . Still, this approach allowed for a very quick addition of origin tracking to Stratego without a need to change the compiled (for the static analysis), and in fact Stratego and its compiler don't even really know of origins as a thing that terms can have. A Proposal for Improved Origin Tracking in Stratego \u00b6 Attempting to replicate the static analysis of ASF for Stratego would be a rather large task that would influence the CTree format between the front-end and back-end of the compiler. That format has been stable for a long time, and changing it could have unexpected effects throughout the codebase of Spoofax. It would also require quite some effort to integrate into both the front-end of the compiler and make the corresponding changes to the back-end, check all the code in between can handle the changed CTree format, etc. And we'll inherit the shortcoming of that analysis, in particular how some terms will still not have origins, and other terms have multiple origins. The first is annoying, and we can possibly overcome it, the second is more dangerous as it breaks the interface Spoofax expects of exactly one origin term. So instead my proposal has the following properties: (1) all terms get an origin, (2) which origin is still fairly easy to understand, (3) the implementation effort is minimized. The downsides are: (1) origins may be a bit suboptimal in some situations, (2) a certain (hypothetical!) optimisation of Stratego becomes more complicated. The proposal: the origin of a newly built term is the current term \u00b6 To avoid the whole static analysis, changing the CTree format, etc, the origin tracking rule needs to be dead simple. This proposal will allow for origin tracking to be implemented in back-end of the compiler without extra analysis and support in the front-end, side-stepping the CTree format changes. This does mean that we've lost the information on what were strategy and what were rules in the source text of the Stratego program. But we want origin tracking to be defined for even the most basic strategy expressions anyway, and this seems like the most reasonable approach there. With minor changes to the code generation code in the Stratego compiler, we can target already available methods of the ITermFactory to set the origin of a newly built term as the current term. Desugared rules will have the correct redex-contractum origin relation. Common variable work as expected due to sharing. In the case of non-linear rewrite rules the leftmost occurrence of the variable is the origin. Common subterms will not get the, perhaps expected, origin relation, instead a subterm will be related to the whole term of the left-hand side. Once this rule is established and understood by Stratego users, they will find code patterns to work around with this. The origin term will still be close to the desired one, just higher up in the tree. Apart from suboptimal origins for subterms sometimes, there is the question of optimisation, really of semantics. In core Stratego, you could have a sequence of two builds. In general we cannot optimise this to only the second build, because the first can fail if it uses an unbound variable. But now that the current term is relevant for any build that constructs a new term (rather than only a variable), there will be more edge-cases to keep in mind when attempting to optimise sequences of builds. This is not a concern that affects any current optimisations in Stratego though. But it should be noted as the current term and origins are not explicitly mentioned anywhere in the AST of a Stratego program. Performance implications \u00b6 Estimated performance impact of the changes proposed above should be minimal, but expectations like that should of course be verified. This will require the construction of a general Stratego benchmark, which we currently do not have available (although there is some source code available from some TRS benchmarking contests). The current API of the ITermFactory would require that each term build would need an extra method call to track origins. This short method is expected to be inlined by the JIT compiler, but it could be expensive. If that turns out to be the case, the ITermFactory interface could be expanded to allow for a single method to build a new term and provide the origin term as well. Some of these methods already exist, but their semantics is rather constrained. Making proper, backward compatible changes that keep bootstrapping in mind should be possible and not particularly challenging. References \u00b6 Arie van Deursen, Paul Klint, and Frank Tip. Origin tracking. Journal of Symbolic Computation , 15 \\(5/6\\) :523\u2013545, 1993. \u21a9","title":"Origin Tracking"},{"location":"background/stratego/origin-tracking/#origin-tracking","text":"Origin tracking is the idea to build an origin relation between input and output terms of a transformation in a term transformation system (TRS). It was first proposed by Van Deursen et al. in 1993 in the eponymous article 1 . The origin relation can be used to traverse from a result term, to the term before the final transformation happened. This relation can be followed further, all the way back to the original input term before all transformations. Examples of uses for this relation (from the paper) are: constructing language-specific debuggers, visualising program execution, and associating positional information with messages in error reports. The first two examples relate to using the TRS to implement evaluation (interpretation) of a programming language, where the abstract syntax tree (AST) of a program is a term that is transformed to execute it. The origin relation allows a debugger or visualiser to show the execution one step at a time. The third example uses the origin relation transitively to reach the original AST of a program and extract positional information from it which was left by the process that created the AST (e.g. a parser or projectional editor). Spoofax, in particular Stratego inside of Spoofax, can also do some origin tracking. This origin relation is used as a one-to-one relation to the original AST only, as it is mostly used for accessing the positional information on the original AST, left there by the parser. Stratego origin tracking as it is implemented today is rather limited and confusing to users. Therefore this page describes how it works today, what the original paper described, and how origin tracking might be improved in Stratego 2.","title":"Origin Tracking"},{"location":"background/stratego/origin-tracking/#origin-tracking-in-asf","text":"Caveat Lector This is a summary based on the first and last sections of the paper. The high-level explanation was intuitive so this is based only on that and not the formal definition in the middle. The TRS used in the paper was the ASF+SDF meta-environment, of which the ASF part, the algebraic specification formalism, was a conditional term transformation system (CRTS). The origin tracking system was a static analysis applied to rewrite rules to find the origin relations between the left-hand side terms and the right-hand side terms. Through a number of rules this system would track relations between contexts, common variables, common subterms, and the redex. Let's quickly go over","title":"Origin Tracking in ASF"},{"location":"background/stratego/origin-tracking/#common-variables","text":"Because of non-linear rewrite rules, e.g. plus(X,X) -> mul(2, X) , an origin relation could be from a term captured by variable X and both the subterms of plus on the left. That's the only special case, otherwise it's just a matter of the thing captured in variables and used on both sides of the rule have the obvious relation.","title":"Common Variables"},{"location":"background/stratego/origin-tracking/#common-subterms","text":"Common subterms are related. This is not only the case of variables, but also for constants (e.g. empty-list in append(E,empty-list) -> cons(E,empty-list) ), and larger combinations (e.g. while(Ezp,S-list) in ev-stat(while(Ezp,S-list) , Env) -> ev-stat (while (Ezp, S-list), ev-list (S-list, Env) ) when ev-exp(Exp, Env) = true ). (Capitalized things are variables in ASF).","title":"Common Subterms"},{"location":"background/stratego/origin-tracking/#redex-contractum","text":"If not related by a previous rule, the left-hand topmost term and righ-hand topmost term are related. For example, the rule real-const(Char-list) -> real-type which returns a type for an expression, relates the type real-type on the right to the expression real-const(...) on the left.","title":"Redex-Contractum"},{"location":"background/stratego/origin-tracking/#conditional-rewriting","text":"Because ASF is a conditional rewrite system, there can be when clauses that bind more variables and deep pattern matches etc. This means that certain deeper terms on the right-hand side may not get an origin, such as the second seq and add in trans(plus(E1,E2)) -> seq(trans(El), seq(trans(E2), add)) .","title":"Conditional Rewriting"},{"location":"background/stratego/origin-tracking/#origin-tracking-in-stratego","text":"Origin tracking in Stratego is implemented in the runtime. Calling it a dynamic analysis would be generous, it looks more like an afterthought that's hacked in quickly. The place where origin tracking is implemented is the runtime of the all , some and one language constructs. These will make an origin relation between the origin of the term on which the construct is applied, and the result term. This is so the origin relation is always directly to the original AST. This works for tuples and constructor applications, where children are actually changed by the given strategy to apply to the children. The construct also work on lists. There is an extra hack where the origin tracking goes one level deeper in lists (directly implemented in all , implemented in the OriginTermFactory for some and one ). Therefore the Stratego strategy origin-track-forced(s) = ![<id>]; map(s); ?[<id>] adds the \"redex-contractum\" style origin tracking to a strategy s ( map is implemented in terms of all in a performance override for the Java implementation of the Stratego runtime). So, the reason why origin tracking in Stratego seems to be broken in edge-cases that you can never quite remember, is because it is only applied by generic traversals, mapping over lists, and other strategies that internally use all , some , or one . Still, this approach allowed for a very quick addition of origin tracking to Stratego without a need to change the compiled (for the static analysis), and in fact Stratego and its compiler don't even really know of origins as a thing that terms can have.","title":"Origin Tracking in Stratego"},{"location":"background/stratego/origin-tracking/#a-proposal-for-improved-origin-tracking-in-stratego","text":"Attempting to replicate the static analysis of ASF for Stratego would be a rather large task that would influence the CTree format between the front-end and back-end of the compiler. That format has been stable for a long time, and changing it could have unexpected effects throughout the codebase of Spoofax. It would also require quite some effort to integrate into both the front-end of the compiler and make the corresponding changes to the back-end, check all the code in between can handle the changed CTree format, etc. And we'll inherit the shortcoming of that analysis, in particular how some terms will still not have origins, and other terms have multiple origins. The first is annoying, and we can possibly overcome it, the second is more dangerous as it breaks the interface Spoofax expects of exactly one origin term. So instead my proposal has the following properties: (1) all terms get an origin, (2) which origin is still fairly easy to understand, (3) the implementation effort is minimized. The downsides are: (1) origins may be a bit suboptimal in some situations, (2) a certain (hypothetical!) optimisation of Stratego becomes more complicated.","title":"A Proposal for Improved Origin Tracking in Stratego"},{"location":"background/stratego/origin-tracking/#the-proposal-the-origin-of-a-newly-built-term-is-the-current-term","text":"To avoid the whole static analysis, changing the CTree format, etc, the origin tracking rule needs to be dead simple. This proposal will allow for origin tracking to be implemented in back-end of the compiler without extra analysis and support in the front-end, side-stepping the CTree format changes. This does mean that we've lost the information on what were strategy and what were rules in the source text of the Stratego program. But we want origin tracking to be defined for even the most basic strategy expressions anyway, and this seems like the most reasonable approach there. With minor changes to the code generation code in the Stratego compiler, we can target already available methods of the ITermFactory to set the origin of a newly built term as the current term. Desugared rules will have the correct redex-contractum origin relation. Common variable work as expected due to sharing. In the case of non-linear rewrite rules the leftmost occurrence of the variable is the origin. Common subterms will not get the, perhaps expected, origin relation, instead a subterm will be related to the whole term of the left-hand side. Once this rule is established and understood by Stratego users, they will find code patterns to work around with this. The origin term will still be close to the desired one, just higher up in the tree. Apart from suboptimal origins for subterms sometimes, there is the question of optimisation, really of semantics. In core Stratego, you could have a sequence of two builds. In general we cannot optimise this to only the second build, because the first can fail if it uses an unbound variable. But now that the current term is relevant for any build that constructs a new term (rather than only a variable), there will be more edge-cases to keep in mind when attempting to optimise sequences of builds. This is not a concern that affects any current optimisations in Stratego though. But it should be noted as the current term and origins are not explicitly mentioned anywhere in the AST of a Stratego program.","title":"The proposal: the origin of a newly built term is the current term"},{"location":"background/stratego/origin-tracking/#performance-implications","text":"Estimated performance impact of the changes proposed above should be minimal, but expectations like that should of course be verified. This will require the construction of a general Stratego benchmark, which we currently do not have available (although there is some source code available from some TRS benchmarking contests). The current API of the ITermFactory would require that each term build would need an extra method call to track origins. This short method is expected to be inlined by the JIT compiler, but it could be expensive. If that turns out to be the case, the ITermFactory interface could be expanded to allow for a single method to build a new term and provide the origin term as well. Some of these methods already exist, but their semantics is rather constrained. Making proper, backward compatible changes that keep bootstrapping in mind should be possible and not particularly challenging.","title":"Performance implications"},{"location":"background/stratego/origin-tracking/#references","text":"Arie van Deursen, Paul Klint, and Frank Tip. Origin tracking. Journal of Symbolic Computation , 15 \\(5/6\\) :523\u2013545, 1993. \u21a9","title":"References"},{"location":"background/stratego/strategic-rewriting/limitations-of-rewriting/","text":"Limitations of Rewriting \u00b6 Term rewriting can be used to implement transformations on programs represented by means of terms. Term rewriting involves exhaustively applying rules to subterms until no more rules apply. This requires a strategy for selecting the order in which subterms are rewritten. The innermost strategy applies rules automatically throughout a term from inner to outer terms, starting with the leaves. The nice thing about term rewriting is that there is no need to define traversals over the syntax tree; the rules express basic transformation steps and the strategy takes care of applying it everywhere. However, the complete normalization approach of rewriting turns out not to be adequate for program transformation, because rewrite systems for programming languages will often be non-terminating and/or non-confluent. In general, it is not desirable to apply all rules at the same time or to apply all rules under all circumstances. Consider for example, the following extension of prop-dnf-rules with distribution rules to achieve conjunctive normal forms: module prop-cnf imports prop-eval-rules rules E : Or ( And ( x , y ), z ) - > And ( Or ( x , z ), Or ( y , z )) E : Or ( z , And ( x , y )) - > And ( Or ( z , x ), Or ( z , y )) strategies cnf = innermost ( E ) This rewrite system is non-terminating because after applying one of the and-over-or distribution rules, the or-over-and distribution rules introduced here can be applied, and vice versa. And ( Or ( Atom ( \"p\" ), Atom ( \"q\" )), Atom ( \"r\" )) - > Or ( And ( Atom ( \"p\" ), Atom ( \"r\" )), And ( Atom ( \"q\" ), Atom ( \"r\" ))) - > And ( Or ( Atom ( \"p\" ), And ( Atom ( \"q\" ), Atom ( \"r\" ))), Or ( Atom ( \"r\" ), And ( Atom ( \"q\" ), Atom ( \"r\" )))) - > ... There are a number of solutions to this problem. We will first discuss a couple of solutions within pure rewriting, and then show how programmable rewriting strategies can overcome the problems of these solutions. Attempt 1: Remodularization \u00b6 The non-termination of prop-cnf is due to the fact that the and-over-or and or-over-and distribution rules interfere with each other. This can be prevented by refactoring the module structure such that the two sets of rules are not present in the same rewrite system. For example, we could split module prop-dnf-rules into prop-simplify and prop-dnf2 as follows: module prop-simplify imports prop-eval-rules rules E : Impl ( x , y ) - > Or ( Not ( x ), y ) E : Eq ( x , y ) - > And ( Impl ( x , y ), Impl ( y , x )) E : Not ( Not ( x )) - > x E : Not ( And ( x , y )) - > Or ( Not ( x ), Not ( y )) E : Not ( Or ( x , y )) - > And ( Not ( x ), Not ( y )) module prop-dnf2 imports prop-simplify rules E : And ( Or ( x , y ), z ) - > Or ( And ( x , z ), And ( y , z )) E : And ( z , Or ( x , y )) - > Or ( And ( z , x ), And ( z , y )) strategies dnf = innermost ( E ) Now we can reuse the rules from prop-simplify without the and-over-or distribution rules to create a prop-cnf2 for normalizing to conjunctive normal form: module prop-cnf2 imports prop-simplify rules E : Or ( And ( x , y ), z ) - > And ( Or ( x , z ), Or ( y , z )) E : Or ( z , And ( x , y )) - > And ( Or ( z , x ), Or ( z , y )) strategies cnf = innermost ( E ) Although this solves the non-termination problem, it is not an ideal solution. In the first place it is not possible to apply the two transformations in the same program. In the second place, extrapolating the approach to fine-grained selection of rules might require definition of a single rule per module. Attempt 2: Functionalization \u00b6 Another common solution to this kind of problem is to introduce additional constructors that achieve normalization under a restricted set of rules. That is, the original set of rules p1 -> p2 is transformed into rules of the form f(p_1) -> p_2' , where f is some new constructor symbol and the right-hand side of the rule also contains such new constructors. In this style of programming, constructors such as f are called functions and are distinguished from constructors. Normal forms over such rewrite systems are assumed to be free of these function symbols; otherwise the function would have an incomplete definition. To illustrate the approach we adapt the DNF rules by introducing the function symbols Dnf and DnfR . (We ignore the evaluation rules in this example.) module prop-dnf3 imports libstrategolib prop signature constructors Dnf : Prop - > Prop DnfR : Prop - > Prop rules E : Dnf ( Atom ( x )) - > Atom ( x ) E : Dnf ( Not ( x )) - > DnfR ( Not ( Dnf ( x ))) E : Dnf ( And ( x , y )) - > DnfR ( And ( Dnf ( x ), Dnf ( y ))) E : Dnf ( Or ( x , y )) - > Or ( Dnf ( x ), Dnf ( y )) E : Dnf ( Impl ( x , y )) - > Dnf ( Or ( Not ( x ), y )) E : Dnf ( Eq ( x , y )) - > Dnf ( And ( Impl ( x , y ), Impl ( y , x ))) E : DnfR ( Not ( Not ( x ))) - > x E : DnfR ( Not ( And ( x , y ))) - > Or ( Dnf ( Not ( x )), Dnf ( Not ( y ))) E : DnfR ( Not ( Or ( x , y ))) - > Dnf ( And ( Not ( x ), Not ( y ))) D : DnfR ( Not ( x )) - > Not ( x ) E : DnfR ( And ( Or ( x , y ), z )) - > Or ( Dnf ( And ( x , z )), Dnf ( And ( y , z ))) E : DnfR ( And ( z , Or ( x , y ))) - > Or ( Dnf ( And ( z , x )), Dnf ( And ( z , y ))) D : DnfR ( And ( x , y )) - > And ( x , y ) strategies dnf = innermost ( E < + D ) The Dnf function mimics the innermost normalization strategy by recursively traversing terms. The auxiliary transformation function DnfR is used to encode the distribution and negation rules. The D rules are default rules that are only applied if none of the E rules apply, as specified by the strategy expression E <+ D . In order to compute the disjunctive normal form of a term, we have to apply the Dnf function to it, as illustrated in the following application of the prop-dnf3 program: < dnf > Dnf ( And ( Impl ( Atom ( \"r\" ), And ( Atom ( \"p\" ), Atom ( \"q\" ))), Atom ( \"p\" ))) => Or ( And ( Not ( Atom ( \"r\" )), Atom ( \"p\" )), And ( And ( Atom ( \"p\" ), Atom ( \"q\" )), Atom ( \"p\" ))) Intermezzo: DNF in Spoofax/Eclipse (move to tutorial?) \u00b6 If you\u2019re going to try to run this example in Spoofax/Eclipse, a few words of caution. First, it\u2019s easiest to just accumulate all of the different test modules as imports in your main language \u201c.str\u201d file. But if you do that, all of the rules will be in the same namespace. So you\u2019re going to want to use different identifiers (say E3 and D3 ) in place of E and D in your prop-dnf3.str file. Also, the concrete syntax has no way to represent the \u201cextra\u201d function symbol Dnf that is used here, so you\u2019ll want to use alternate triggering strategies like make-nf = innermost ( E3 < + D3 ) dnf3 : x - > < make-nf > Dnf ( x ) that wrap the input in Dnf( ... ) themselves. For conjunctive normal form we can create a similar definition, which can now co-exist with the definition of DNF . Indeed, we could then simultaneously rewrite one subterm to DNF and the other to CNF . E : DC ( x ) - > ( Dnf ( x ), Cnf ( x )) Evaluation \u00b6 In the solution above, the original rules have been completely intertwined with the Dnf transformation. The rules for negation cannot be reused in the definition of normalization to conjunctive normal form. For each new transformation a new traversal function and new transformation functions have to be defined. Many additional rules had to be added to traverse the term to find the places to apply the rules. In the modular solution we had 5 basic rules and 2 additional rules for DNF and 2 rules for CNF, 9 in total. In the functionalized version we needed 13 rules for each transformation, that is 26 rules in total.","title":"Limitations of Rewriting"},{"location":"background/stratego/strategic-rewriting/limitations-of-rewriting/#limitations-of-rewriting","text":"Term rewriting can be used to implement transformations on programs represented by means of terms. Term rewriting involves exhaustively applying rules to subterms until no more rules apply. This requires a strategy for selecting the order in which subterms are rewritten. The innermost strategy applies rules automatically throughout a term from inner to outer terms, starting with the leaves. The nice thing about term rewriting is that there is no need to define traversals over the syntax tree; the rules express basic transformation steps and the strategy takes care of applying it everywhere. However, the complete normalization approach of rewriting turns out not to be adequate for program transformation, because rewrite systems for programming languages will often be non-terminating and/or non-confluent. In general, it is not desirable to apply all rules at the same time or to apply all rules under all circumstances. Consider for example, the following extension of prop-dnf-rules with distribution rules to achieve conjunctive normal forms: module prop-cnf imports prop-eval-rules rules E : Or ( And ( x , y ), z ) - > And ( Or ( x , z ), Or ( y , z )) E : Or ( z , And ( x , y )) - > And ( Or ( z , x ), Or ( z , y )) strategies cnf = innermost ( E ) This rewrite system is non-terminating because after applying one of the and-over-or distribution rules, the or-over-and distribution rules introduced here can be applied, and vice versa. And ( Or ( Atom ( \"p\" ), Atom ( \"q\" )), Atom ( \"r\" )) - > Or ( And ( Atom ( \"p\" ), Atom ( \"r\" )), And ( Atom ( \"q\" ), Atom ( \"r\" ))) - > And ( Or ( Atom ( \"p\" ), And ( Atom ( \"q\" ), Atom ( \"r\" ))), Or ( Atom ( \"r\" ), And ( Atom ( \"q\" ), Atom ( \"r\" )))) - > ... There are a number of solutions to this problem. We will first discuss a couple of solutions within pure rewriting, and then show how programmable rewriting strategies can overcome the problems of these solutions.","title":"Limitations of Rewriting"},{"location":"background/stratego/strategic-rewriting/limitations-of-rewriting/#attempt-1-remodularization","text":"The non-termination of prop-cnf is due to the fact that the and-over-or and or-over-and distribution rules interfere with each other. This can be prevented by refactoring the module structure such that the two sets of rules are not present in the same rewrite system. For example, we could split module prop-dnf-rules into prop-simplify and prop-dnf2 as follows: module prop-simplify imports prop-eval-rules rules E : Impl ( x , y ) - > Or ( Not ( x ), y ) E : Eq ( x , y ) - > And ( Impl ( x , y ), Impl ( y , x )) E : Not ( Not ( x )) - > x E : Not ( And ( x , y )) - > Or ( Not ( x ), Not ( y )) E : Not ( Or ( x , y )) - > And ( Not ( x ), Not ( y )) module prop-dnf2 imports prop-simplify rules E : And ( Or ( x , y ), z ) - > Or ( And ( x , z ), And ( y , z )) E : And ( z , Or ( x , y )) - > Or ( And ( z , x ), And ( z , y )) strategies dnf = innermost ( E ) Now we can reuse the rules from prop-simplify without the and-over-or distribution rules to create a prop-cnf2 for normalizing to conjunctive normal form: module prop-cnf2 imports prop-simplify rules E : Or ( And ( x , y ), z ) - > And ( Or ( x , z ), Or ( y , z )) E : Or ( z , And ( x , y )) - > And ( Or ( z , x ), Or ( z , y )) strategies cnf = innermost ( E ) Although this solves the non-termination problem, it is not an ideal solution. In the first place it is not possible to apply the two transformations in the same program. In the second place, extrapolating the approach to fine-grained selection of rules might require definition of a single rule per module.","title":"Attempt 1: Remodularization"},{"location":"background/stratego/strategic-rewriting/limitations-of-rewriting/#attempt-2-functionalization","text":"Another common solution to this kind of problem is to introduce additional constructors that achieve normalization under a restricted set of rules. That is, the original set of rules p1 -> p2 is transformed into rules of the form f(p_1) -> p_2' , where f is some new constructor symbol and the right-hand side of the rule also contains such new constructors. In this style of programming, constructors such as f are called functions and are distinguished from constructors. Normal forms over such rewrite systems are assumed to be free of these function symbols; otherwise the function would have an incomplete definition. To illustrate the approach we adapt the DNF rules by introducing the function symbols Dnf and DnfR . (We ignore the evaluation rules in this example.) module prop-dnf3 imports libstrategolib prop signature constructors Dnf : Prop - > Prop DnfR : Prop - > Prop rules E : Dnf ( Atom ( x )) - > Atom ( x ) E : Dnf ( Not ( x )) - > DnfR ( Not ( Dnf ( x ))) E : Dnf ( And ( x , y )) - > DnfR ( And ( Dnf ( x ), Dnf ( y ))) E : Dnf ( Or ( x , y )) - > Or ( Dnf ( x ), Dnf ( y )) E : Dnf ( Impl ( x , y )) - > Dnf ( Or ( Not ( x ), y )) E : Dnf ( Eq ( x , y )) - > Dnf ( And ( Impl ( x , y ), Impl ( y , x ))) E : DnfR ( Not ( Not ( x ))) - > x E : DnfR ( Not ( And ( x , y ))) - > Or ( Dnf ( Not ( x )), Dnf ( Not ( y ))) E : DnfR ( Not ( Or ( x , y ))) - > Dnf ( And ( Not ( x ), Not ( y ))) D : DnfR ( Not ( x )) - > Not ( x ) E : DnfR ( And ( Or ( x , y ), z )) - > Or ( Dnf ( And ( x , z )), Dnf ( And ( y , z ))) E : DnfR ( And ( z , Or ( x , y ))) - > Or ( Dnf ( And ( z , x )), Dnf ( And ( z , y ))) D : DnfR ( And ( x , y )) - > And ( x , y ) strategies dnf = innermost ( E < + D ) The Dnf function mimics the innermost normalization strategy by recursively traversing terms. The auxiliary transformation function DnfR is used to encode the distribution and negation rules. The D rules are default rules that are only applied if none of the E rules apply, as specified by the strategy expression E <+ D . In order to compute the disjunctive normal form of a term, we have to apply the Dnf function to it, as illustrated in the following application of the prop-dnf3 program: < dnf > Dnf ( And ( Impl ( Atom ( \"r\" ), And ( Atom ( \"p\" ), Atom ( \"q\" ))), Atom ( \"p\" ))) => Or ( And ( Not ( Atom ( \"r\" )), Atom ( \"p\" )), And ( And ( Atom ( \"p\" ), Atom ( \"q\" )), Atom ( \"p\" )))","title":"Attempt 2: Functionalization"},{"location":"background/stratego/strategic-rewriting/limitations-of-rewriting/#intermezzo-dnf-in-spoofaxeclipse-move-to-tutorial","text":"If you\u2019re going to try to run this example in Spoofax/Eclipse, a few words of caution. First, it\u2019s easiest to just accumulate all of the different test modules as imports in your main language \u201c.str\u201d file. But if you do that, all of the rules will be in the same namespace. So you\u2019re going to want to use different identifiers (say E3 and D3 ) in place of E and D in your prop-dnf3.str file. Also, the concrete syntax has no way to represent the \u201cextra\u201d function symbol Dnf that is used here, so you\u2019ll want to use alternate triggering strategies like make-nf = innermost ( E3 < + D3 ) dnf3 : x - > < make-nf > Dnf ( x ) that wrap the input in Dnf( ... ) themselves. For conjunctive normal form we can create a similar definition, which can now co-exist with the definition of DNF . Indeed, we could then simultaneously rewrite one subterm to DNF and the other to CNF . E : DC ( x ) - > ( Dnf ( x ), Cnf ( x ))","title":"Intermezzo: DNF in Spoofax/Eclipse (move to tutorial?)"},{"location":"background/stratego/strategic-rewriting/limitations-of-rewriting/#evaluation","text":"In the solution above, the original rules have been completely intertwined with the Dnf transformation. The rules for negation cannot be reused in the definition of normalization to conjunctive normal form. For each new transformation a new traversal function and new transformation functions have to be defined. Many additional rules had to be added to traverse the term to find the places to apply the rules. In the modular solution we had 5 basic rules and 2 additional rules for DNF and 2 rules for CNF, 9 in total. In the functionalized version we needed 13 rules for each transformation, that is 26 rules in total.","title":"Evaluation"},{"location":"background/stratego/strategic-rewriting/strategic-rewriting/","text":"Strategic Rewriting \u00b6 Limitations of Term Rewriting \u00b6 Term rewriting involves exhaustively applying rules to subterms until no more rules apply. This requires a strategy for selecting the order in which subterms are rewritten. The innermost strategy applies rules automatically throughout a term from inner to outer terms, starting with the leaves. The nice thing about term rewriting is that there is no need to define traversals over the syntax tree; the rules express basic transformation steps and the strategy takes care of applying it everywhere. However, the complete normalization approach of rewriting turns out not to be adequate for program transformation, because rewrite systems for programming languages will often be non-terminating and/or non-confluent. In general, it is not desirable to apply all rules at the same time or to apply all rules under all circumstances. The usual solution is to encode the strategy in the rewrite rules . But this intertwines the strategy with the rules, and makes the latter unreusable. Programmable Rewriting Strategies \u00b6 In general, there are two problems with the functional approach to encoding the control over the application of rewrite rules, when comparing it to the original term rewriting approach: traversal overhead and loss of separation of rules and strategies. In the first place, the functional encoding incurs a large overhead due to the explicit specification of traversal. In pure term rewriting, the strategy takes care of traversing the term in search of subterms to rewrite. In the functional approach traversal is spelled out in the definition of the function, requiring the specification of many additional rules. A traversal rule needs to be defined for each constructor in the signature and for each transformation. The overhead for transformation systems for real languages can be inferred from the number of constructors for some typical languages: language : constructors Tiger : 65 C : 140 Java : 140 COBOL : 300 - 1200 In the second place, rewrite rules and the strategy that defines their application are completely intertwined. Another advantage of pure term rewriting is the separation of the specification of the rules and the strategy that controls their application. Intertwining these specifications makes it more difficult to understand the specification, since rules cannot be distinguished from the transformation they are part of. Furthermore, intertwining makes it impossible to reuse the rules in a different transformation. Stratego introduces the paradigm of programmable rewriting strategies with generic traversals, a unifying solution in which application of rules can be carefully controlled, while incurring minimal traversal overhead and preserving separation of rules and strategies 1 . The following are the design criteria for strategies in Stratego: Separation of rules and strategy: Basic transformation rules can be defined separately from the strategy that applies them, such that they can be understood independently. Rule selection: A transformation can select the necessary set of rules from a collection (library) of rules. Control: A transformation can exercise complete control over the application of rules. This control may be fine-grained or course-grained depending on the application. No traversal overhead: Transformations can be defined without overhead for the definition of traversals. Reuse of rules: Rules can be reused in different transformations. Reuse of traversal schemas: Traversal schemas can be defined generically and reused in different transformations. Idioms of Strategic Rewriting \u00b6 We will examine the language constructs that Stratego provides for programming with strategies, starting with the low-level actions of building and matching terms. To get a feeling for the purpose of these constructs, we first look at a couple of typical idioms of strategic rewriting. Cascading Transformations \u00b6 The basic idiom of program transformation achieved with term rewriting is that of cascading transformations. Instead of applying a single complex transformation algorithm to a program, a number of small, independent transformations are applied in combination throughout a program or program unit to achieve the desired effect. Although each individual transformation step achieves little, the cumulative effect can be significant, since each transformation feeds on the results of the ones that came before it. One common cascading of transformations is accomplished by exhaustively applying rewrite rules to a subject term. In Stratego the definition of a cascading normalization strategy with respect to rules R1 , \u2026 , Rn can be formalized using the innermost strategy that we saw before: simplify = innermost ( R1 < + ... < + Rn ) The argument strategy of innermost is a selection of rules. By giving different names to rules, we can control the selection used in each transformation. There can be multiple applications of innermost to different sets of rules, such that different transformations can co-exist in the same module without interference. Thus, it is now possible to develop a large library of transformation rules that can be called upon when necessary, without having to compose a rewrite system by cutting and pasting. For example, the following module defines the normalization of proposition formulae to both disjunctive and to conjunctive normal form: module prop-laws imports prop rules DefI : Impl ( x , y ) - > Or ( Not ( x ), y ) DefE : Eq ( x , y ) - > And ( Impl ( x , y ), Impl ( y , x )) DN : Not ( Not ( x )) - > x DMA : Not ( And ( x , y )) - > Or ( Not ( x ), Not ( y )) DMO : Not ( Or ( x , y )) - > And ( Not ( x ), Not ( y )) DAOL : And ( Or ( x , y ), z ) - > Or ( And ( x , z ), And ( y , z )) DAOR : And ( z , Or ( x , y )) - > Or ( And ( z , x ), And ( z , y )) DOAL : Or ( And ( x , y ), z ) - > And ( Or ( x , z ), Or ( y , z )) DOAR : Or ( z , And ( x , y )) - > And ( Or ( z , x ), Or ( z , y )) strategies dnf = innermost ( DefI < + DefE < + DAOL < + DAOR < + DN < + DMA < + DMO ) cnf = innermost ( DefI < + DefE < + DOAL < + DOAR < + DN < + DMA < + DMO ) The rules are named, and for each strategy different selections from the rule set are made. One-pass Traversals \u00b6 Cascading transformations can be defined with other strategies as well, and these strategies need not be exhaustive, but can be simpler one-pass traversals. For example, constant folding of Boolean expressions only requires a simple one-pass bottom-up traversal. This can be achieved using the bottomup strategy according the following scheme: simplify = bottomup ( repeat ( R1 < + ... < + Rn )) The bottomup strategy applies its argument strategy to each subterm in a bottom-to-top traversal. The repeat strategy applies its argument strategy repeatedly to a term. Module prop-eval2 defines the evaluation rules for Boolean expressions and a strategy for applying them using this approach: module prop-eval2 imports libstrategolib prop rules Eval : Not ( True ()) - > False () Eval : Not ( False ()) - > True () Eval : And ( True (), x ) - > x Eval : And ( x , True ()) - > x Eval : And ( False (), x ) - > False () Eval : And ( x , False ()) - > False () Eval : Or ( True (), x ) - > True () Eval : Or ( x , True ()) - > True () Eval : Or ( False (), x ) - > x Eval : Or ( x , False ()) - > x Eval : Impl ( True (), x ) - > x Eval : Impl ( x , True ()) - > True () Eval : Impl ( False (), x ) - > True () Eval : Impl ( x , False ()) - > Not ( x ) Eval : Eq ( False (), x ) - > Not ( x ) Eval : Eq ( x , False ()) - > Not ( x ) Eval : Eq ( True (), x ) - > x Eval : Eq ( x , True ()) - > x strategies main = io-wrap ( eval ) eval = bottomup ( repeat ( Eval )) The strategy eval applies these rules in a bottom-up traversal over a term, using the bottomup(s) strategy. At each sub-term, the rules are applied repeatedly until no more rule applies using the repeat(s) strategy. This is sufficient for the Eval rules, since the rules never construct a term with subterms that can be rewritten. Another typical example of the use of one-pass traversals is desugaring, that is rewriting language constructs to more basic language constructs. Simple desugarings can usually be expressed using a single top-to-bottom traversal according to the scheme simplify = topdown ( try ( R1 < + ... < + Rn )) The topdown strategy applies its argument strategy to a term and then traverses the resulting term. The try strategy tries to apply its argument strategy once to a term. Module prop-desugar defines a number of desugaring rules for Boolean expressions, defining propositional operators in terms of others. For example, rule DefN defines Not in terms of Impl , and rule DefI defines Impl in terms of Or and Not . So not all rules should be applied in the same transformation or non-termination would result. module prop-desugar imports prop libstrategolib rules DefN : Not ( x ) - > Impl ( x , False ()) DefI : Impl ( x , y ) - > Or ( Not ( x ), y ) DefE : Eq ( x , y ) - > And ( Impl ( x , y ), Impl ( y , x )) DefO1 : Or ( x , y ) - > Impl ( Not ( x ), y ) DefO2 : Or ( x , y ) - > Not ( And ( Not ( x ), Not ( y ))) DefA1 : And ( x , y ) - > Not ( Or ( Not ( x ), Not ( y ))) DefA2 : And ( x , y ) - > Not ( Impl ( x , Not ( y ))) IDefI : Or ( Not ( x ), y ) - > Impl ( x , y ) IDefE : And ( Impl ( x , y ), Impl ( y , x )) - > Eq ( x , y ) strategies desugar = topdown ( try ( DefI < + DefE )) impl-nf = topdown ( repeat ( DefN < + DefA2 < + DefO1 < + DefE )) main-desugar = io-wrap ( desugar ) main-inf = io-wrap ( impl-nf ) The strategies desugar and impl-nf define two different desugaring transformation based on these rules. The desugar strategy gets rid of the implication and equivalence operators, while the impl-nf strategy reduces an expression to implicative normal-form, a format in which only implication ( Impl ) and False() are used. A final example of a one-pass traversal is the downup strategy, which applies its argument transformation during a traversal on the way down, and again on the way up: simplify = downup ( repeat ( R1 < + ... < + Rn )) An application of this strategy is a more efficient implementation of constant folding for Boolean expressions: eval = downup ( repeat ( Eval )) This strategy reduces terms such as And ( ... big expression ... , False ()) in one step (to False() in this case), while the bottomup strategy defined above would first evaluate the big expression. Staged Transformations \u00b6 Cascading transformations apply a number of rules one after another to an entire tree. But in some cases this is not appropriate. For instance, two transformations may be inverses of one another, so that repeatedly applying one and then the other would lead to non-termination. To remedy this difficulty, Stratego supports the idiom of staged transformation. In staged computation, transformations are not applied to a subject term all at once, but rather in stages. In each stage, only rules from some particular subset of the entire set of available rules are applied. In the TAMPR program transformation system this idiom is called sequence of normal forms, since a program tree is transformed in a sequence of steps, each of which performs a normalization with respect to a specified set of rules. In Stratego this idiom can be expressed directly according to the following scheme: strategies simplify = innermost ( A1 < + ... < + Ak ) ; innermost ( B1 < + ... < + Bl ) ; ... ; innermost ( C1 < + ... < + Cm ) Local Transformations \u00b6 In conventional program optimization, transformations are applied throughout a program. In optimizing imperative programs, for example, complex transformations are applied to entire programs. In GHC-style compilation-by-transformation, small transformation steps are applied throughout programs. Another style of transformation is a mixture of these ideas. Instead of applying a complex transformation algorithm to a program we use staged, cascading transformations to accumulate small transformation steps for large effect. However, instead of applying transformations throughout the subject program, we often wish to apply them locally, i.e., only to selected parts of the subject program. This allows us to use transformations rules that would not be beneficial if applied everywhere. One example of a strategy which achieves such a transformation is strategies transformation = alltd ( trigger-transformation ; innermost ( A1 < + ... < + An ) ) The strategy alltd(s) descends into a term until a subterm is encountered for which the transformation s succeeds. In this case the strategy trigger-transformation recognizes a program fragment that should be transformed. Thus, cascading transformations are applied locally to terms for which the transformation is triggered. Of course more sophisticated strategies can be used for finding application locations, as well as for applying the rules locally. Nevertheless, the key observation underlying this idiom remains: Because the transformations to be applied are local, special knowledge about the subject program at the point of application can be used. This allows the application of rules that would not be otherwise applicable. References \u00b6 Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9","title":"Strategic Rewriting"},{"location":"background/stratego/strategic-rewriting/strategic-rewriting/#strategic-rewriting","text":"","title":"Strategic Rewriting"},{"location":"background/stratego/strategic-rewriting/strategic-rewriting/#limitations-of-term-rewriting","text":"Term rewriting involves exhaustively applying rules to subterms until no more rules apply. This requires a strategy for selecting the order in which subterms are rewritten. The innermost strategy applies rules automatically throughout a term from inner to outer terms, starting with the leaves. The nice thing about term rewriting is that there is no need to define traversals over the syntax tree; the rules express basic transformation steps and the strategy takes care of applying it everywhere. However, the complete normalization approach of rewriting turns out not to be adequate for program transformation, because rewrite systems for programming languages will often be non-terminating and/or non-confluent. In general, it is not desirable to apply all rules at the same time or to apply all rules under all circumstances. The usual solution is to encode the strategy in the rewrite rules . But this intertwines the strategy with the rules, and makes the latter unreusable.","title":"Limitations of Term Rewriting"},{"location":"background/stratego/strategic-rewriting/strategic-rewriting/#programmable-rewriting-strategies","text":"In general, there are two problems with the functional approach to encoding the control over the application of rewrite rules, when comparing it to the original term rewriting approach: traversal overhead and loss of separation of rules and strategies. In the first place, the functional encoding incurs a large overhead due to the explicit specification of traversal. In pure term rewriting, the strategy takes care of traversing the term in search of subterms to rewrite. In the functional approach traversal is spelled out in the definition of the function, requiring the specification of many additional rules. A traversal rule needs to be defined for each constructor in the signature and for each transformation. The overhead for transformation systems for real languages can be inferred from the number of constructors for some typical languages: language : constructors Tiger : 65 C : 140 Java : 140 COBOL : 300 - 1200 In the second place, rewrite rules and the strategy that defines their application are completely intertwined. Another advantage of pure term rewriting is the separation of the specification of the rules and the strategy that controls their application. Intertwining these specifications makes it more difficult to understand the specification, since rules cannot be distinguished from the transformation they are part of. Furthermore, intertwining makes it impossible to reuse the rules in a different transformation. Stratego introduces the paradigm of programmable rewriting strategies with generic traversals, a unifying solution in which application of rules can be carefully controlled, while incurring minimal traversal overhead and preserving separation of rules and strategies 1 . The following are the design criteria for strategies in Stratego: Separation of rules and strategy: Basic transformation rules can be defined separately from the strategy that applies them, such that they can be understood independently. Rule selection: A transformation can select the necessary set of rules from a collection (library) of rules. Control: A transformation can exercise complete control over the application of rules. This control may be fine-grained or course-grained depending on the application. No traversal overhead: Transformations can be defined without overhead for the definition of traversals. Reuse of rules: Rules can be reused in different transformations. Reuse of traversal schemas: Traversal schemas can be defined generically and reused in different transformations.","title":"Programmable Rewriting Strategies"},{"location":"background/stratego/strategic-rewriting/strategic-rewriting/#idioms-of-strategic-rewriting","text":"We will examine the language constructs that Stratego provides for programming with strategies, starting with the low-level actions of building and matching terms. To get a feeling for the purpose of these constructs, we first look at a couple of typical idioms of strategic rewriting.","title":"Idioms of Strategic Rewriting"},{"location":"background/stratego/strategic-rewriting/strategic-rewriting/#cascading-transformations","text":"The basic idiom of program transformation achieved with term rewriting is that of cascading transformations. Instead of applying a single complex transformation algorithm to a program, a number of small, independent transformations are applied in combination throughout a program or program unit to achieve the desired effect. Although each individual transformation step achieves little, the cumulative effect can be significant, since each transformation feeds on the results of the ones that came before it. One common cascading of transformations is accomplished by exhaustively applying rewrite rules to a subject term. In Stratego the definition of a cascading normalization strategy with respect to rules R1 , \u2026 , Rn can be formalized using the innermost strategy that we saw before: simplify = innermost ( R1 < + ... < + Rn ) The argument strategy of innermost is a selection of rules. By giving different names to rules, we can control the selection used in each transformation. There can be multiple applications of innermost to different sets of rules, such that different transformations can co-exist in the same module without interference. Thus, it is now possible to develop a large library of transformation rules that can be called upon when necessary, without having to compose a rewrite system by cutting and pasting. For example, the following module defines the normalization of proposition formulae to both disjunctive and to conjunctive normal form: module prop-laws imports prop rules DefI : Impl ( x , y ) - > Or ( Not ( x ), y ) DefE : Eq ( x , y ) - > And ( Impl ( x , y ), Impl ( y , x )) DN : Not ( Not ( x )) - > x DMA : Not ( And ( x , y )) - > Or ( Not ( x ), Not ( y )) DMO : Not ( Or ( x , y )) - > And ( Not ( x ), Not ( y )) DAOL : And ( Or ( x , y ), z ) - > Or ( And ( x , z ), And ( y , z )) DAOR : And ( z , Or ( x , y )) - > Or ( And ( z , x ), And ( z , y )) DOAL : Or ( And ( x , y ), z ) - > And ( Or ( x , z ), Or ( y , z )) DOAR : Or ( z , And ( x , y )) - > And ( Or ( z , x ), Or ( z , y )) strategies dnf = innermost ( DefI < + DefE < + DAOL < + DAOR < + DN < + DMA < + DMO ) cnf = innermost ( DefI < + DefE < + DOAL < + DOAR < + DN < + DMA < + DMO ) The rules are named, and for each strategy different selections from the rule set are made.","title":"Cascading Transformations"},{"location":"background/stratego/strategic-rewriting/strategic-rewriting/#one-pass-traversals","text":"Cascading transformations can be defined with other strategies as well, and these strategies need not be exhaustive, but can be simpler one-pass traversals. For example, constant folding of Boolean expressions only requires a simple one-pass bottom-up traversal. This can be achieved using the bottomup strategy according the following scheme: simplify = bottomup ( repeat ( R1 < + ... < + Rn )) The bottomup strategy applies its argument strategy to each subterm in a bottom-to-top traversal. The repeat strategy applies its argument strategy repeatedly to a term. Module prop-eval2 defines the evaluation rules for Boolean expressions and a strategy for applying them using this approach: module prop-eval2 imports libstrategolib prop rules Eval : Not ( True ()) - > False () Eval : Not ( False ()) - > True () Eval : And ( True (), x ) - > x Eval : And ( x , True ()) - > x Eval : And ( False (), x ) - > False () Eval : And ( x , False ()) - > False () Eval : Or ( True (), x ) - > True () Eval : Or ( x , True ()) - > True () Eval : Or ( False (), x ) - > x Eval : Or ( x , False ()) - > x Eval : Impl ( True (), x ) - > x Eval : Impl ( x , True ()) - > True () Eval : Impl ( False (), x ) - > True () Eval : Impl ( x , False ()) - > Not ( x ) Eval : Eq ( False (), x ) - > Not ( x ) Eval : Eq ( x , False ()) - > Not ( x ) Eval : Eq ( True (), x ) - > x Eval : Eq ( x , True ()) - > x strategies main = io-wrap ( eval ) eval = bottomup ( repeat ( Eval )) The strategy eval applies these rules in a bottom-up traversal over a term, using the bottomup(s) strategy. At each sub-term, the rules are applied repeatedly until no more rule applies using the repeat(s) strategy. This is sufficient for the Eval rules, since the rules never construct a term with subterms that can be rewritten. Another typical example of the use of one-pass traversals is desugaring, that is rewriting language constructs to more basic language constructs. Simple desugarings can usually be expressed using a single top-to-bottom traversal according to the scheme simplify = topdown ( try ( R1 < + ... < + Rn )) The topdown strategy applies its argument strategy to a term and then traverses the resulting term. The try strategy tries to apply its argument strategy once to a term. Module prop-desugar defines a number of desugaring rules for Boolean expressions, defining propositional operators in terms of others. For example, rule DefN defines Not in terms of Impl , and rule DefI defines Impl in terms of Or and Not . So not all rules should be applied in the same transformation or non-termination would result. module prop-desugar imports prop libstrategolib rules DefN : Not ( x ) - > Impl ( x , False ()) DefI : Impl ( x , y ) - > Or ( Not ( x ), y ) DefE : Eq ( x , y ) - > And ( Impl ( x , y ), Impl ( y , x )) DefO1 : Or ( x , y ) - > Impl ( Not ( x ), y ) DefO2 : Or ( x , y ) - > Not ( And ( Not ( x ), Not ( y ))) DefA1 : And ( x , y ) - > Not ( Or ( Not ( x ), Not ( y ))) DefA2 : And ( x , y ) - > Not ( Impl ( x , Not ( y ))) IDefI : Or ( Not ( x ), y ) - > Impl ( x , y ) IDefE : And ( Impl ( x , y ), Impl ( y , x )) - > Eq ( x , y ) strategies desugar = topdown ( try ( DefI < + DefE )) impl-nf = topdown ( repeat ( DefN < + DefA2 < + DefO1 < + DefE )) main-desugar = io-wrap ( desugar ) main-inf = io-wrap ( impl-nf ) The strategies desugar and impl-nf define two different desugaring transformation based on these rules. The desugar strategy gets rid of the implication and equivalence operators, while the impl-nf strategy reduces an expression to implicative normal-form, a format in which only implication ( Impl ) and False() are used. A final example of a one-pass traversal is the downup strategy, which applies its argument transformation during a traversal on the way down, and again on the way up: simplify = downup ( repeat ( R1 < + ... < + Rn )) An application of this strategy is a more efficient implementation of constant folding for Boolean expressions: eval = downup ( repeat ( Eval )) This strategy reduces terms such as And ( ... big expression ... , False ()) in one step (to False() in this case), while the bottomup strategy defined above would first evaluate the big expression.","title":"One-pass Traversals"},{"location":"background/stratego/strategic-rewriting/strategic-rewriting/#staged-transformations","text":"Cascading transformations apply a number of rules one after another to an entire tree. But in some cases this is not appropriate. For instance, two transformations may be inverses of one another, so that repeatedly applying one and then the other would lead to non-termination. To remedy this difficulty, Stratego supports the idiom of staged transformation. In staged computation, transformations are not applied to a subject term all at once, but rather in stages. In each stage, only rules from some particular subset of the entire set of available rules are applied. In the TAMPR program transformation system this idiom is called sequence of normal forms, since a program tree is transformed in a sequence of steps, each of which performs a normalization with respect to a specified set of rules. In Stratego this idiom can be expressed directly according to the following scheme: strategies simplify = innermost ( A1 < + ... < + Ak ) ; innermost ( B1 < + ... < + Bl ) ; ... ; innermost ( C1 < + ... < + Cm )","title":"Staged Transformations"},{"location":"background/stratego/strategic-rewriting/strategic-rewriting/#local-transformations","text":"In conventional program optimization, transformations are applied throughout a program. In optimizing imperative programs, for example, complex transformations are applied to entire programs. In GHC-style compilation-by-transformation, small transformation steps are applied throughout programs. Another style of transformation is a mixture of these ideas. Instead of applying a complex transformation algorithm to a program we use staged, cascading transformations to accumulate small transformation steps for large effect. However, instead of applying transformations throughout the subject program, we often wish to apply them locally, i.e., only to selected parts of the subject program. This allows us to use transformations rules that would not be beneficial if applied everywhere. One example of a strategy which achieves such a transformation is strategies transformation = alltd ( trigger-transformation ; innermost ( A1 < + ... < + An ) ) The strategy alltd(s) descends into a term until a subterm is encountered for which the transformation s succeeds. In this case the strategy trigger-transformation recognizes a program fragment that should be transformed. Thus, cascading transformations are applied locally to terms for which the transformation is triggered. Of course more sophisticated strategies can be used for finding application locations, as well as for applying the rules locally. Nevertheless, the key observation underlying this idiom remains: Because the transformations to be applied are local, special knowledge about the subject program at the point of application can be used. This allows the application of rules that would not be otherwise applicable.","title":"Local Transformations"},{"location":"background/stratego/strategic-rewriting/strategic-rewriting/#references","text":"Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9","title":"References"},{"location":"background/stratego/strategic-rewriting/term-rewriting/","text":"Term Rewriting \u00b6 In term rewriting a term is transformed by repeated application of rewrite rules. To see how this works we take as example the language of propositional formulae, also known as Boolean expressions: module prop signature sorts Prop constructors False : Prop True : Prop Atom : String - > Prop Not : Prop - > Prop And : Prop * Prop - > Prop Or : Prop * Prop - > Prop Impl : Prop * Prop - > Prop Eq : Prop * Prop - > Prop Given this signature we can write terms such as And(Impl(True(),False()),False()) , and And(Atom(\"p\"),False())) . Atoms are also known as proposition letters; they are the variables in propositional formulae. That is, the truth value of an atom should be provided in order to fully evaluate an expression. Here we will evaluate expressions as far as possible, a transformation also known as constant folding. We will do this using rewrite rules that define how to simplify a single operator application. Term Patterns \u00b6 A term pattern is a term with meta variables, which are identifiers that are not declared as (nullary) constructors. For example, And(x, True()) is a term pattern with variable x . Variables in term patterns are sometimes called meta variables, to distinguish them from variables in the source language being processed. For example, while atoms in the proposition expressions are variables from the point of view of the language, they are not variables from the perspective of a Stratego program. A term pattern p matches with a term t , if there is a substitution that replaces the variables in p such that it becomes equal to t . For example, the pattern And(x, True()) matches the term And(Impl(True(),Atom(\"p\")),True()) because replacing the variable x in the pattern by Impl(True(),Atom(\"p\")) makes the pattern equal to the term. Note that And(Atom(\"x\"),True()) does not match the term And(Impl(True(),Atom(\"p\")),True()) , since the subterms Atom(\"x\") and Impl(True(),Atom(\"p\")) do not match. Rewrite Rules \u00b6 An unconditional rewrite rule has the form L : p1 -> p2 , where L is the name of the rule, p1 is the left-hand side and p2 the right-hand side term pattern. A rewrite rule L : p1 -> p2 applies to a term t when the pattern p1 matches t . The result is the instantiation of p2 with the variable bindings found during matching. For example, the rewrite rule E : Eq ( x , False ()) - > Not ( x ) rewrites the term Eq(Atom(\"q\"),False()) to Not(Atom(\"q\")) , since the variable x is bound to the subterm Atom(\"q\") . Evaluation Rules \u00b6 Now we can create similar evaluation rules for all constructors of sort Prop : module prop-eval-rules imports prop rules E : Not ( True ()) - > False () E : Not ( False ()) - > True () E : And ( True (), x ) - > x E : And ( x , True ()) - > x E : And ( False (), x ) - > False () E : And ( x , False ()) - > False () E : Or ( True (), x ) - > True () E : Or ( x , True ()) - > True () E : Or ( False (), x ) - > x E : Or ( x , False ()) - > x E : Impl ( True (), x ) - > x E : Impl ( x , True ()) - > True () E : Impl ( False (), x ) - > True () E : Impl ( x , False ()) - > Not ( x ) E : Eq ( False (), x ) - > Not ( x ) E : Eq ( x , False ()) - > Not ( x ) E : Eq ( True (), x ) - > x E : Eq ( x , True ()) - > x strategies eval = innermost ( E ) Note that all rules have the same name, which is allowed in Stratego. The module defines the eval strategy to apply innermost(E) to the input term. The innermost strategy from the library exhaustively applies its argument transformation to the term it is applied to, starting with inner subterms. As an aside, we have now seen Stratego modules with rules and strategies sections. It is worth noting that a module can have any number of sections of either type, and that there is no actual semantic difference between the two section headings. In fact, either rewrite rules and/or strategy definitions can occur in either kind of section. Nevertheless, it often helps with making your transformations clearer to generally segregate rules and strategy definitions, and so both headings are allowed so you can punctuate your Stratego modules with them to improve readability. The next commands apply the eval strategy to various terms. < eval > And ( Impl ( True (), And ( False , True )), True ) => False < eval > And ( Impl ( True , And ( Atom ( \"p\" ), Atom ( \"q\" ))), Atom ( \"p\" )) => And ( And ( Atom ( \"p\" ), Atom ( \"q\" )), Atom ( \"p\" )) Adding Rules to a Rewrite System \u00b6 Next we extend the rewrite rules above to rewrite a Boolean expression to disjunctive normal form. A Boolean expression is in disjunctive normal form if it conforms to the following signature: signature sorts Or And NAtom Atom constructors Or : Or * Or - > Or : And - > Or And : And * And - > And : NAtom - > And Not : Atom - > NAtom : Atom - > NAtom Atom : String - > Atom We use this signature only to describe what a disjunctive normal form is, not in an the actual Stratego program. This is not necessary, since terms conforming to the DNF signature are also Prop terms as defined before. For example, the disjunctive normal form of And ( Impl ( Atom ( \"r\" ), And ( Atom ( \"p\" ), Atom ( \"q\" ))), Atom ( \"p\" )) is Or ( And ( Not ( Atom ( \"r\" )), Atom ( \"p\" )), And ( And ( Atom ( \"p\" ), Atom ( \"q\" )), Atom ( \"p\" ))) Module prop-dnf-rules extends the rules defined in prop-eval-rules with rules to achieve disjunctive normal forms: module prop-dnf-rules imports prop-eval-rules rules E : Impl ( x , y ) - > Or ( Not ( x ), y ) E : Eq ( x , y ) - > And ( Impl ( x , y ), Impl ( y , x )) E : Not ( Not ( x )) - > x E : Not ( And ( x , y )) - > Or ( Not ( x ), Not ( y )) E : Not ( Or ( x , y )) - > And ( Not ( x ), Not ( y )) E : And ( Or ( x , y ), z ) - > Or ( And ( x , z ), And ( y , z )) E : And ( z , Or ( x , y )) - > Or ( And ( z , x ), And ( z , y )) strategies dnf = innermost ( E ) The first two rules rewrite implication ( Impl ) and equivalence ( Eq ) to combinations of And , Or , and Not . The third rule removes double negation. The fifth and sixth rules implement the well known DeMorgan laws. The last two rules define distribution of conjunction over disjunction.","title":"Term Rewriting"},{"location":"background/stratego/strategic-rewriting/term-rewriting/#term-rewriting","text":"In term rewriting a term is transformed by repeated application of rewrite rules. To see how this works we take as example the language of propositional formulae, also known as Boolean expressions: module prop signature sorts Prop constructors False : Prop True : Prop Atom : String - > Prop Not : Prop - > Prop And : Prop * Prop - > Prop Or : Prop * Prop - > Prop Impl : Prop * Prop - > Prop Eq : Prop * Prop - > Prop Given this signature we can write terms such as And(Impl(True(),False()),False()) , and And(Atom(\"p\"),False())) . Atoms are also known as proposition letters; they are the variables in propositional formulae. That is, the truth value of an atom should be provided in order to fully evaluate an expression. Here we will evaluate expressions as far as possible, a transformation also known as constant folding. We will do this using rewrite rules that define how to simplify a single operator application.","title":"Term Rewriting"},{"location":"background/stratego/strategic-rewriting/term-rewriting/#term-patterns","text":"A term pattern is a term with meta variables, which are identifiers that are not declared as (nullary) constructors. For example, And(x, True()) is a term pattern with variable x . Variables in term patterns are sometimes called meta variables, to distinguish them from variables in the source language being processed. For example, while atoms in the proposition expressions are variables from the point of view of the language, they are not variables from the perspective of a Stratego program. A term pattern p matches with a term t , if there is a substitution that replaces the variables in p such that it becomes equal to t . For example, the pattern And(x, True()) matches the term And(Impl(True(),Atom(\"p\")),True()) because replacing the variable x in the pattern by Impl(True(),Atom(\"p\")) makes the pattern equal to the term. Note that And(Atom(\"x\"),True()) does not match the term And(Impl(True(),Atom(\"p\")),True()) , since the subterms Atom(\"x\") and Impl(True(),Atom(\"p\")) do not match.","title":"Term Patterns"},{"location":"background/stratego/strategic-rewriting/term-rewriting/#rewrite-rules","text":"An unconditional rewrite rule has the form L : p1 -> p2 , where L is the name of the rule, p1 is the left-hand side and p2 the right-hand side term pattern. A rewrite rule L : p1 -> p2 applies to a term t when the pattern p1 matches t . The result is the instantiation of p2 with the variable bindings found during matching. For example, the rewrite rule E : Eq ( x , False ()) - > Not ( x ) rewrites the term Eq(Atom(\"q\"),False()) to Not(Atom(\"q\")) , since the variable x is bound to the subterm Atom(\"q\") .","title":"Rewrite Rules"},{"location":"background/stratego/strategic-rewriting/term-rewriting/#evaluation-rules","text":"Now we can create similar evaluation rules for all constructors of sort Prop : module prop-eval-rules imports prop rules E : Not ( True ()) - > False () E : Not ( False ()) - > True () E : And ( True (), x ) - > x E : And ( x , True ()) - > x E : And ( False (), x ) - > False () E : And ( x , False ()) - > False () E : Or ( True (), x ) - > True () E : Or ( x , True ()) - > True () E : Or ( False (), x ) - > x E : Or ( x , False ()) - > x E : Impl ( True (), x ) - > x E : Impl ( x , True ()) - > True () E : Impl ( False (), x ) - > True () E : Impl ( x , False ()) - > Not ( x ) E : Eq ( False (), x ) - > Not ( x ) E : Eq ( x , False ()) - > Not ( x ) E : Eq ( True (), x ) - > x E : Eq ( x , True ()) - > x strategies eval = innermost ( E ) Note that all rules have the same name, which is allowed in Stratego. The module defines the eval strategy to apply innermost(E) to the input term. The innermost strategy from the library exhaustively applies its argument transformation to the term it is applied to, starting with inner subterms. As an aside, we have now seen Stratego modules with rules and strategies sections. It is worth noting that a module can have any number of sections of either type, and that there is no actual semantic difference between the two section headings. In fact, either rewrite rules and/or strategy definitions can occur in either kind of section. Nevertheless, it often helps with making your transformations clearer to generally segregate rules and strategy definitions, and so both headings are allowed so you can punctuate your Stratego modules with them to improve readability. The next commands apply the eval strategy to various terms. < eval > And ( Impl ( True (), And ( False , True )), True ) => False < eval > And ( Impl ( True , And ( Atom ( \"p\" ), Atom ( \"q\" ))), Atom ( \"p\" )) => And ( And ( Atom ( \"p\" ), Atom ( \"q\" )), Atom ( \"p\" ))","title":"Evaluation Rules"},{"location":"background/stratego/strategic-rewriting/term-rewriting/#adding-rules-to-a-rewrite-system","text":"Next we extend the rewrite rules above to rewrite a Boolean expression to disjunctive normal form. A Boolean expression is in disjunctive normal form if it conforms to the following signature: signature sorts Or And NAtom Atom constructors Or : Or * Or - > Or : And - > Or And : And * And - > And : NAtom - > And Not : Atom - > NAtom : Atom - > NAtom Atom : String - > Atom We use this signature only to describe what a disjunctive normal form is, not in an the actual Stratego program. This is not necessary, since terms conforming to the DNF signature are also Prop terms as defined before. For example, the disjunctive normal form of And ( Impl ( Atom ( \"r\" ), And ( Atom ( \"p\" ), Atom ( \"q\" ))), Atom ( \"p\" )) is Or ( And ( Not ( Atom ( \"r\" )), Atom ( \"p\" )), And ( And ( Atom ( \"p\" ), Atom ( \"q\" )), Atom ( \"p\" ))) Module prop-dnf-rules extends the rules defined in prop-eval-rules with rules to achieve disjunctive normal forms: module prop-dnf-rules imports prop-eval-rules rules E : Impl ( x , y ) - > Or ( Not ( x ), y ) E : Eq ( x , y ) - > And ( Impl ( x , y ), Impl ( y , x )) E : Not ( Not ( x )) - > x E : Not ( And ( x , y )) - > Or ( Not ( x ), Not ( y )) E : Not ( Or ( x , y )) - > And ( Not ( x ), Not ( y )) E : And ( Or ( x , y ), z ) - > Or ( And ( x , z ), And ( y , z )) E : And ( z , Or ( x , y )) - > Or ( And ( z , x ), And ( z , y )) strategies dnf = innermost ( E ) The first two rules rewrite implication ( Impl ) and equivalence ( Eq ) to combinations of And , Or , and Not . The third rule removes double negation. The fifth and sixth rules implement the well known DeMorgan laws. The last two rules define distribution of conjunction over disjunction.","title":"Adding Rules to a Rewrite System"},{"location":"background/stratego/strategic-rewriting/traversal-with-rules/","text":"Factoring out Traversal \u00b6 Continuing the inspection of limitations of term rewriting , we explore how term traversal can be factored out into separate rules. Attempt 3: Using Rules for Traversal \u00b6 We saw the following definition of the map strategy, which applies a strategy to each element of a list: map ( s ) : [] - > [] map ( s ) : [ x | xs ] - > [< s > x | < map ( s )> xs ] The definition uses explicit recursive calls to the strategy in the right-hand side of the second rule. What map does is to traverse the list in order to apply the argument strategy to all elements. We can use the same technique to other term structures as well. We will explore the definition of traversals using the propositional formulae, where we introduced the following rewrite rules: module prop-rules imports libstrategolib prop rules DefI : Impl ( x , y ) - > Or ( Not ( x ), y ) DefE : Eq ( x , y ) - > And ( Impl ( x , y ), Impl ( y , x )) DN : Not ( Not ( x )) - > x DMA : Not ( And ( x , y )) - > Or ( Not ( x ), Not ( y )) DMO : Not ( Or ( x , y )) - > And ( Not ( x ), Not ( y )) DAOL : And ( Or ( x , y ), z ) - > Or ( And ( x , z ), And ( y , z )) DAOR : And ( z , Or ( x , y )) - > Or ( And ( z , x ), And ( z , y )) DOAL : Or ( And ( x , y ), z ) - > And ( Or ( x , z ), Or ( y , z )) DOAR : Or ( z , And ( x , y )) - > And ( Or ( z , x ), Or ( z , y )) Above we saw how a functional style of rewriting could be encoded using extra constructors. In Stratego we can achieve a similar approach by using rule names, instead of extra constructors. Thus, one way to achieve normalization to disjunctive normal form, is the use of an explicitly programmed traversal, implemented using recursive rules, similarly to the map example above: module prop-dnf4 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules dnf : True () - > True () dnf : False () - > False () dnf : Atom ( x ) - > Atom ( x ) dnf : Not ( x ) - > < dnfred > Not (< dnf > x ) dnf : And ( x , y ) - > < dnfred > And (< dnf > x , < dnf > y ) dnf : Or ( x , y ) - > Or (< dnf > x , < dnf > y ) dnf : Impl ( x , y ) - > < dnfred > Impl (< dnf > x , < dnf > y ) dnf : Eq ( x , y ) - > < dnfred > Eq (< dnf > x , < dnf > y ) strategies dnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf ) The dnf rules recursively apply themselves to the direct subterms and then apply dnfred to actually apply the rewrite rules. We can reduce this program by abstracting over the base cases. Since there is no traversal into True , False , and Atom s, these rules can be be left out. module prop-dnf5 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules dnft : Not ( x ) - > < dnfred > Not (< dnf > x ) dnft : And ( x , y ) - > < dnfred > And (< dnf > x , < dnf > y ) dnft : Or ( x , y ) - > Or (< dnf > x , < dnf > y ) dnft : Impl ( x , y ) - > < dnfred > Impl (< dnf > x , < dnf > y ) dnft : Eq ( x , y ) - > < dnfred > Eq (< dnf > x , < dnf > y ) strategies dnf = try ( dnft ) dnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf ) The dnf strategy is now defined in terms of the dnft rules, which implement traversal over the constructors. By using try(dnft) , terms for which no traversal rule has been specified are not transformed. We can further simplify the definition by observing that the application of dnfred does not necessarily have to take place in the right-hand side of the traversal rules. module prop-dnf6 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules dnft : Not ( x ) - > Not (< dnf > x ) dnft : And ( x , y ) - > And (< dnf > x , < dnf > y ) dnft : Or ( x , y ) - > Or (< dnf > x , < dnf > y ) dnft : Impl ( x , y ) - > Impl (< dnf > x , < dnf > y ) dnft : Eq ( x , y ) - > Eq (< dnf > x , < dnf > y ) strategies dnf = try ( dnft ); dnfred dnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf ) In this program dnf first calls dnft to transform the subterms of the subject term, and then calls dnfred to apply the transformation rules (and possibly a recursive invocation of dnf ). The program above has two problems. First, the traversal behavior is mostly uniform, so we would like to specify that more concisely. We will address that concern below. Second, the traversal is not reusable, for example, to define a conjunctive normal form transformation. This last concern can be addressed by factoring out the recursive call to dnf and making it a parameter of the traversal rules. module prop-dnf7 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules proptr ( s ) : Not ( x ) - > Not (< s > x ) proptr ( s ) : And ( x , y ) - > And (< s > x , < s > y ) proptr ( s ) : Or ( x , y ) - > Or (< s > x , < s > y ) proptr ( s ) : Impl ( x , y ) - > Impl (< s > x , < s > y ) proptr ( s ) : Eq ( x , y ) - > Eq (< s > x , < s > y ) strategies dnf = try ( proptr ( dnf )); dnfred dnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf ) cnf = try ( proptr ( cnf )); cnfred cnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ); cnf ) Now the traversal rules are reusable and used in two different transformations, by instantiation with a call to the particular strategy in which they are used ( dnf or cnf ). But we can do better, and also make the composition of this strategy reusable. module prop-dnf8 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules proptr ( s ) : Not ( x ) - > Not (< s > x ) proptr ( s ) : And ( x , y ) - > And (< s > x , < s > y ) proptr ( s ) : Or ( x , y ) - > Or (< s > x , < s > y ) proptr ( s ) : Impl ( x , y ) - > Impl (< s > x , < s > y ) proptr ( s ) : Eq ( x , y ) - > Eq (< s > x , < s > y ) strategies propbu ( s ) = try ( proptr ( propbu ( s ))); s strategies dnf = propbu ( dnfred ) dnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf ) cnf = propbu ( cnfred ) cnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ); cnf ) That is, the propbu(s) strategy defines a complete bottom-up traversal over proposition terms, applying the strategy s to a term after transforming its subterms. The strategy is completely independent of the dnf and cnf transformations, which instantiate the strategy using the dnfred and cnfred strategies. Come to think of it, dnfred and cnfred are somewhat useless now and can be inlined directly in the instantiation of the propbu(s) strategy: module prop-dnf9 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules proptr ( s ) : Not ( x ) - > Not (< s > x ) proptr ( s ) : And ( x , y ) - > And (< s > x , < s > y ) proptr ( s ) : Or ( x , y ) - > Or (< s > x , < s > y ) proptr ( s ) : Impl ( x , y ) - > Impl (< s > x , < s > y ) proptr ( s ) : Eq ( x , y ) - > Eq (< s > x , < s > y ) strategies propbu ( s ) = try ( proptr ( propbu ( s ))); s strategies dnf = propbu ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf )) cnf = propbu ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ); cnf )) Now we have defined a transformation independent traversal strategy that is specific for proposition terms.","title":"Factoring out Traversal"},{"location":"background/stratego/strategic-rewriting/traversal-with-rules/#factoring-out-traversal","text":"Continuing the inspection of limitations of term rewriting , we explore how term traversal can be factored out into separate rules.","title":"Factoring out Traversal"},{"location":"background/stratego/strategic-rewriting/traversal-with-rules/#attempt-3-using-rules-for-traversal","text":"We saw the following definition of the map strategy, which applies a strategy to each element of a list: map ( s ) : [] - > [] map ( s ) : [ x | xs ] - > [< s > x | < map ( s )> xs ] The definition uses explicit recursive calls to the strategy in the right-hand side of the second rule. What map does is to traverse the list in order to apply the argument strategy to all elements. We can use the same technique to other term structures as well. We will explore the definition of traversals using the propositional formulae, where we introduced the following rewrite rules: module prop-rules imports libstrategolib prop rules DefI : Impl ( x , y ) - > Or ( Not ( x ), y ) DefE : Eq ( x , y ) - > And ( Impl ( x , y ), Impl ( y , x )) DN : Not ( Not ( x )) - > x DMA : Not ( And ( x , y )) - > Or ( Not ( x ), Not ( y )) DMO : Not ( Or ( x , y )) - > And ( Not ( x ), Not ( y )) DAOL : And ( Or ( x , y ), z ) - > Or ( And ( x , z ), And ( y , z )) DAOR : And ( z , Or ( x , y )) - > Or ( And ( z , x ), And ( z , y )) DOAL : Or ( And ( x , y ), z ) - > And ( Or ( x , z ), Or ( y , z )) DOAR : Or ( z , And ( x , y )) - > And ( Or ( z , x ), Or ( z , y )) Above we saw how a functional style of rewriting could be encoded using extra constructors. In Stratego we can achieve a similar approach by using rule names, instead of extra constructors. Thus, one way to achieve normalization to disjunctive normal form, is the use of an explicitly programmed traversal, implemented using recursive rules, similarly to the map example above: module prop-dnf4 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules dnf : True () - > True () dnf : False () - > False () dnf : Atom ( x ) - > Atom ( x ) dnf : Not ( x ) - > < dnfred > Not (< dnf > x ) dnf : And ( x , y ) - > < dnfred > And (< dnf > x , < dnf > y ) dnf : Or ( x , y ) - > Or (< dnf > x , < dnf > y ) dnf : Impl ( x , y ) - > < dnfred > Impl (< dnf > x , < dnf > y ) dnf : Eq ( x , y ) - > < dnfred > Eq (< dnf > x , < dnf > y ) strategies dnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf ) The dnf rules recursively apply themselves to the direct subterms and then apply dnfred to actually apply the rewrite rules. We can reduce this program by abstracting over the base cases. Since there is no traversal into True , False , and Atom s, these rules can be be left out. module prop-dnf5 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules dnft : Not ( x ) - > < dnfred > Not (< dnf > x ) dnft : And ( x , y ) - > < dnfred > And (< dnf > x , < dnf > y ) dnft : Or ( x , y ) - > Or (< dnf > x , < dnf > y ) dnft : Impl ( x , y ) - > < dnfred > Impl (< dnf > x , < dnf > y ) dnft : Eq ( x , y ) - > < dnfred > Eq (< dnf > x , < dnf > y ) strategies dnf = try ( dnft ) dnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf ) The dnf strategy is now defined in terms of the dnft rules, which implement traversal over the constructors. By using try(dnft) , terms for which no traversal rule has been specified are not transformed. We can further simplify the definition by observing that the application of dnfred does not necessarily have to take place in the right-hand side of the traversal rules. module prop-dnf6 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules dnft : Not ( x ) - > Not (< dnf > x ) dnft : And ( x , y ) - > And (< dnf > x , < dnf > y ) dnft : Or ( x , y ) - > Or (< dnf > x , < dnf > y ) dnft : Impl ( x , y ) - > Impl (< dnf > x , < dnf > y ) dnft : Eq ( x , y ) - > Eq (< dnf > x , < dnf > y ) strategies dnf = try ( dnft ); dnfred dnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf ) In this program dnf first calls dnft to transform the subterms of the subject term, and then calls dnfred to apply the transformation rules (and possibly a recursive invocation of dnf ). The program above has two problems. First, the traversal behavior is mostly uniform, so we would like to specify that more concisely. We will address that concern below. Second, the traversal is not reusable, for example, to define a conjunctive normal form transformation. This last concern can be addressed by factoring out the recursive call to dnf and making it a parameter of the traversal rules. module prop-dnf7 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules proptr ( s ) : Not ( x ) - > Not (< s > x ) proptr ( s ) : And ( x , y ) - > And (< s > x , < s > y ) proptr ( s ) : Or ( x , y ) - > Or (< s > x , < s > y ) proptr ( s ) : Impl ( x , y ) - > Impl (< s > x , < s > y ) proptr ( s ) : Eq ( x , y ) - > Eq (< s > x , < s > y ) strategies dnf = try ( proptr ( dnf )); dnfred dnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf ) cnf = try ( proptr ( cnf )); cnfred cnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ); cnf ) Now the traversal rules are reusable and used in two different transformations, by instantiation with a call to the particular strategy in which they are used ( dnf or cnf ). But we can do better, and also make the composition of this strategy reusable. module prop-dnf8 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules proptr ( s ) : Not ( x ) - > Not (< s > x ) proptr ( s ) : And ( x , y ) - > And (< s > x , < s > y ) proptr ( s ) : Or ( x , y ) - > Or (< s > x , < s > y ) proptr ( s ) : Impl ( x , y ) - > Impl (< s > x , < s > y ) proptr ( s ) : Eq ( x , y ) - > Eq (< s > x , < s > y ) strategies propbu ( s ) = try ( proptr ( propbu ( s ))); s strategies dnf = propbu ( dnfred ) dnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf ) cnf = propbu ( cnfred ) cnfred = try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ); cnf ) That is, the propbu(s) strategy defines a complete bottom-up traversal over proposition terms, applying the strategy s to a term after transforming its subterms. The strategy is completely independent of the dnf and cnf transformations, which instantiate the strategy using the dnfred and cnfred strategies. Come to think of it, dnfred and cnfred are somewhat useless now and can be inlined directly in the instantiation of the propbu(s) strategy: module prop-dnf9 imports libstrategolib prop-rules strategies main = io-wrap ( dnf ) rules proptr ( s ) : Not ( x ) - > Not (< s > x ) proptr ( s ) : And ( x , y ) - > And (< s > x , < s > y ) proptr ( s ) : Or ( x , y ) - > Or (< s > x , < s > y ) proptr ( s ) : Impl ( x , y ) - > Impl (< s > x , < s > y ) proptr ( s ) : Eq ( x , y ) - > Eq (< s > x , < s > y ) strategies propbu ( s ) = try ( proptr ( propbu ( s ))); s strategies dnf = propbu ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf )) cnf = propbu ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ); cnf )) Now we have defined a transformation independent traversal strategy that is specific for proposition terms.","title":"Attempt 3: Using Rules for Traversal"},{"location":"background/stratego/strategy-combinators/sequential/","text":"Sequential Combinators \u00b6 Rather than defining rewrite rules and high-level strategies as primitives of the language, Stratego provides strategy combinators as basic building blocks from which these can defined 1 . Thus, Stratego consists of a core language 2 and a 'sugar' language defined by reduction to the core language. Identity and Failure \u00b6 The most basic operations in Stratego are id and fail . The identity strategy id always succeeds and behaves as the identity function on terms. The failure strategy fail always fails. The operations have no side effects. Sequential Composition \u00b6 The sequential composition s1 ; s2 of the strategies s1 and s2 first applies the strategy s1 to the subject term and then s2 to the result of that first application. The strategy fails if either s1 or s2 fails. Sequential composition is associative. Identity is a left and right unit for sequential composition; since id always succeeds and leaves the term alone, it has no additional effect to the strategy that it is composed with. Failure is a left zero for sequential composition; since fail always fails the next strategy will never be reached. This leads to the following equations: ( s1 ; s2 ) ; s3 = s1 ; ( s2 ; s3 ) id ; s = s s ; id = s fail ; s = fail However, not for all strategies s we have that failure is a right zero for sequential composition: s ; fail = fail // is not a law Although the composition s; fail will always fail, the execution of s may have side effects that are not performed by fail . For example, consider printing a term in s . As an example of the use of sequential composition consider the following rewrite rules. A : P ( Z (), x ) - > x B : P ( S ( x ), y ) - > P ( x , S ( y )) The following applications shows the effect of first applying B and then A : < B > ! P ( S ( Z ()), Z ()) => P ( S ( Z ), Z ) < A > P ( Z , S ( Z )) => S ( Z ) Using the sequential composition of the two rules, this effect can be achieved \u2018in one step\u2019: < B ; A > ! P ( S ( Z ()), Z ()) => S ( Z ) The following application shows that the application of a composition fails if the second strategy in the composition fails to apply to the result of the first: < B ; B > ! P ( S ( Z ()), Z ()) // fails Left Choice \u00b6 Choosing between rules to apply is achieved using one of several choice combinators, all of which are based on the guarded choice combinator. The common approach is that failure to apply one strategy leads to backtracking to an alternative strategy. The left choice or deterministic choice s1 <+ s2 tries to apply s1 and s2 in that order. That is, it first tries to apply s1 , and if that succeeds the choice succeeds. However, if the application of s1 fails, s2 is applied to the original term. Properties. Left choice is associative. Identity is a left zero for left choice; since id always succeeds, the alternative strategy will never be tried. Failure is a left and right unit for left choice; since fail always fails, the choice will always backtrack to the alternative strategy, and use of fail as alternative strategy is pointless. ( s1 < + s2 ) < + s3 = s1 < + ( s2 < + s3 ) id < + s = id fail < + s = s s < + fail = s However, identity is not a right zero for left choice. That is, not for all strategies s we have that s < + id = s // is not a law The expression s <+ id always succeeds, even (especially) in the case that s fails, in which case the right-hand side of the equation fails of course. Local Backtracking. The left choice combinator is a local backtracking combinator. That is, the choice is committed once the left-hand side strategy has succeeded, even if the continuation strategy fails. This is expressed by the fact that the property ( s1 < + s2 ); s3 = ( s1 ; s3 ) < + ( s2 ; s3 ) // is not a law does not hold for all s1 , s2 , and s3 . The difference is illustrated by the following applications: <( B < + id ); B > P ( S ( Z ), Z ) // fails <( B ; B ) < + ( id ; B )> P ( S ( Z ()), Z ()) => P ( Z , S ( Z )) In the application of (B <+ id); B , the first application of B succeeds after which the choice is committed. The subsequent application of B then fails. This is equivalent to first applying (B <+ id) and then applying B to the result. The application of (B; B) <+ (id; B) , however, is successful; the application of B; B fails, after which the choice backtracks to id; B , which succeeds. Choosing between Transformations. \u00b6 The typical use of left choice is to create a composite strategy trying one from several possible transformations. If the strategies that are composed are mutually exclusive, that is, don\u2019t succeed for the same terms, their sum is a transformation that (deterministically) covers a larger set of terms. For example, consider the following two rewrite rules: PlusAssoc : Plus ( Plus ( e1 , e2 ), e3 ) - > Plus ( e1 , Plus ( e2 , e3 )) PlusZero : Plus ( Int ( \"0\" ), e ) - > e These rules are mutually exclusive, since there is no term that matches the left-hand sides of both rules. Combining the rules with left choice into PlusAssoc <+ PlusZero creates a strategy that transforms terms matching both rules as illustrated by the following applications: < PlusAssoc > Plus ( Int ( \"0\" ), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Int ( \"0\" ), Int ( \"3\" )) => Int ( \"3\" ) < PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) => Plus ( Var ( \"x\" ), Plus ( Int ( \"42\" ), Int ( \"3\" ))) Ordering Overlapping Rules. \u00b6 When two rules or strategies are mutually exlusive the order of applying them does not matter. In cases where strategies are overlapping, that is, succeed for the same terms, the order becomes crucial to determining the semantics of the composition. For example, consider the following rewrite rules reducing applications of Mem: Mem1 : Mem ( x ,[]) - > False () Mem2 : Mem ( x ,[ x | xs ]) - > True () Mem3 : Mem ( x ,[ y | ys ]) - > Mem ( x , ys ) Rules Mem2 and Mem3 have overlapping left-hand sides. Rule Mem2 only applies if the first argument is equal to the head element of the list in the second argument. Rule Mem3 applies always if the list in the second argument is non-empty. < Mem2 > Mem ( 1 , [ 1 , 2 , 3 ]) => True () < Mem3 > Mem ( 1 , [ 1 , 2 , 3 ]) => Mem ( 1 ,[ 2 , 3 ]) In such situations, depending on the order of the rules, different results are produced. (The rules form a non-confluent rewriting system.) By ordering the rules as Mem2 <+ Mem3 , rule Mem2 is tried before Mem3 , and we have a deterministic transformation strategy. Try \u00b6 A useful application of <+ in combination with id is the reflexive closure of a strategy s: try ( s ) = s < + id The user-defined strategy combinator try tries to apply its argument strategy s , but if that fails, just succeeds using id . Guarded Left Choice \u00b6 Sometimes it is not desirable to backtrack to the alternative specified in a choice. Rather, after passing a guard, the choice should be committed. This can be expressed using the guarded left choice operator s1 < s2 + s3 . If s1 succeeds s2 is applied, else s3 is applied. If s2 fails, the complete expression fails; no backtracking to s3 takes place. Properties. This combinator is a generalization of the left choice combinator <+ . s1 < + s2 = s1 < id + s2 The following laws make clear that the \u2018branches\u2019 of the choice are selected by the success or failure of the guard: id < s2 + s3 = s2 fail < s2 + s3 = s3 If the right branch always fails, the construct reduces to the sequential composition of the guard and the left branch. s1 < s2 + fail = s1 ; s2 Guarded choice is not associative: ( s1 < s2 + s3 ) < s4 + s5 = s1 < s2 + ( s3 < s4 + s5 ) // not a law To see why consider the possible traces of these expressions. For example, when s1 and s2 succeed subsequently, the left-hand side expression calls s4 , while the right-hand side expression does not. However, sequential composition distributes over guarded choice from left and right: ( s1 < s2 + s3 ); s4 = s1 < ( s2 ; s4 ) + ( s3 ; s4 ) s0 ; ( s1 < s2 + s3 ) = ( s0 ; s1 ) < s2 + s3 Examples. The guarded left choice operator is most useful for the implementation of higher-level control-flow strategies. For example, the negation not(s) of a strategy s , succeeds if s fails, and fails when it succeeds: not ( s ) = s < fail + id Since failure discards the effect of a (successful) transformation, this has the effect of testing whether s succeeds. So we have the following laws for not: not ( id ) = fail not ( fail ) = id However, side effects performed by s are not undone, of course. Therefore, the following equation does not hold: not ( not ( s )) = s // not a law Another example of the use of guarded choice is the restore-always combinator: restore-always ( s , r ) = s < r + ( r ; fail ) It applies a \u2018restore\u2019 strategy r after applying a strategy s , even if s fails, and preserves the success/failure behavior of s . Since fail discards the transformation effect of r , this is mostly useful for ensuring that some side-effecting operation is done (or undone) after applying s . If-then-else \u00b6 The guarded choice combinator is similar to the traditional if-then-else construct of programming languages. The difference is that the \u2018then\u2019 branch applies to the result of the application of the condition. Stratego\u2019s if s1 then s2 else s3 end construct is more like the traditional construct since both branches apply to the original term. The condition strategy is only used to test if it succeeds or fails, but it\u2019s transformation effect is undone. However, the condition strategy s1 is still applied to the current term. The if s1 then s2 end strategy is similar; if the condition fails, the strategy succeeds. The if-then-else-end strategy is just syntactic sugar for a combination of guarded choice and the where combinator: if s1 then s2 else s3 end ==> // transforms to where ( s1 ) < s2 + s3 The strategy where(s) succeeds if s succeeds, but returns the original subject term. The implementation of the where combinator is discussed in the section on matching and building terms . The following laws show that the branches are selected by success or failure of the condition: if id then s2 else s3 end = s2 if fail then s2 else s3 end = s3 The if-then-end strategy is an abbreviation for the if-then-else-end with the identity strategy as right branch: if s1 then s2 end = where ( s1 ) < s2 + id Examples. The inclusive or or(s1, s2) succeeds if one of the strategies s1 or s2 succeeds, but guarantees that both are applied, in the order s1 first, then s2 : or ( s1 , s2 ) = if s1 then try ( where ( s2 )) else where ( s2 ) end This ensures that any side effects are always performed, in contrast to s1 <\\+ s2 , where s2 is only executed if s1 fails. (Thus, left choice implements a short circuit Boolean or.) Similarly, the following and(s1, s2) combinator is the non-short circuit version of Boolean conjunction: and ( s1 , s2 ) = if s1 then where ( s2 ) else where ( s2 ); fail end Switch \u00b6 The switch construct is an n-ary branching construct similar to its counter parts in other programming languages. It is defined in terms of guarded choice. The switch construct has the following form: switch s0 case s1 : s1 ' case s2 : s2 ' ... otherwise : sdef end The switch first applies the s0 strategy to the current term t resulting in a term t' . Then it tries the cases in turn applying each si to t' . As soon as this succeeds the corresponding case is selected and si' is applied to the t , the term to which the switch was applied. If none of the cases applies, the default strategy sdef from the otherwise is applied. The switch construct is syntactic sugar for a nested if-then-else: { x : where ( s0 => x ); if < s1 > x then s1 ' else if < s2 > x then s2 ' else if ... then ... else sdef end end end } Non-Deterministic Choice \u00b6 The deterministic left choice operator prescribes that the left alternative should be tried before the right alternative, and that the latter is only used if the first fails. There are applications where it is not necessary to define the order of the alternatives. In those cases non-deterministic choice can be used. The non-deterministic choice operator s1 + s2 chooses one of the two strategies s1 or s2 to apply, such that the one it chooses succeeds. If both strategies fail, then the choice fails as well. Operationally the choice operator first tries one strategy, and, if that fails, tries the other. The order in which this is done is undefined, i.e., arbitrarily chosen by the compiler. The + combinator is used to model modular composition of rewrite rules and strategies with the same name, but in different modules. Multiple definitions with the same name in different modules, are merged into a single definition with that name, where the bodies are composed with + . The following transformation illustrates this: module A f = s1 module B f = s2 module main imports A B => f = s2 + s1 This transformation is somewhat simplified; the complete transformation needs to take care of local variables and parameters. While the + combinator is used internally by the compiler for this purpose, programmers are advised not to use this combinator, but rather use the left choice combinator <+ to avoid surprises. In the past, the + combinator was also used to compose definitions with the same name within a module. This has been replaced by interpreting such compositions with the textual order of the definitions. The following transformation illustrates this: module A f = s1 f = s2 => f = s1 < + s2 Recursion \u00b6 Repeated application of a strategy can be achieved with recursion. There are two styles for doing this; with a recursive definition or using the fixpoint operator rec . A recursive definition is a normal strategy definition with a recursive call in its body. f ( s ) = ... f ( s ) ... Another way to define recursion is using the fixpoint operator rec x(s) , which recurses on applications of x within s. For example, the definition f ( s ) = rec x ( ... x ... ) is equivalent to the one above. The advantage of the rec operator is that it allows the definition of an unnamed strategy expression to be recursive. For example, in the definition g ( s ) = foo ; rec x ( ... x ... ); bar the strategy between foo and bar is a recursive strategy that does not recurse to g(s) . Originally, the rec operator was the only way to define recursive strategies. It is still in the language in the first place because it is widely used in many existing programs, and in the second place because it can be a concise expression of a recursive strategy, since call parameters are not included in the call. Furthermore, all free variables remain in scope. The repeat strategy applies a transformation s until it fails. It is defined as a recursive definition using try as follows: try ( s ) = s < + id repeat ( s ) = try ( s ; repeat ( s )) An equivalent definition using rec is: repeat ( s ) = rec x ( try ( s ; x )) A Library of Iteration Strategies. \u00b6 Using sequential composition, choice, and recursion a large variety of iteration strategies can be defined. The following definitions are part of the Stratego Library (in module strategy/iteration ). repeat ( s ) = rec x ( try ( s ; x )) repeat ( s , c ) = ( s ; repeat ( s , c )) < + c repeat1 ( s , c ) = s ; ( repeat1 ( s , c ) < + c ) repeat1 ( s ) = repeat1 ( s , id ) repeat-until ( s , c ) = s ; if c then id else repeat-until ( s , c ) end while ( c , s ) = if c then s ; while ( c , s ) end do-while ( s , c ) = s ; if c then do-while ( s , c ) end References \u00b6 Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661 \\(05\\) 80027-1 , doi:10.1016/S1571-0661 \\(05\\) 80027-1 . \u21a9","title":"Sequential Combinators"},{"location":"background/stratego/strategy-combinators/sequential/#sequential-combinators","text":"Rather than defining rewrite rules and high-level strategies as primitives of the language, Stratego provides strategy combinators as basic building blocks from which these can defined 1 . Thus, Stratego consists of a core language 2 and a 'sugar' language defined by reduction to the core language.","title":"Sequential Combinators"},{"location":"background/stratego/strategy-combinators/sequential/#identity-and-failure","text":"The most basic operations in Stratego are id and fail . The identity strategy id always succeeds and behaves as the identity function on terms. The failure strategy fail always fails. The operations have no side effects.","title":"Identity and Failure"},{"location":"background/stratego/strategy-combinators/sequential/#sequential-composition","text":"The sequential composition s1 ; s2 of the strategies s1 and s2 first applies the strategy s1 to the subject term and then s2 to the result of that first application. The strategy fails if either s1 or s2 fails. Sequential composition is associative. Identity is a left and right unit for sequential composition; since id always succeeds and leaves the term alone, it has no additional effect to the strategy that it is composed with. Failure is a left zero for sequential composition; since fail always fails the next strategy will never be reached. This leads to the following equations: ( s1 ; s2 ) ; s3 = s1 ; ( s2 ; s3 ) id ; s = s s ; id = s fail ; s = fail However, not for all strategies s we have that failure is a right zero for sequential composition: s ; fail = fail // is not a law Although the composition s; fail will always fail, the execution of s may have side effects that are not performed by fail . For example, consider printing a term in s . As an example of the use of sequential composition consider the following rewrite rules. A : P ( Z (), x ) - > x B : P ( S ( x ), y ) - > P ( x , S ( y )) The following applications shows the effect of first applying B and then A : < B > ! P ( S ( Z ()), Z ()) => P ( S ( Z ), Z ) < A > P ( Z , S ( Z )) => S ( Z ) Using the sequential composition of the two rules, this effect can be achieved \u2018in one step\u2019: < B ; A > ! P ( S ( Z ()), Z ()) => S ( Z ) The following application shows that the application of a composition fails if the second strategy in the composition fails to apply to the result of the first: < B ; B > ! P ( S ( Z ()), Z ()) // fails","title":"Sequential Composition"},{"location":"background/stratego/strategy-combinators/sequential/#left-choice","text":"Choosing between rules to apply is achieved using one of several choice combinators, all of which are based on the guarded choice combinator. The common approach is that failure to apply one strategy leads to backtracking to an alternative strategy. The left choice or deterministic choice s1 <+ s2 tries to apply s1 and s2 in that order. That is, it first tries to apply s1 , and if that succeeds the choice succeeds. However, if the application of s1 fails, s2 is applied to the original term. Properties. Left choice is associative. Identity is a left zero for left choice; since id always succeeds, the alternative strategy will never be tried. Failure is a left and right unit for left choice; since fail always fails, the choice will always backtrack to the alternative strategy, and use of fail as alternative strategy is pointless. ( s1 < + s2 ) < + s3 = s1 < + ( s2 < + s3 ) id < + s = id fail < + s = s s < + fail = s However, identity is not a right zero for left choice. That is, not for all strategies s we have that s < + id = s // is not a law The expression s <+ id always succeeds, even (especially) in the case that s fails, in which case the right-hand side of the equation fails of course. Local Backtracking. The left choice combinator is a local backtracking combinator. That is, the choice is committed once the left-hand side strategy has succeeded, even if the continuation strategy fails. This is expressed by the fact that the property ( s1 < + s2 ); s3 = ( s1 ; s3 ) < + ( s2 ; s3 ) // is not a law does not hold for all s1 , s2 , and s3 . The difference is illustrated by the following applications: <( B < + id ); B > P ( S ( Z ), Z ) // fails <( B ; B ) < + ( id ; B )> P ( S ( Z ()), Z ()) => P ( Z , S ( Z )) In the application of (B <+ id); B , the first application of B succeeds after which the choice is committed. The subsequent application of B then fails. This is equivalent to first applying (B <+ id) and then applying B to the result. The application of (B; B) <+ (id; B) , however, is successful; the application of B; B fails, after which the choice backtracks to id; B , which succeeds.","title":"Left Choice"},{"location":"background/stratego/strategy-combinators/sequential/#choosing-between-transformations","text":"The typical use of left choice is to create a composite strategy trying one from several possible transformations. If the strategies that are composed are mutually exclusive, that is, don\u2019t succeed for the same terms, their sum is a transformation that (deterministically) covers a larger set of terms. For example, consider the following two rewrite rules: PlusAssoc : Plus ( Plus ( e1 , e2 ), e3 ) - > Plus ( e1 , Plus ( e2 , e3 )) PlusZero : Plus ( Int ( \"0\" ), e ) - > e These rules are mutually exclusive, since there is no term that matches the left-hand sides of both rules. Combining the rules with left choice into PlusAssoc <+ PlusZero creates a strategy that transforms terms matching both rules as illustrated by the following applications: < PlusAssoc > Plus ( Int ( \"0\" ), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Int ( \"0\" ), Int ( \"3\" )) => Int ( \"3\" ) < PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) => Plus ( Var ( \"x\" ), Plus ( Int ( \"42\" ), Int ( \"3\" )))","title":"Choosing between Transformations."},{"location":"background/stratego/strategy-combinators/sequential/#ordering-overlapping-rules","text":"When two rules or strategies are mutually exlusive the order of applying them does not matter. In cases where strategies are overlapping, that is, succeed for the same terms, the order becomes crucial to determining the semantics of the composition. For example, consider the following rewrite rules reducing applications of Mem: Mem1 : Mem ( x ,[]) - > False () Mem2 : Mem ( x ,[ x | xs ]) - > True () Mem3 : Mem ( x ,[ y | ys ]) - > Mem ( x , ys ) Rules Mem2 and Mem3 have overlapping left-hand sides. Rule Mem2 only applies if the first argument is equal to the head element of the list in the second argument. Rule Mem3 applies always if the list in the second argument is non-empty. < Mem2 > Mem ( 1 , [ 1 , 2 , 3 ]) => True () < Mem3 > Mem ( 1 , [ 1 , 2 , 3 ]) => Mem ( 1 ,[ 2 , 3 ]) In such situations, depending on the order of the rules, different results are produced. (The rules form a non-confluent rewriting system.) By ordering the rules as Mem2 <+ Mem3 , rule Mem2 is tried before Mem3 , and we have a deterministic transformation strategy.","title":"Ordering Overlapping Rules."},{"location":"background/stratego/strategy-combinators/sequential/#try","text":"A useful application of <+ in combination with id is the reflexive closure of a strategy s: try ( s ) = s < + id The user-defined strategy combinator try tries to apply its argument strategy s , but if that fails, just succeeds using id .","title":"Try"},{"location":"background/stratego/strategy-combinators/sequential/#guarded-left-choice","text":"Sometimes it is not desirable to backtrack to the alternative specified in a choice. Rather, after passing a guard, the choice should be committed. This can be expressed using the guarded left choice operator s1 < s2 + s3 . If s1 succeeds s2 is applied, else s3 is applied. If s2 fails, the complete expression fails; no backtracking to s3 takes place. Properties. This combinator is a generalization of the left choice combinator <+ . s1 < + s2 = s1 < id + s2 The following laws make clear that the \u2018branches\u2019 of the choice are selected by the success or failure of the guard: id < s2 + s3 = s2 fail < s2 + s3 = s3 If the right branch always fails, the construct reduces to the sequential composition of the guard and the left branch. s1 < s2 + fail = s1 ; s2 Guarded choice is not associative: ( s1 < s2 + s3 ) < s4 + s5 = s1 < s2 + ( s3 < s4 + s5 ) // not a law To see why consider the possible traces of these expressions. For example, when s1 and s2 succeed subsequently, the left-hand side expression calls s4 , while the right-hand side expression does not. However, sequential composition distributes over guarded choice from left and right: ( s1 < s2 + s3 ); s4 = s1 < ( s2 ; s4 ) + ( s3 ; s4 ) s0 ; ( s1 < s2 + s3 ) = ( s0 ; s1 ) < s2 + s3 Examples. The guarded left choice operator is most useful for the implementation of higher-level control-flow strategies. For example, the negation not(s) of a strategy s , succeeds if s fails, and fails when it succeeds: not ( s ) = s < fail + id Since failure discards the effect of a (successful) transformation, this has the effect of testing whether s succeeds. So we have the following laws for not: not ( id ) = fail not ( fail ) = id However, side effects performed by s are not undone, of course. Therefore, the following equation does not hold: not ( not ( s )) = s // not a law Another example of the use of guarded choice is the restore-always combinator: restore-always ( s , r ) = s < r + ( r ; fail ) It applies a \u2018restore\u2019 strategy r after applying a strategy s , even if s fails, and preserves the success/failure behavior of s . Since fail discards the transformation effect of r , this is mostly useful for ensuring that some side-effecting operation is done (or undone) after applying s .","title":"Guarded Left Choice"},{"location":"background/stratego/strategy-combinators/sequential/#if-then-else","text":"The guarded choice combinator is similar to the traditional if-then-else construct of programming languages. The difference is that the \u2018then\u2019 branch applies to the result of the application of the condition. Stratego\u2019s if s1 then s2 else s3 end construct is more like the traditional construct since both branches apply to the original term. The condition strategy is only used to test if it succeeds or fails, but it\u2019s transformation effect is undone. However, the condition strategy s1 is still applied to the current term. The if s1 then s2 end strategy is similar; if the condition fails, the strategy succeeds. The if-then-else-end strategy is just syntactic sugar for a combination of guarded choice and the where combinator: if s1 then s2 else s3 end ==> // transforms to where ( s1 ) < s2 + s3 The strategy where(s) succeeds if s succeeds, but returns the original subject term. The implementation of the where combinator is discussed in the section on matching and building terms . The following laws show that the branches are selected by success or failure of the condition: if id then s2 else s3 end = s2 if fail then s2 else s3 end = s3 The if-then-end strategy is an abbreviation for the if-then-else-end with the identity strategy as right branch: if s1 then s2 end = where ( s1 ) < s2 + id Examples. The inclusive or or(s1, s2) succeeds if one of the strategies s1 or s2 succeeds, but guarantees that both are applied, in the order s1 first, then s2 : or ( s1 , s2 ) = if s1 then try ( where ( s2 )) else where ( s2 ) end This ensures that any side effects are always performed, in contrast to s1 <\\+ s2 , where s2 is only executed if s1 fails. (Thus, left choice implements a short circuit Boolean or.) Similarly, the following and(s1, s2) combinator is the non-short circuit version of Boolean conjunction: and ( s1 , s2 ) = if s1 then where ( s2 ) else where ( s2 ); fail end","title":"If-then-else"},{"location":"background/stratego/strategy-combinators/sequential/#switch","text":"The switch construct is an n-ary branching construct similar to its counter parts in other programming languages. It is defined in terms of guarded choice. The switch construct has the following form: switch s0 case s1 : s1 ' case s2 : s2 ' ... otherwise : sdef end The switch first applies the s0 strategy to the current term t resulting in a term t' . Then it tries the cases in turn applying each si to t' . As soon as this succeeds the corresponding case is selected and si' is applied to the t , the term to which the switch was applied. If none of the cases applies, the default strategy sdef from the otherwise is applied. The switch construct is syntactic sugar for a nested if-then-else: { x : where ( s0 => x ); if < s1 > x then s1 ' else if < s2 > x then s2 ' else if ... then ... else sdef end end end }","title":"Switch"},{"location":"background/stratego/strategy-combinators/sequential/#non-deterministic-choice","text":"The deterministic left choice operator prescribes that the left alternative should be tried before the right alternative, and that the latter is only used if the first fails. There are applications where it is not necessary to define the order of the alternatives. In those cases non-deterministic choice can be used. The non-deterministic choice operator s1 + s2 chooses one of the two strategies s1 or s2 to apply, such that the one it chooses succeeds. If both strategies fail, then the choice fails as well. Operationally the choice operator first tries one strategy, and, if that fails, tries the other. The order in which this is done is undefined, i.e., arbitrarily chosen by the compiler. The + combinator is used to model modular composition of rewrite rules and strategies with the same name, but in different modules. Multiple definitions with the same name in different modules, are merged into a single definition with that name, where the bodies are composed with + . The following transformation illustrates this: module A f = s1 module B f = s2 module main imports A B => f = s2 + s1 This transformation is somewhat simplified; the complete transformation needs to take care of local variables and parameters. While the + combinator is used internally by the compiler for this purpose, programmers are advised not to use this combinator, but rather use the left choice combinator <+ to avoid surprises. In the past, the + combinator was also used to compose definitions with the same name within a module. This has been replaced by interpreting such compositions with the textual order of the definitions. The following transformation illustrates this: module A f = s1 f = s2 => f = s1 < + s2","title":"Non-Deterministic Choice"},{"location":"background/stratego/strategy-combinators/sequential/#recursion","text":"Repeated application of a strategy can be achieved with recursion. There are two styles for doing this; with a recursive definition or using the fixpoint operator rec . A recursive definition is a normal strategy definition with a recursive call in its body. f ( s ) = ... f ( s ) ... Another way to define recursion is using the fixpoint operator rec x(s) , which recurses on applications of x within s. For example, the definition f ( s ) = rec x ( ... x ... ) is equivalent to the one above. The advantage of the rec operator is that it allows the definition of an unnamed strategy expression to be recursive. For example, in the definition g ( s ) = foo ; rec x ( ... x ... ); bar the strategy between foo and bar is a recursive strategy that does not recurse to g(s) . Originally, the rec operator was the only way to define recursive strategies. It is still in the language in the first place because it is widely used in many existing programs, and in the second place because it can be a concise expression of a recursive strategy, since call parameters are not included in the call. Furthermore, all free variables remain in scope. The repeat strategy applies a transformation s until it fails. It is defined as a recursive definition using try as follows: try ( s ) = s < + id repeat ( s ) = try ( s ; repeat ( s )) An equivalent definition using rec is: repeat ( s ) = rec x ( try ( s ; x ))","title":"Recursion"},{"location":"background/stratego/strategy-combinators/sequential/#a-library-of-iteration-strategies","text":"Using sequential composition, choice, and recursion a large variety of iteration strategies can be defined. The following definitions are part of the Stratego Library (in module strategy/iteration ). repeat ( s ) = rec x ( try ( s ; x )) repeat ( s , c ) = ( s ; repeat ( s , c )) < + c repeat1 ( s , c ) = s ; ( repeat1 ( s , c ) < + c ) repeat1 ( s ) = repeat1 ( s , id ) repeat-until ( s , c ) = s ; if c then id else repeat-until ( s , c ) end while ( c , s ) = if c then s ; while ( c , s ) end do-while ( s , c ) = s ; if c then do-while ( s , c ) end","title":"A Library of Iteration Strategies."},{"location":"background/stratego/strategy-combinators/sequential/#references","text":"Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661 \\(05\\) 80027-1 , doi:10.1016/S1571-0661 \\(05\\) 80027-1 . \u21a9","title":"References"},{"location":"background/stratego/strategy-combinators/term/","text":"Term Combinators \u00b6 Previously we have presented rewrite rules as basic transformation steps. However, rules are not atomic transformation actions. To see this, consider what happens when the rewrite rule DAO : And ( Or ( x , y ), z ) - > Or ( And ( x , z ), And ( y , z )) is applied. First it matches the subject term against the pattern And(Or(x, y), z) in the left-hand side. This means that a substitution for the variables x , y , and z is sought, that makes the pattern equal to the subject term. If the match fails, the rule fails. If the match succeeds, the pattern Or(And(x, z), And(y, z)) on the right-hand side is instantiated with the bindings found during the match of the left-hand side. The instantiated term then replaces the original subject term. Furthermore, the rule limits the scope of the variables occurring in the rule. That is, the variables x , y , z are local to this rule. After the rule is applied the bindings to these variables are invisible again. Thus, rather than considering rules as the atomic actions of transformation programs, Stratego provides their constituents, that is building terms from patterns and matching terms against patterns, as atomic actions, and makes these available to the programmer. In this section we define the basic actions and their use in the composition of more complex operations such as various flavors of rewrite rules. Building Terms \u00b6 The build operation !p replaces the subject term with the instantiation of the pattern p using the bindings from the environment to the variables occurring in p . For example, the strategy !Or(And(x, z), And(y, z)) replaces the subject term with the instantiation of Or(And(x, z), And(y, z)) using bindings to variables x , y and z . ! Int ( \"10\" ) => Int ( \"10\" ) ! Plus ( Var ( \"a\" ), Int ( \"10\" )) => Plus ( Var ( \"a\" ), Int ( \"10\" )) It is possible to build terms with variables. We call this building a term pattern. A pattern is a term with meta-variables. The current term is replaced by an instantiation of pattern p . For example, in a context where e is bound to Var(\"b\") ! Plus ( Var ( \"a\" ), e ) => Plus ( Var ( \"a\" ), Var ( \"b\" )) Matching Terms \u00b6 Pattern matching allows the analysis of terms. The simplest case is matching against a literal term. The match operation ?t matches the subject term against the term t . < ? Plus ( Var ( \"a\" ), Int ( \"3\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // succeeds < ? Plus ( Int ( \"3\" ), Var ( \"b\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails Matching against a term pattern with variables binds those variables to (parts of) the current term. The match strategy ?x compares the current term ( t ) to variable x . It binds variable x to term t in the environment. A variable can only be bound once, or to the same term. < ? e > Plus ( Var ( \"a\" ), Int ( \"3\" )) // binds e to Plus(Var(\"a\"),Int(\"3\")) < ? e > ! Int ( \"17\" ) // fails The general case is matching against an arbitrary term pattern. The match strategy ?p compares the current term to a pattern p . It will add bindings for the variables in pattern p to the environment. The wildcard _ in a match will match any term. < ? Plus ( e , _ )> Plus ( Var ( \"a\" ), Int ( \"3\" )) // e is bound to Var(\"a\") Patterns may be non-linear. Multiple occurrences of the same variable can occur and each occurrence has to match the same term. < ? Plus ( e , e )> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails < ? Plus ( e , e )> ! Plus ( Var ( \"a\" ), Var ( \"a\" )) // e is bound to Var(\"a\") Non-linear pattern matching is a way to test equality of terms. Indeed the equality predicates from the Stratego Library are defined using non-linear pattern matching: equal = ? ( x , x ) equal (| x ) = ? x The equal strategy tests whether the current term is a a pair of the same terms. The equal(|x) strategy tests whether the current term is equal to the argument term. < equal >( \"a\" , \"a\" ) // succeeds < equal >( \"a\" , \"b\" ) // fails < equal (| Foo ( Baz ()))> Foo ( Bar ()) // fails < equal (| Foo ( Bar ()))> Foo ( Bar ()) // succeeds Implementing Rewrite Rules \u00b6 Match and build are first-class citizens in Stratego, which means that they can be used and combined just like any other strategy expressions. In particular, we can implement rewrite rules using these operations, since a rewrite rule is basically a match followed by a build. For example, consider the following combination of match and build: < ? Plus ( e1 , e2 ); ! Plus ( e2 , e1 )> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) This combination first recognizes a term, binds variables to the pattern in the match, and then replaces the current term with the instantiation of the build pattern. Note that the variable bindings are propagated from the match to the build. Stratego provides syntactic sugar for various combinations of match and build. Anonymous Rewrite Rule \u00b6 An anonymous rewrite rule (p1 -> p2) transforms a term matching p1 into an instantiation of p2 . Such a rule is equivalent to the sequence ?p1; !p2 . <( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) Term variable scope \u00b6 Once a variable is bound it cannot be rebound to a different term. Thus, when we have applied an anonymous rule once, its variables are bound and the next time it is applied it only succeeds for the same term. For example, in the next session the second application of the rule fails, because e2 is bound to Int(\"3\") and does not match with Var(\"b\") . <( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) // e1 is bound to Var(\"a\") // e2 is bound to Int(\"3\") <( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))> Plus ( Var ( \"a\" ), Var ( \"b\" )) // fails To use a variable name more than once Stratego provides term variable scope. A scope {x1,...,xn : s} locally undefines the variables xi . That is, the binding to a variable xi outside the scope is not visible inside it, nor is the binding to xi inside the scope visible outside it. For example, to continue the session above, if we wrap the anonymous swap rule in a scope for its variables, it can be applied multiple times. <{ e3 , e4 : ( Plus ( e3 , e4 ) - > Plus ( e4 , e3 ))}> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) // e3 is not bound to a term <{ e3 , e4 : ( Plus ( e3 , e4 ) - > Plus ( e4 , e3 ))}> Plus ( Var ( \"a\" ), Var ( \"b\" )) => Plus ( Var ( \"b\" ), Var ( \"a\" )) Of course we can name such a scoped rule using a strategy definition, and then invoke it by its name: SwapArgs = { e1 , e2 : ( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))} < SwapArgs > Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) Implicit Variable Scope \u00b6 When using match and build directly in a strategy definition, rather than in the form of a rule, the definition contains free variables. Strictly speaking such variables should be declared using a scope, that is one should write SwapArgs = { e1 , e2 : ( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))} However, since declaring all variables at the top of a definition is distracting and does not add much to the definition, such a scope declaration can be left out. Thus, one can write SwapArgs = ( Plus ( e1 , e2 ) - > Plus ( e2 , e1 )) instead. The scope is automatically inserted by the compiler. This implies that there is no global scope for term variables. Of course, variables in inner scopes should be declared where necessary. In particular, note that variable scope is not inserted for strategy definitions in a let binding, such as let SwapArgs = ( Plus ( e1 , e2 ) - > Plus ( e2 , e1 )) in ... end While the variables are bound in the enclosing definition, they are not restricted to SwapArgs in this case, since in a let you typically want to use bindings to variables in the enclosing code. Where \u00b6 Often it is useful to apply a strategy only to test whether some property holds or to compute some auxiliary result. For this purpose, Stratego provides the where(s) combinator, which applies s to the current term, but restores that term afterwards. Any bindings to variables are kept, however. < where ( ? Plus ( Int ( i ), Int ( j )); < addS >( i , j ) => k )> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Int ( \"14\" ), Int ( \"3\" )) // i is bound to \"14\" // k is bound to \"17\" With the match and build constructs where(s) is in fact just syntactic sugar for {x: ?x; s; !x} with x a fresh variable not occurring in s . Thus, the current subject term is saved by binding it to a new variable x , then the strategy s is applied, and finally, the original term is restored by building x . Conditional Rewrite Rules \u00b6 A simple rewrite rule succeeds if the match of the left-hand side succeeds. Sometimes it is useful to place additional requirements on the application of a rule, or to compute some value for use in the right-hand side of the rule. This can be achieved with conditional rewrite rules. A conditional rule L: p1 -> p2 where s is a simple rule extended with an additional computation s which should succeed in order for the rule to apply. The condition can be used to test properties of terms in the left-hand side, or to compute terms to be used in the right-hand side. The latter is done by binding such new terms to variables used in the right-hand side. For example, the EvalPlus rule in the following session uses a condition to compute the sum of i and j : EvalPlus : Plus ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k < EvalPlus > Plus ( Int ( \"14\" ), Int ( \"3\" )) => Int ( \"17\" ) A conditional rule can be desugared similarly to an unconditional rule. That is, a conditional rule of the form L : p1 - > p2 where s is syntactic sugar for L = ? p1 ; where ( s ); ! p2 Thus, after the match with p1 succeeds the strategy s is applied to the subject term. Only if the application of s succeeds, is the right-hand side p2 built. Note that since s is applied within a where, the build !p2 is applied to the original subject term; only variable bindings computed within s can be used in p2 . As an example, consider the following constant folding rule, which reduces an addition of two integer constants to the constant obtained by computing the addition. EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k The addition is computed by applying the primitive strategy addS to the pair of integers (i,j) and matching the result against the variable k , which is then used in the right-hand side. This rule is desugared to EvalPlus = ? Add ( Int ( i ), Int ( j )); where ( ! ( i , j ); addS ; ? k ); ! Int ( k ) Lambda Rules \u00b6 Sometimes it is useful to define a rule anonymously within a strategy expression. The syntax for anonymous rules with scopes is a bit much since it requires enumerating all variables. A lambda rule of the form \\ p1 - > p2 where s \\ is an anonymous rewrite rule for which the variables in the left-hand side p1 are local to the rule, that is, it is equivalent to an expression of the form { x1 , ... , xn : ( p1 - > p2 where s )} with x1 ,\u2026, xn the variables of p1 . This means that any variables used in s and p2 that do not occur in p1 are bound in the context of the rule. A typical example of the use of an anonymous rule is < map ( \\ ( x , y ) - > x \\ )> [( 1 , 2 ),( 3 , 4 ),( 5 , 6 )] => [ 1 , 3 , 5 ] Apply and Match \u00b6 One frequently occuring scenario is that of applying a strategy to a term and then matching the result against a pattern. This typically occurs in the condition of a rule. In the constant folding example above we saw this scenario: EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k In the condition, first the term (i,j) is built, then the strategy addS is applied to it, and finally the result is matched against the pattern k . To improve the readability of such expressions, the following two constructs are provided. The operation <s> p captures the notion of applying a strategy to a term, i.e., the scenario !p; s . The operation s => p capture the notion of applying a strategy to the current subject term and then matching the result against the pattern p , i.e., s; ?p . The combined operation <s> p1 => p2 thus captures the notion of applying a strategy to a term p1 and matching the result against p2 , i.e, !p1; s; ?p2 . Using this notation we can improve the constant folding rule above as EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where < addS >( i , j ) => k Applying Strategies in Build \u00b6 Sometimes it useful to apply a strategy directly to a subterm of a pattern, for example in the right-hand side of a rule, instead of computing a value in a condition, binding the result to a variable, and then using the variable in the build pattern. The constant folding rule above, for example, could be further simplified by directly applying the addition in the right-hand side: EvalPlus : Add ( Int ( i ), Int ( j )) - > Int (< addS >( i , j )) This abbreviates the conditional rule above. In general, a strategy application in a build pattern can always be expressed by computing the application before the build and binding the result to a new variable, which then replaces the application in the build pattern. Another example is the following definition of the map(s) strategy, which applies a strategy to each term in a list: map ( s ) : [] - > [] map ( s ) : [ x | xs ] - > [< s > x | < map ( s )> xs ] Auxiliary Values \u00b6 As mentioned above, it can be convenient to apply a strategy only to compute some auxiliary result. Although the where construct created to constrain when a rule or strategy may apply (as covered above) can be used for this purpose, often it is better to use the with strategy specifically designed with computing auxiliaries in mind. Specifically, if s is any strategy, the strategy with(s) executes s on the current subject term and then restores the current subject term. In other words, s is executed solely for its side effects, such as binding variables. In this respect, with is like where . However, with(s) differs in a key way: if the strategy s fails, Stratego immediately stops with an error, reporting the strategy that failed. Thus, if with(s) is used for auxiliary computations that really should not fail if the transformation is proceeding properly, there is no opportunity for Stratego to backtrack and/or continue applying other strategies, potentially creating an error at a point far removed from the place that things actually went awry. In short, using with(s) instead of where(s) any time the intention is not to constrain the applicability of a rule or strategy generally makes debugging your Stratego program significantly easier. Also as with where , we can add a with clause to a rewrite rule in exactly the same way. In other words, L : p1 - > p2 with s is syntactic sugar for L = ? p1 ; with ( s ); ! p2 So as an example, the where version of EvalPlus above would be better cast as EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) with < addS >( i , j ) => k because after all, there is no chance that Stratego will be unable to add two integers, and so if the contents of the with clause fails it means something has gone wrong \u2013 perhaps an Int term somehow ended up with a parameter that does not actually represent an integer \u2013 and Stratego should quit now. Assignment \u00b6 The assignment combinator := is a variation of apply-and-match with terms on both sides of the assignment. The strategy p1 := p2 builds p2 and matches the result against p1 , i.e. it is syntactic sugar for !p2; ?p1 . The strategy is often combine with strategy application into p1 := <s>p2 , which is equivalent to <s>p2 => p1 , but more familiar to an audience with an imperative mindset. For example, consider the following rewrite rule EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) with k := < addS >( i , j ) Term Wrap \u00b6 One often write rules of the form x -> Foo(Bar(x)) , i.e. wrapping a term pattern around the current term. Using rule syntax this is quite verbose. The syntactic abstraction of term wraps, allows the concise specification of such little transformations as !Foo(Bar(<id>)) . In general, a term wrap is a build strategy !p[<s>] containing one or more strategy applications <s> that are not applied to a term. When executing the the build operation, each occurrence of such a strategy application <s> is replaced with the term resulting from applying s to the current subject term, i.e., the one that is being replaced by the build. The following applications illustrate some uses of term wraps: < ! (< id >,< id >)> 3 => ( 3 , 3 ) <(< Fst ; inc >,< Snd >)> ( 3 , 3 ) => ( 4 , 3 ) < ! Call (< id >, [])> \"foobar\" => Call ( \"foobar\" , []) mod2 = < mod >(< id >, 2 ) < mod2 > 6 => 0 As should now be a common pattern, term projects are implemented by translation to a combination of match and build expressions. Thus, a term wrap !p[<s>] is translated to a strategy expression { x : where ( s => x ); ! p [ x ]} where x is a fresh variable not occurring in s . In other words, the strategy s is applied to the current subject term, i.e., the term to which the build is applied. As an example, the term wrap !Foo(Bar(<id>)) is desugared to the strategy { x : where ( id => x ); ! Foo ( Bar ( x ))} which after simplification is equivalent to {x: ?x; !Foo(Bar(x))} , i.e., exactly the original lambda rule \\x -> Foo(Bar(x))\\ . Term Project \u00b6 Term projections are the match dual of term wraps. Term projections can be used to project a subterm from a term pattern. For example, the expression ?And(<id>,x) matches terms of the form And(t1,t2) and reduces them to the first subterm t1 . Another example is the strategy map ( ? FunDec (< id >, _ , _ )) which reduces a list of function declarations to a list of the names of the functions, i.e., the first arguments of the FunDec constructor. Here are some more examples: < ? [ _ |< id >]> [ 1 , 2 , 3 ] => [ 2 , 3 ] < ? Call (< id >, [])> Call ( \"foobar\" , []) => \"foobar\" Term projections can also be used to apply additional constraints to subterms in a match pattern. For example, ?Call(x, <?args; length => 3>) matches only with function calls with three arguments. A match expression ?p[<s>] is desugared as { x : ? p [ x ]; < s > x } That is, after the pattern p[x] matches, it is reduced to the subterm bound to x to which s is applied. The result is also the result of the projection. When multiple projects are used within a match the outcome is undefined, i.e., the order in which the projects will be performed can not be counted on.","title":"Term Combinators"},{"location":"background/stratego/strategy-combinators/term/#term-combinators","text":"Previously we have presented rewrite rules as basic transformation steps. However, rules are not atomic transformation actions. To see this, consider what happens when the rewrite rule DAO : And ( Or ( x , y ), z ) - > Or ( And ( x , z ), And ( y , z )) is applied. First it matches the subject term against the pattern And(Or(x, y), z) in the left-hand side. This means that a substitution for the variables x , y , and z is sought, that makes the pattern equal to the subject term. If the match fails, the rule fails. If the match succeeds, the pattern Or(And(x, z), And(y, z)) on the right-hand side is instantiated with the bindings found during the match of the left-hand side. The instantiated term then replaces the original subject term. Furthermore, the rule limits the scope of the variables occurring in the rule. That is, the variables x , y , z are local to this rule. After the rule is applied the bindings to these variables are invisible again. Thus, rather than considering rules as the atomic actions of transformation programs, Stratego provides their constituents, that is building terms from patterns and matching terms against patterns, as atomic actions, and makes these available to the programmer. In this section we define the basic actions and their use in the composition of more complex operations such as various flavors of rewrite rules.","title":"Term Combinators"},{"location":"background/stratego/strategy-combinators/term/#building-terms","text":"The build operation !p replaces the subject term with the instantiation of the pattern p using the bindings from the environment to the variables occurring in p . For example, the strategy !Or(And(x, z), And(y, z)) replaces the subject term with the instantiation of Or(And(x, z), And(y, z)) using bindings to variables x , y and z . ! Int ( \"10\" ) => Int ( \"10\" ) ! Plus ( Var ( \"a\" ), Int ( \"10\" )) => Plus ( Var ( \"a\" ), Int ( \"10\" )) It is possible to build terms with variables. We call this building a term pattern. A pattern is a term with meta-variables. The current term is replaced by an instantiation of pattern p . For example, in a context where e is bound to Var(\"b\") ! Plus ( Var ( \"a\" ), e ) => Plus ( Var ( \"a\" ), Var ( \"b\" ))","title":"Building Terms"},{"location":"background/stratego/strategy-combinators/term/#matching-terms","text":"Pattern matching allows the analysis of terms. The simplest case is matching against a literal term. The match operation ?t matches the subject term against the term t . < ? Plus ( Var ( \"a\" ), Int ( \"3\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // succeeds < ? Plus ( Int ( \"3\" ), Var ( \"b\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails Matching against a term pattern with variables binds those variables to (parts of) the current term. The match strategy ?x compares the current term ( t ) to variable x . It binds variable x to term t in the environment. A variable can only be bound once, or to the same term. < ? e > Plus ( Var ( \"a\" ), Int ( \"3\" )) // binds e to Plus(Var(\"a\"),Int(\"3\")) < ? e > ! Int ( \"17\" ) // fails The general case is matching against an arbitrary term pattern. The match strategy ?p compares the current term to a pattern p . It will add bindings for the variables in pattern p to the environment. The wildcard _ in a match will match any term. < ? Plus ( e , _ )> Plus ( Var ( \"a\" ), Int ( \"3\" )) // e is bound to Var(\"a\") Patterns may be non-linear. Multiple occurrences of the same variable can occur and each occurrence has to match the same term. < ? Plus ( e , e )> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails < ? Plus ( e , e )> ! Plus ( Var ( \"a\" ), Var ( \"a\" )) // e is bound to Var(\"a\") Non-linear pattern matching is a way to test equality of terms. Indeed the equality predicates from the Stratego Library are defined using non-linear pattern matching: equal = ? ( x , x ) equal (| x ) = ? x The equal strategy tests whether the current term is a a pair of the same terms. The equal(|x) strategy tests whether the current term is equal to the argument term. < equal >( \"a\" , \"a\" ) // succeeds < equal >( \"a\" , \"b\" ) // fails < equal (| Foo ( Baz ()))> Foo ( Bar ()) // fails < equal (| Foo ( Bar ()))> Foo ( Bar ()) // succeeds","title":"Matching Terms"},{"location":"background/stratego/strategy-combinators/term/#implementing-rewrite-rules","text":"Match and build are first-class citizens in Stratego, which means that they can be used and combined just like any other strategy expressions. In particular, we can implement rewrite rules using these operations, since a rewrite rule is basically a match followed by a build. For example, consider the following combination of match and build: < ? Plus ( e1 , e2 ); ! Plus ( e2 , e1 )> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) This combination first recognizes a term, binds variables to the pattern in the match, and then replaces the current term with the instantiation of the build pattern. Note that the variable bindings are propagated from the match to the build. Stratego provides syntactic sugar for various combinations of match and build.","title":"Implementing Rewrite Rules"},{"location":"background/stratego/strategy-combinators/term/#anonymous-rewrite-rule","text":"An anonymous rewrite rule (p1 -> p2) transforms a term matching p1 into an instantiation of p2 . Such a rule is equivalent to the sequence ?p1; !p2 . <( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" ))","title":"Anonymous Rewrite Rule"},{"location":"background/stratego/strategy-combinators/term/#term-variable-scope","text":"Once a variable is bound it cannot be rebound to a different term. Thus, when we have applied an anonymous rule once, its variables are bound and the next time it is applied it only succeeds for the same term. For example, in the next session the second application of the rule fails, because e2 is bound to Int(\"3\") and does not match with Var(\"b\") . <( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) // e1 is bound to Var(\"a\") // e2 is bound to Int(\"3\") <( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))> Plus ( Var ( \"a\" ), Var ( \"b\" )) // fails To use a variable name more than once Stratego provides term variable scope. A scope {x1,...,xn : s} locally undefines the variables xi . That is, the binding to a variable xi outside the scope is not visible inside it, nor is the binding to xi inside the scope visible outside it. For example, to continue the session above, if we wrap the anonymous swap rule in a scope for its variables, it can be applied multiple times. <{ e3 , e4 : ( Plus ( e3 , e4 ) - > Plus ( e4 , e3 ))}> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) // e3 is not bound to a term <{ e3 , e4 : ( Plus ( e3 , e4 ) - > Plus ( e4 , e3 ))}> Plus ( Var ( \"a\" ), Var ( \"b\" )) => Plus ( Var ( \"b\" ), Var ( \"a\" )) Of course we can name such a scoped rule using a strategy definition, and then invoke it by its name: SwapArgs = { e1 , e2 : ( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))} < SwapArgs > Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" ))","title":"Term variable scope"},{"location":"background/stratego/strategy-combinators/term/#implicit-variable-scope","text":"When using match and build directly in a strategy definition, rather than in the form of a rule, the definition contains free variables. Strictly speaking such variables should be declared using a scope, that is one should write SwapArgs = { e1 , e2 : ( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))} However, since declaring all variables at the top of a definition is distracting and does not add much to the definition, such a scope declaration can be left out. Thus, one can write SwapArgs = ( Plus ( e1 , e2 ) - > Plus ( e2 , e1 )) instead. The scope is automatically inserted by the compiler. This implies that there is no global scope for term variables. Of course, variables in inner scopes should be declared where necessary. In particular, note that variable scope is not inserted for strategy definitions in a let binding, such as let SwapArgs = ( Plus ( e1 , e2 ) - > Plus ( e2 , e1 )) in ... end While the variables are bound in the enclosing definition, they are not restricted to SwapArgs in this case, since in a let you typically want to use bindings to variables in the enclosing code.","title":"Implicit Variable Scope"},{"location":"background/stratego/strategy-combinators/term/#where","text":"Often it is useful to apply a strategy only to test whether some property holds or to compute some auxiliary result. For this purpose, Stratego provides the where(s) combinator, which applies s to the current term, but restores that term afterwards. Any bindings to variables are kept, however. < where ( ? Plus ( Int ( i ), Int ( j )); < addS >( i , j ) => k )> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Int ( \"14\" ), Int ( \"3\" )) // i is bound to \"14\" // k is bound to \"17\" With the match and build constructs where(s) is in fact just syntactic sugar for {x: ?x; s; !x} with x a fresh variable not occurring in s . Thus, the current subject term is saved by binding it to a new variable x , then the strategy s is applied, and finally, the original term is restored by building x .","title":"Where"},{"location":"background/stratego/strategy-combinators/term/#conditional-rewrite-rules","text":"A simple rewrite rule succeeds if the match of the left-hand side succeeds. Sometimes it is useful to place additional requirements on the application of a rule, or to compute some value for use in the right-hand side of the rule. This can be achieved with conditional rewrite rules. A conditional rule L: p1 -> p2 where s is a simple rule extended with an additional computation s which should succeed in order for the rule to apply. The condition can be used to test properties of terms in the left-hand side, or to compute terms to be used in the right-hand side. The latter is done by binding such new terms to variables used in the right-hand side. For example, the EvalPlus rule in the following session uses a condition to compute the sum of i and j : EvalPlus : Plus ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k < EvalPlus > Plus ( Int ( \"14\" ), Int ( \"3\" )) => Int ( \"17\" ) A conditional rule can be desugared similarly to an unconditional rule. That is, a conditional rule of the form L : p1 - > p2 where s is syntactic sugar for L = ? p1 ; where ( s ); ! p2 Thus, after the match with p1 succeeds the strategy s is applied to the subject term. Only if the application of s succeeds, is the right-hand side p2 built. Note that since s is applied within a where, the build !p2 is applied to the original subject term; only variable bindings computed within s can be used in p2 . As an example, consider the following constant folding rule, which reduces an addition of two integer constants to the constant obtained by computing the addition. EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k The addition is computed by applying the primitive strategy addS to the pair of integers (i,j) and matching the result against the variable k , which is then used in the right-hand side. This rule is desugared to EvalPlus = ? Add ( Int ( i ), Int ( j )); where ( ! ( i , j ); addS ; ? k ); ! Int ( k )","title":"Conditional Rewrite Rules"},{"location":"background/stratego/strategy-combinators/term/#lambda-rules","text":"Sometimes it is useful to define a rule anonymously within a strategy expression. The syntax for anonymous rules with scopes is a bit much since it requires enumerating all variables. A lambda rule of the form \\ p1 - > p2 where s \\ is an anonymous rewrite rule for which the variables in the left-hand side p1 are local to the rule, that is, it is equivalent to an expression of the form { x1 , ... , xn : ( p1 - > p2 where s )} with x1 ,\u2026, xn the variables of p1 . This means that any variables used in s and p2 that do not occur in p1 are bound in the context of the rule. A typical example of the use of an anonymous rule is < map ( \\ ( x , y ) - > x \\ )> [( 1 , 2 ),( 3 , 4 ),( 5 , 6 )] => [ 1 , 3 , 5 ]","title":"Lambda Rules"},{"location":"background/stratego/strategy-combinators/term/#apply-and-match","text":"One frequently occuring scenario is that of applying a strategy to a term and then matching the result against a pattern. This typically occurs in the condition of a rule. In the constant folding example above we saw this scenario: EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k In the condition, first the term (i,j) is built, then the strategy addS is applied to it, and finally the result is matched against the pattern k . To improve the readability of such expressions, the following two constructs are provided. The operation <s> p captures the notion of applying a strategy to a term, i.e., the scenario !p; s . The operation s => p capture the notion of applying a strategy to the current subject term and then matching the result against the pattern p , i.e., s; ?p . The combined operation <s> p1 => p2 thus captures the notion of applying a strategy to a term p1 and matching the result against p2 , i.e, !p1; s; ?p2 . Using this notation we can improve the constant folding rule above as EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where < addS >( i , j ) => k","title":"Apply and Match"},{"location":"background/stratego/strategy-combinators/term/#applying-strategies-in-build","text":"Sometimes it useful to apply a strategy directly to a subterm of a pattern, for example in the right-hand side of a rule, instead of computing a value in a condition, binding the result to a variable, and then using the variable in the build pattern. The constant folding rule above, for example, could be further simplified by directly applying the addition in the right-hand side: EvalPlus : Add ( Int ( i ), Int ( j )) - > Int (< addS >( i , j )) This abbreviates the conditional rule above. In general, a strategy application in a build pattern can always be expressed by computing the application before the build and binding the result to a new variable, which then replaces the application in the build pattern. Another example is the following definition of the map(s) strategy, which applies a strategy to each term in a list: map ( s ) : [] - > [] map ( s ) : [ x | xs ] - > [< s > x | < map ( s )> xs ]","title":"Applying Strategies in Build"},{"location":"background/stratego/strategy-combinators/term/#auxiliary-values","text":"As mentioned above, it can be convenient to apply a strategy only to compute some auxiliary result. Although the where construct created to constrain when a rule or strategy may apply (as covered above) can be used for this purpose, often it is better to use the with strategy specifically designed with computing auxiliaries in mind. Specifically, if s is any strategy, the strategy with(s) executes s on the current subject term and then restores the current subject term. In other words, s is executed solely for its side effects, such as binding variables. In this respect, with is like where . However, with(s) differs in a key way: if the strategy s fails, Stratego immediately stops with an error, reporting the strategy that failed. Thus, if with(s) is used for auxiliary computations that really should not fail if the transformation is proceeding properly, there is no opportunity for Stratego to backtrack and/or continue applying other strategies, potentially creating an error at a point far removed from the place that things actually went awry. In short, using with(s) instead of where(s) any time the intention is not to constrain the applicability of a rule or strategy generally makes debugging your Stratego program significantly easier. Also as with where , we can add a with clause to a rewrite rule in exactly the same way. In other words, L : p1 - > p2 with s is syntactic sugar for L = ? p1 ; with ( s ); ! p2 So as an example, the where version of EvalPlus above would be better cast as EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) with < addS >( i , j ) => k because after all, there is no chance that Stratego will be unable to add two integers, and so if the contents of the with clause fails it means something has gone wrong \u2013 perhaps an Int term somehow ended up with a parameter that does not actually represent an integer \u2013 and Stratego should quit now.","title":"Auxiliary Values"},{"location":"background/stratego/strategy-combinators/term/#assignment","text":"The assignment combinator := is a variation of apply-and-match with terms on both sides of the assignment. The strategy p1 := p2 builds p2 and matches the result against p1 , i.e. it is syntactic sugar for !p2; ?p1 . The strategy is often combine with strategy application into p1 := <s>p2 , which is equivalent to <s>p2 => p1 , but more familiar to an audience with an imperative mindset. For example, consider the following rewrite rule EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) with k := < addS >( i , j )","title":"Assignment"},{"location":"background/stratego/strategy-combinators/term/#term-wrap","text":"One often write rules of the form x -> Foo(Bar(x)) , i.e. wrapping a term pattern around the current term. Using rule syntax this is quite verbose. The syntactic abstraction of term wraps, allows the concise specification of such little transformations as !Foo(Bar(<id>)) . In general, a term wrap is a build strategy !p[<s>] containing one or more strategy applications <s> that are not applied to a term. When executing the the build operation, each occurrence of such a strategy application <s> is replaced with the term resulting from applying s to the current subject term, i.e., the one that is being replaced by the build. The following applications illustrate some uses of term wraps: < ! (< id >,< id >)> 3 => ( 3 , 3 ) <(< Fst ; inc >,< Snd >)> ( 3 , 3 ) => ( 4 , 3 ) < ! Call (< id >, [])> \"foobar\" => Call ( \"foobar\" , []) mod2 = < mod >(< id >, 2 ) < mod2 > 6 => 0 As should now be a common pattern, term projects are implemented by translation to a combination of match and build expressions. Thus, a term wrap !p[<s>] is translated to a strategy expression { x : where ( s => x ); ! p [ x ]} where x is a fresh variable not occurring in s . In other words, the strategy s is applied to the current subject term, i.e., the term to which the build is applied. As an example, the term wrap !Foo(Bar(<id>)) is desugared to the strategy { x : where ( id => x ); ! Foo ( Bar ( x ))} which after simplification is equivalent to {x: ?x; !Foo(Bar(x))} , i.e., exactly the original lambda rule \\x -> Foo(Bar(x))\\ .","title":"Term Wrap"},{"location":"background/stratego/strategy-combinators/term/#term-project","text":"Term projections are the match dual of term wraps. Term projections can be used to project a subterm from a term pattern. For example, the expression ?And(<id>,x) matches terms of the form And(t1,t2) and reduces them to the first subterm t1 . Another example is the strategy map ( ? FunDec (< id >, _ , _ )) which reduces a list of function declarations to a list of the names of the functions, i.e., the first arguments of the FunDec constructor. Here are some more examples: < ? [ _ |< id >]> [ 1 , 2 , 3 ] => [ 2 , 3 ] < ? Call (< id >, [])> Call ( \"foobar\" , []) => \"foobar\" Term projections can also be used to apply additional constraints to subterms in a match pattern. For example, ?Call(x, <?args; length => 3>) matches only with function calls with three arguments. A match expression ?p[<s>] is desugared as { x : ? p [ x ]; < s > x } That is, after the pattern p[x] matches, it is reduced to the subterm bound to x to which s is applied. The result is also the result of the projection. When multiple projects are used within a match the outcome is undefined, i.e., the order in which the projects will be performed can not be counted on.","title":"Term Project"},{"location":"background/stratego/strategy-combinators/traversal/","text":"Traversal Combinators \u00b6 There are many ways to traverse a tree. For example, a bottom-up traversal, visits the subterms of a node before it visits the node itself, while a top-down traversal visits nodes before it visits children. One-pass traversals traverse the tree one time, while fixed-point traversals, such as innermost, repeatedly traverse a term until a normal form is reached. Rather than provide built-in implementations for all traversals needed in transformations, Stratego defines traversals in terms of the primitive ingredients of traversal. For example, a top-down, one-pass traversal strategy will first visit a node, and then descend to the children of a node in order to recursively traverse all subterms. Similarly, the bottom-up, fixed-point traversal strategy innermost, will first descend to the children of a node in order to recursively traverse all subterms, then visit the node itself, and possibly recursively reapply the strategy. Traversal in Stratego is based on the observation 1 that a full term traversal is a recursive closure of a one-step descent, that is, an operation that applies a strategy to one or more direct subterms of the subject term. By separating this one-step descent operator from recursion, and making it a first-class operation, many different traversals can be defined. Here we explore the ways in which Stratego supports the definition of traversal strategies. We start with explicitly programmed traversals using recursive traversal rules. Next, congruences operators provide a more concise notation for such data-type specific traversal rules. Finally, generic traversal operators support data type independent definitions of traversals, which can be reused for any data type. Given these basic mechanisms, we conclude with an exploration of idioms for traversal and standard traversal strategies in the Stratego Library. Congruence Operators \u00b6 Congruence operators provide a convenient abbreviation of traversal with rewrite rules . A congruence operator applies a strategy to each direct subterm of a specific constructor. For each n-ary constructor c declared in a signature, there is a corresponding congruence operator c(s1 , ..., sn) , which applies to terms of the form c(t1 , ..., tn) by applying the argument strategies to the corresponding argument terms. A congruence fails if the application of one the argument strategies fails or if constructor of the operator and that of the term do not match. For example, consider the following signature of expressions: module expressions signature sorts Exp constructors Int : String - > Exp Var : String - > Exp Plus : Exp * Exp - > Exp Times : Exp * Exp - > Exp The following applications apply the congruence operators Plus and Times to a term: < Plus ( ! Var ( \"a\" ), id )> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) < Times ( id , ! Int ( \"42\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails The first application shows how a congruence transforms a specific subterm, that is the strategy applied can be different for each subterm. The second application shows that a congruence only succeeds for terms constructed with the same constructor. Defining Traversals with Congruences \u00b6 Since congruence operators define a one-step traversal for a specific constructor, they capture the pattern of traversal rules . That is, a traversal rule such as proptr ( s ) : And ( x , y ) - > And (< s > x , < s > y ) can be written by the congruence And(s,s) . Applying this to the prop-dnf program we can replace the traversal rules by congruences as follows: module prop-dnf10 imports prop-rules strategies proptr ( s ) = Not ( s ) < + And ( s , s ) < + Or ( s , s ) < + Impl ( s , s ) < + Eq ( s , s ) propbu ( s ) = try ( proptr ( propbu ( s ))); s strategies dnf = propbu ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf )) cnf = propbu ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ); cnf )) Traversing Tuples and Lists \u00b6 Congruences can also be applied to tuples, (s1,s2,...,sn) , and lists, [s1,s2,...,sn] . A special list congruence is [] which \u2018visits\u2019 the empty list. As an example, consider again the definition of map(s) using recursive traversal rules: map ( s ) : [] - > [] map ( s ) : [ x | xs ] - > [< s > x | < map ( s )> xs ] Using list congruences we can define this strategy as: map ( s ) = [] < + [ s | map ( s )] The [] congruence matches an empty list. The [s | map(s)] congruence matches a non-empty list, and applies s to the head of the list and map(s) to the tail. Thus, map(s) applies s to each element of a list: < map ( inc )> [ 1 , 2 , 3 ] => [ 2 , 3 , 4 ] Note that map(s) only succeeds if s succeeds for each element of the list. The fetch and filter strategies are variations on map that use the failure of s to list elements. fetch ( s ) = [ s | id ] < + [ id | fetch ( s )] The fetch strategy traverses a list until it finds a element for which s succeeds and then stops. That element is the only one that is transformed. filter ( s ) = [] + ([ s | filter ( s )] < + ? [ |< id >]; filter ( s )) The filter strategy applies s to each element of a list, but only keeps the elements for which it succeeds. even = where (< eq >(< mod >(< id >, 2 ), 0 )) < filter ( even )> [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] => [ 2 , 4 , 6 , 8 ] Format Checking \u00b6 Another application of congruences is in the definition of format checkers. A format checker describes a subset of a term language using a recursive pattern. This can be used to verify input or output of a transformation, and for documentation purposes. Format checkers defined with congruences can check subsets of signatures or regular tree grammars. For example, the subset of terms of a signature in a some normal form. As an example, consider checking the output of the dnf and cnf transformations. conj ( s ) = And ( conj ( s ), conj ( s )) < + s disj ( s ) = Or ( disj ( s ), disj ( s )) < + s // Conjunctive normal form conj-nf = conj ( disj ( Not ( Atom ( id )) < + Atom ( id ))) // Disjunctive normal form disj-nf = disj ( conj ( Not ( Atom ( id )) < + Atom ( id ))) The strategies conj(s) and disj(s) check that the subject term is a conjunct or a disjunct, respectively, with terms satisfying s at the leaves. The strategies conj-nf and disj-nf check that the subject term is in conjunctive or disjunctive normal form, respectively. Generic Traversal \u00b6 Using congruence operators we constructed a generic, i.e. transformation independent, bottom-up traversal for proposition terms. The same can be done for other data types. However, since the sets of constructors of abstract syntax trees of typical programming languages can be quite large, this may still amount to quite a bit of work that is not reusable across data types; even though a strategy such as bottom-up traversal, is basically data-type independent. Thus, Stratego provides generic traversal by means of several generic one-step descent operators. The operator all , applies a strategy to all direct subterms. The operator one , applies a strategy to one direct subterm, and the operator some , applies a strategy to as many direct subterms as possible, and at least one. Visiting All Subterms \u00b6 The all(s) strategy transforms a constructor application by applying the parameter strategy s to each direct subterm. An application of all(s) fails if the application to one of the subterms fails. The following example shows how all (1) applies to any term, and (2) applies its argument strategy uniformly to all direct subterms. That is, it is not possible to do something special for a particular subterm (that\u2019s what congruences are for). < all ( ! Var ( \"a\" ))> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Var ( \"a\" )) < all ( ! Var ( \"z\" ))> Times ( Var ( \"b\" ), Int ( \"3\" )) => Times ( Var ( \"z\" ), Var ( \"z\" )) Defining Traversals with All \u00b6 The all(s) operator is really the ultimate replacement for the traversal with rules idiom. Instead of specifying a rule or congruence for each constructor, the single application of the all operator takes care of traversing all constructors. Thus, we can replace the propbu strategy by a completely generic definition of bottom-up traversal. Consider again the last definition of propbu : proptr ( s ) = Not ( s ) < + And ( s , s ) < + Or ( s , s ) < + Impl ( s , s ) < + Eq ( s , s ) propbu ( s ) = try ( proptr ( propbu ( s ))); s The role of proptr(s) in this definition can be replaced by all(s) , since that achieves exactly the same, namely applying s to the direct subterms of constructors: propbu ( s ) = all ( propbu ( s )); s Moreover, all succeeds on any constructor in any signature, so we can also drop the try as well, which was there only because proptr fails on the Atom(...) , True() , and False() nodes at the leaves. However, the strategy now is completely generic, i.e. independent of the particular structure it is applied to. In the Stratego Library this strategy is called bottomup(s) , and defined as follows: bottomup ( s ) = all ( bottomup ( s )); s It first recursively transforms the subterms of the subject term and then applies s to the result. Using this definition, the normalization of propositions now reduces to the following module, which is only concerned with the selection and composition of rewrite rules: module prop-dnf11 imports prop-rules strategies dnf = bottomup ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf )) cnf = bottomup ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ); cnf )) In fact, these definitions still contain a reusable pattern. With a little squinting we see that the definitions match the following pattern: dnf = bottomup ( try ( dnf-rules ; dnf )) cnf = bottomup ( try ( cnf-rules ; cnf )) In which we can recognize the definition of innermost reduction, which the Stratego Library defines as: innermost ( s ) = bottomup ( try ( s ; innermost ( s ))) The innermost strategy performs a bottom-up traversal of a term. After transforming the subterms of a term it tries to apply the transformation s . If successful the result is recursively transformed with an application of innermost . This brings us to the final form for the proposition normalizations: module prop-dnf12 imports prop-rules strategies dnf = innermost ( DN < + DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ) cnf = innermost ( DN < + DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ) Different transformations can be achieved by using a selection of rules and a strategy, which is generic, yet defined in Stratego itself using strategy combinators. Visiting One Subterm \u00b6 The one(s) strategy transforms a constructor application by applying the parameter strategy s to exactly one direct subterm. An application of one(s) fails if the application to all of the subterms fails. The following applications illustrate the behavior of the combinator: < one ( ! Var ( \"a\" ))> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) < one ( \\ Int ( x ) - > Int (< addS >( x , \"1\" )) \\ )> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"4\" )) < one ( ? Plus ( _ , _ ))> Plus ( Var ( \"a\" ), Int ( \"4\" )) // fails Defining Traversals with One \u00b6 A frequently used application of one is the oncetd(s) traversal, which performs a left to right depth first search/transformation that stops as soon as s has been successfully applied. oncetd ( s ) = s < + one ( oncetd ( s )) Thus, s is first applied to the root of the subject term. If that fails, its direct subterms are searched one by one (from left to right), with a recursive call to oncetd(s) . An application of oncetd is the contains(|t) strategy, which checks whether the subject term contains a subterm that is equal to t. contains (| t ) = oncetd ( ? t ) Through the depth first search of oncetd , either an occurrence of t is found, or all subterms are verified to be unequal to t . Here are some other one-pass traversals using the one combinator: oncebu ( s ) = one ( oncebu ( s )) < + s spinetd ( s ) = s ; try ( one ( spinetd ( s ))) spinebu ( s ) = try ( one ( spinebu ( s ))); s Here are some fixe-point traversals, i.e., traversals that apply their argument transformation exhaustively to the subject term. reduce ( s ) = repeat ( rec x ( one ( x ) + s )) outermost ( s ) = repeat ( oncetd ( s )) innermostI ( s ) = repeat ( oncebu ( s )) The difference is the subterm selection strategy. Visiting Some Subterms \u00b6 The some(s) strategy transforms a constructor application by applying the parameter strategy s to as many direct subterms as possible and at least one. An application of some(s) fails if the application to all of the subterms fails. Some one-pass traversals based on some: sometd ( s ) = s < + some ( sometd ( s )) somebu ( s ) = some ( somebu ( s )) < + s A fixed-point traversal with some: reduce-par ( s ) = repeat ( rec x ( some ( x ) + s )) References \u00b6 Bas Luttik and Eelco Visser. Specification of rewriting strategies. In M. P. A. Sellink, editor, 2 nd International Workshop on the Theory and Practice of Algebraic Specifications \\(ASF\\+SDF 1997\\) , Electronic Workshops in Computing. Berlin, November 1997. Springer-Verlag. \u21a9","title":"Traversal Combinators"},{"location":"background/stratego/strategy-combinators/traversal/#traversal-combinators","text":"There are many ways to traverse a tree. For example, a bottom-up traversal, visits the subterms of a node before it visits the node itself, while a top-down traversal visits nodes before it visits children. One-pass traversals traverse the tree one time, while fixed-point traversals, such as innermost, repeatedly traverse a term until a normal form is reached. Rather than provide built-in implementations for all traversals needed in transformations, Stratego defines traversals in terms of the primitive ingredients of traversal. For example, a top-down, one-pass traversal strategy will first visit a node, and then descend to the children of a node in order to recursively traverse all subterms. Similarly, the bottom-up, fixed-point traversal strategy innermost, will first descend to the children of a node in order to recursively traverse all subterms, then visit the node itself, and possibly recursively reapply the strategy. Traversal in Stratego is based on the observation 1 that a full term traversal is a recursive closure of a one-step descent, that is, an operation that applies a strategy to one or more direct subterms of the subject term. By separating this one-step descent operator from recursion, and making it a first-class operation, many different traversals can be defined. Here we explore the ways in which Stratego supports the definition of traversal strategies. We start with explicitly programmed traversals using recursive traversal rules. Next, congruences operators provide a more concise notation for such data-type specific traversal rules. Finally, generic traversal operators support data type independent definitions of traversals, which can be reused for any data type. Given these basic mechanisms, we conclude with an exploration of idioms for traversal and standard traversal strategies in the Stratego Library.","title":"Traversal Combinators"},{"location":"background/stratego/strategy-combinators/traversal/#congruence-operators","text":"Congruence operators provide a convenient abbreviation of traversal with rewrite rules . A congruence operator applies a strategy to each direct subterm of a specific constructor. For each n-ary constructor c declared in a signature, there is a corresponding congruence operator c(s1 , ..., sn) , which applies to terms of the form c(t1 , ..., tn) by applying the argument strategies to the corresponding argument terms. A congruence fails if the application of one the argument strategies fails or if constructor of the operator and that of the term do not match. For example, consider the following signature of expressions: module expressions signature sorts Exp constructors Int : String - > Exp Var : String - > Exp Plus : Exp * Exp - > Exp Times : Exp * Exp - > Exp The following applications apply the congruence operators Plus and Times to a term: < Plus ( ! Var ( \"a\" ), id )> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) < Times ( id , ! Int ( \"42\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails The first application shows how a congruence transforms a specific subterm, that is the strategy applied can be different for each subterm. The second application shows that a congruence only succeeds for terms constructed with the same constructor.","title":"Congruence Operators"},{"location":"background/stratego/strategy-combinators/traversal/#defining-traversals-with-congruences","text":"Since congruence operators define a one-step traversal for a specific constructor, they capture the pattern of traversal rules . That is, a traversal rule such as proptr ( s ) : And ( x , y ) - > And (< s > x , < s > y ) can be written by the congruence And(s,s) . Applying this to the prop-dnf program we can replace the traversal rules by congruences as follows: module prop-dnf10 imports prop-rules strategies proptr ( s ) = Not ( s ) < + And ( s , s ) < + Or ( s , s ) < + Impl ( s , s ) < + Eq ( s , s ) propbu ( s ) = try ( proptr ( propbu ( s ))); s strategies dnf = propbu ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf )) cnf = propbu ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ); cnf ))","title":"Defining Traversals with Congruences"},{"location":"background/stratego/strategy-combinators/traversal/#traversing-tuples-and-lists","text":"Congruences can also be applied to tuples, (s1,s2,...,sn) , and lists, [s1,s2,...,sn] . A special list congruence is [] which \u2018visits\u2019 the empty list. As an example, consider again the definition of map(s) using recursive traversal rules: map ( s ) : [] - > [] map ( s ) : [ x | xs ] - > [< s > x | < map ( s )> xs ] Using list congruences we can define this strategy as: map ( s ) = [] < + [ s | map ( s )] The [] congruence matches an empty list. The [s | map(s)] congruence matches a non-empty list, and applies s to the head of the list and map(s) to the tail. Thus, map(s) applies s to each element of a list: < map ( inc )> [ 1 , 2 , 3 ] => [ 2 , 3 , 4 ] Note that map(s) only succeeds if s succeeds for each element of the list. The fetch and filter strategies are variations on map that use the failure of s to list elements. fetch ( s ) = [ s | id ] < + [ id | fetch ( s )] The fetch strategy traverses a list until it finds a element for which s succeeds and then stops. That element is the only one that is transformed. filter ( s ) = [] + ([ s | filter ( s )] < + ? [ |< id >]; filter ( s )) The filter strategy applies s to each element of a list, but only keeps the elements for which it succeeds. even = where (< eq >(< mod >(< id >, 2 ), 0 )) < filter ( even )> [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] => [ 2 , 4 , 6 , 8 ]","title":"Traversing Tuples and Lists"},{"location":"background/stratego/strategy-combinators/traversal/#format-checking","text":"Another application of congruences is in the definition of format checkers. A format checker describes a subset of a term language using a recursive pattern. This can be used to verify input or output of a transformation, and for documentation purposes. Format checkers defined with congruences can check subsets of signatures or regular tree grammars. For example, the subset of terms of a signature in a some normal form. As an example, consider checking the output of the dnf and cnf transformations. conj ( s ) = And ( conj ( s ), conj ( s )) < + s disj ( s ) = Or ( disj ( s ), disj ( s )) < + s // Conjunctive normal form conj-nf = conj ( disj ( Not ( Atom ( id )) < + Atom ( id ))) // Disjunctive normal form disj-nf = disj ( conj ( Not ( Atom ( id )) < + Atom ( id ))) The strategies conj(s) and disj(s) check that the subject term is a conjunct or a disjunct, respectively, with terms satisfying s at the leaves. The strategies conj-nf and disj-nf check that the subject term is in conjunctive or disjunctive normal form, respectively.","title":"Format Checking"},{"location":"background/stratego/strategy-combinators/traversal/#generic-traversal","text":"Using congruence operators we constructed a generic, i.e. transformation independent, bottom-up traversal for proposition terms. The same can be done for other data types. However, since the sets of constructors of abstract syntax trees of typical programming languages can be quite large, this may still amount to quite a bit of work that is not reusable across data types; even though a strategy such as bottom-up traversal, is basically data-type independent. Thus, Stratego provides generic traversal by means of several generic one-step descent operators. The operator all , applies a strategy to all direct subterms. The operator one , applies a strategy to one direct subterm, and the operator some , applies a strategy to as many direct subterms as possible, and at least one.","title":"Generic Traversal"},{"location":"background/stratego/strategy-combinators/traversal/#visiting-all-subterms","text":"The all(s) strategy transforms a constructor application by applying the parameter strategy s to each direct subterm. An application of all(s) fails if the application to one of the subterms fails. The following example shows how all (1) applies to any term, and (2) applies its argument strategy uniformly to all direct subterms. That is, it is not possible to do something special for a particular subterm (that\u2019s what congruences are for). < all ( ! Var ( \"a\" ))> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Var ( \"a\" )) < all ( ! Var ( \"z\" ))> Times ( Var ( \"b\" ), Int ( \"3\" )) => Times ( Var ( \"z\" ), Var ( \"z\" ))","title":"Visiting All Subterms"},{"location":"background/stratego/strategy-combinators/traversal/#defining-traversals-with-all","text":"The all(s) operator is really the ultimate replacement for the traversal with rules idiom. Instead of specifying a rule or congruence for each constructor, the single application of the all operator takes care of traversing all constructors. Thus, we can replace the propbu strategy by a completely generic definition of bottom-up traversal. Consider again the last definition of propbu : proptr ( s ) = Not ( s ) < + And ( s , s ) < + Or ( s , s ) < + Impl ( s , s ) < + Eq ( s , s ) propbu ( s ) = try ( proptr ( propbu ( s ))); s The role of proptr(s) in this definition can be replaced by all(s) , since that achieves exactly the same, namely applying s to the direct subterms of constructors: propbu ( s ) = all ( propbu ( s )); s Moreover, all succeeds on any constructor in any signature, so we can also drop the try as well, which was there only because proptr fails on the Atom(...) , True() , and False() nodes at the leaves. However, the strategy now is completely generic, i.e. independent of the particular structure it is applied to. In the Stratego Library this strategy is called bottomup(s) , and defined as follows: bottomup ( s ) = all ( bottomup ( s )); s It first recursively transforms the subterms of the subject term and then applies s to the result. Using this definition, the normalization of propositions now reduces to the following module, which is only concerned with the selection and composition of rewrite rules: module prop-dnf11 imports prop-rules strategies dnf = bottomup ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ); dnf )) cnf = bottomup ( try ( DN < + ( DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ); cnf )) In fact, these definitions still contain a reusable pattern. With a little squinting we see that the definitions match the following pattern: dnf = bottomup ( try ( dnf-rules ; dnf )) cnf = bottomup ( try ( cnf-rules ; cnf )) In which we can recognize the definition of innermost reduction, which the Stratego Library defines as: innermost ( s ) = bottomup ( try ( s ; innermost ( s ))) The innermost strategy performs a bottom-up traversal of a term. After transforming the subterms of a term it tries to apply the transformation s . If successful the result is recursively transformed with an application of innermost . This brings us to the final form for the proposition normalizations: module prop-dnf12 imports prop-rules strategies dnf = innermost ( DN < + DefI < + DefE < + DMA < + DMO < + DAOL < + DAOR ) cnf = innermost ( DN < + DefI < + DefE < + DMA < + DMO < + DOAL < + DOAR ) Different transformations can be achieved by using a selection of rules and a strategy, which is generic, yet defined in Stratego itself using strategy combinators.","title":"Defining Traversals with All"},{"location":"background/stratego/strategy-combinators/traversal/#visiting-one-subterm","text":"The one(s) strategy transforms a constructor application by applying the parameter strategy s to exactly one direct subterm. An application of one(s) fails if the application to all of the subterms fails. The following applications illustrate the behavior of the combinator: < one ( ! Var ( \"a\" ))> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) < one ( \\ Int ( x ) - > Int (< addS >( x , \"1\" )) \\ )> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"4\" )) < one ( ? Plus ( _ , _ ))> Plus ( Var ( \"a\" ), Int ( \"4\" )) // fails","title":"Visiting One Subterm"},{"location":"background/stratego/strategy-combinators/traversal/#defining-traversals-with-one","text":"A frequently used application of one is the oncetd(s) traversal, which performs a left to right depth first search/transformation that stops as soon as s has been successfully applied. oncetd ( s ) = s < + one ( oncetd ( s )) Thus, s is first applied to the root of the subject term. If that fails, its direct subterms are searched one by one (from left to right), with a recursive call to oncetd(s) . An application of oncetd is the contains(|t) strategy, which checks whether the subject term contains a subterm that is equal to t. contains (| t ) = oncetd ( ? t ) Through the depth first search of oncetd , either an occurrence of t is found, or all subterms are verified to be unequal to t . Here are some other one-pass traversals using the one combinator: oncebu ( s ) = one ( oncebu ( s )) < + s spinetd ( s ) = s ; try ( one ( spinetd ( s ))) spinebu ( s ) = try ( one ( spinebu ( s ))); s Here are some fixe-point traversals, i.e., traversals that apply their argument transformation exhaustively to the subject term. reduce ( s ) = repeat ( rec x ( one ( x ) + s )) outermost ( s ) = repeat ( oncetd ( s )) innermostI ( s ) = repeat ( oncebu ( s )) The difference is the subterm selection strategy.","title":"Defining Traversals with One"},{"location":"background/stratego/strategy-combinators/traversal/#visiting-some-subterms","text":"The some(s) strategy transforms a constructor application by applying the parameter strategy s to as many direct subterms as possible and at least one. An application of some(s) fails if the application to all of the subterms fails. Some one-pass traversals based on some: sometd ( s ) = s < + some ( sometd ( s )) somebu ( s ) = some ( somebu ( s )) < + s A fixed-point traversal with some: reduce-par ( s ) = repeat ( rec x ( some ( x ) + s ))","title":"Visiting Some Subterms"},{"location":"background/stratego/strategy-combinators/traversal/#references","text":"Bas Luttik and Eelco Visser. Specification of rewriting strategies. In M. P. A. Sellink, editor, 2 nd International Workshop on the Theory and Practice of Algebraic Specifications \\(ASF\\+SDF 1997\\) , Electronic Workshops in Computing. Berlin, November 1997. Springer-Verlag. \u21a9","title":"References"},{"location":"background/stratego/strategy-combinators/type-unifying/","text":"Type Unifying Traversal \u00b6 In this section we consider the class of type unifying strategies, in which terms of different types are mapped onto one type. The application area for this type of strategy is analysis of expressions with examples such as free variables collection and call-graph extraction. We consider the following example problems: term-size : Count the number of nodes in a term occurrences : Count number of occurrences of a subterm in a term collect-vars : Collect all variables in expression free-vars : Collect all free variables in expression These problems have in common that they reduce a structure to a single value or to a collection of derived values. The structure of the original term is usually lost. We start with examining these problems in the context of lists, and then generalize the solutions we find there to arbitrary terms using generic term deconstruction , which allows concise implementation of generic type unifying strategies. Type Unifying List Transformations \u00b6 We start with considering type-unifying operations on lists. Sum \u00b6 Reducing a list to a value can be conveniently expressed by means of a fold , which has as parameters operations for reducing the list constructors. The foldr/2 strategy reduces a list by replacing each Cons by an application of s2 , and the empty list by s1 . foldr ( s1 , s2 ) = []; s1 < + \\ [ y | ys ] - > < s2 >( y , < foldr ( s1 , s2 )> ys ) \\ Thus, when applied to a list with three terms the result is < foldr ( s1 , s2 )> [ t1 , t2 , t3 ] => < s2 >( t1 , < s2 >( t2 , < s2 >( t3 , < s1 > []))) A typical application of foldr/2 is sum , which reduces a list to the sum of its elements. It sums the elements of a list of integers, using 0 for the empty list and add to combine the head of a list and the result of folding the tail. sum = foldr ( ! 0 , add ) The effect of sum is illustrated by the following application: < foldr ( ! 0 , add )> [ 1 , 2 , 3 ] => < add >( 1 , < add >( 2 , < add >( 3 , < ! 0 > []))) => 6 Note the build operator for replacing the empty list with 0 ; writing foldr(0, add) would be wrong, since 0 by itself is a congruence operator, which basically matches the subject term with the term 0 (rather than replacing it). Size \u00b6 The foldr/2 strategy does not touch the elements of a list. The foldr/3 strategy is a combination of fold and map that extends foldr/2 with a parameter that is applied to the elements of the list. foldr ( s1 , s2 , f ) = []; s1 < + \\ [ y | ys ] - > < s2 >(< f > y , < foldr ( s1 , s2 , f )> ys ) \\ Thus, when applying it to a list with three elements, we get: < foldr ( s1 , s2 )> [ t1 , t2 , t3 ] => < s2 >(< f > t1 , < s2 >(< f > t2 , < s2 >(< f > t3 , < s1 > []))) Now we can solve our first example problem term-size . The size of a list is its length, which corresponds to the sum of the list with the elements replaced by 1 . length = foldr ( ! 0 , add , ! 1 ) Number of occurrences \u00b6 The number of occurrences in a list of terms that satisfy some predicate, entails only counting those elements in the list for which the predicate succeeds. (Where a predicate is implemented with a strategy that succeeds only for the elements in the domain of the predicate.) This follows the same pattern as counting the length of a list, but now only counting the elements for which s succeeds. list-occurrences ( s ) = foldr ( ! 0 , add , s < ! 1 + ! 0 ) Using list-occurrences and a match strategy we can count the number of variables in a list: list-occurrences ( ? Var ( _ )) Collect \u00b6 The next problem is to collect all terms for which a strategy succeeds. We have already seen how to do this for lists. The filter strategy reduces a list to the elements for which its argument strategy succeeds. filter ( s ) = [] < + [ s | filter ( s )] < + ? [ |< filter ( s )>] Collecting the variables in a list is a matter of filtering with the ?Var(_) match. filter ( ? Var ( _ )) The final problem, collecting the free variables in a term, does not really have a counter part in lists, but we can mimick this if we consider having two lists; where the second list is the one with the bound variables that should be excluded. ( filter ( ? Var ( _ )), id ); diff This collects the variables in the first list and subtracts the variables in the second list. Extending Fold to Expressions \u00b6 We have seen how to do typical analysis transformations on lists. How can we generalize this to arbitrary terms? The general idea of a folding operator is that it replaces the constructors of a data-type by applying a function to combine the reduced arguments of constructor applications. For example, the following definition is a sketch for a fold over abstract syntax trees: fold-exp ( binop , assign , if , ... ) = rec f ( fold-binop ( f , binop ) < + fold-assign ( f , assign ) < + fold-if ( f , if ) < + ... ) fold-binop ( f , s ) : BinOp ( op , e1 , e2 ) - > < s >( op , < f > e1 , < f > e2 ) fold-assign ( f , s ) : Assign ( e1 , e2 ) - > < s >(< f > e1 , < f > e2 ) fold-if ( f , s ) : If ( e1 , e2 , e3 ) - > < s >(< f > e1 , < f > e2 , < f > e3 ) For each constructor of the data-type the fold has an argument strategy and a rule that matches applications of the constructor, which it replaces with an application of the strategy to the tuple of subterms reduced by a recursive invocation of the fold. Instantiation of this strategy requires a rule for each constructor of the data-type. For instance, the following instantiation defines term-size using fold-exp by providing rules that sum up the sizes of the subterms and add one ( inc ) to account for the node itself. term-size = fold-exp ( BinOpSize , AssignSize , IfSize , ... ) BinOpSize : ( Plus (), e1 , e2 ) - > < add ; inc >( e1 , e2 ) AssignSize : ( e1 , e2 ) - > < add ; inc >( e1 , e2 ) IfSize : ( e1 , e2 , e3 ) - > < add ; inc >( e1 , < add >( e2 , e3 )) This looks suspiciously like the traversal with rules pattern. Defining folds in this manner has several limitations. In the definition of fold, one parameter for each constructor is provided and traversal is defined explicitly for each constructor. Furthermore, in the instantiation of fold, one rule for each constructor is needed, and the default behaviour is not generically specified. One solution would be to use the generic traversal strategy bottomup to deal with fold: fold-exp ( s ) = bottomup ( s ) term-size = fold-exp ( BinOpSize < + AssignSize < + IfSize < + ... ) BinOpSize : BinOp ( Plus (), e1 , e2 ) - > < add >( 1 , < add >( e1 , e2 )) AssignSize : Assign ( e1 , e2 ) - > < add >( e1 , e2 ) IfSize : If ( e1 , e2 , e3 ) - > < add >( e1 , < add >( e2 , e3 )) Although the recursive application to subterms is now defined generically, one still has to specify rules for the default behavior. Generic Term Deconstruction \u00b6 Instead of having folding rules that are specific to a data type, such as BinOpSize : BinOp ( op , e1 , e2 ) - > < add >( 1 , < add >( e1 , e2 )) AssignSize : Assign ( e1 , e2 ) - > < add >( 1 , < add >( e1 , e2 )) we would like to have a generic definition of the form CSize : c ( e1 , e2 , ... ) - > < add >( e1 , < add >( e2 , ... )) This requires generic decomposition of a constructor application into its constructor and the list with children. This can be done using the # operator. The match strategy ?p1#(p2) decomposes a constructor application into its constructor name and the list of direct subterms. Matching such a pattern against a term of the form C(t1,...,tn) results in a match of \"C\" against p1 and a match of [t1,...,tn] against p2 . < ? c # ( xs )> Plus ( Int ( \"1\" ), Var ( \"2\" )) // variable c bound to \"Plus\" // variable xs bound to [Int(\"1\"), Var(\"2\")] Crush \u00b6 Using generic term deconstruction we can generalize the type unifying operations on lists to arbitrary terms. In analogy with the generic traversal operators we need a generic one-level reduction operator. The crush/3 strategy reduces a constructor application by folding the list of its subterms using foldr/3 . crush ( nul , sum , s ) : c # ( xs ) - > < foldr ( nul , sum , s )> xs Thus, crush performs a fold-map over the direct subterms of a term as illustrated by the following application: < crush ( s1 , s2 , f )> C ( t1 , t2 ) => < s2 >(< f > t1 , < s2 >(< f > t2 , < s1 >[])) The following application instantiates this application in two ways: < crush ( id , id , id )> Plus ( Int ( \"1\" ), Var ( \"2\" )) => ( Int ( \"1\" ),( Var ( \"2\" ),[])) < crush ( ! Tail (< id >), ! Sum (< Fst >,< Snd >), ! Arg (< id >))> Plus ( Int ( \"1\" ), Var ( \"2\" )) => Sum ( Arg ( Int ( \"1\" )), Sum ( Arg ( Var ( \"2\" )), Tail ([]))) The crush strategy is the tool we need to implement solutions for the example problems above. Size \u00b6 Counting the number of direct subterms of a term is similar to counting the number of elements of a list. The definition of node-size is the same as the definition of length , except that it uses crush instead of foldr : node-size = crush ( ! 0 , add , ! 1 ) Counting the number of subterms (nodes) in a term is a similar problem. But, instead of counting each direct subterm as 1 , we need to count its subterms. term-size = crush ( ! 1 , add , term-size ) The term-size strategy achieves this simply with a recursive call to itself. < node-size > Plus ( Int ( \"1\" ), Var ( \"2\" )) => 2 < term-size > Plus ( Int ( \"1\" ), Var ( \"2\" )) => 5 Occurrences \u00b6 Counting the number of occurrences of a certain term in another term, or more generally, counting the number of subterms that satisfy some predicate is similar to counting the term size. However, only those terms satisfying the predicate should be counted. The solution is again similar to the solution for lists, but now using crush. om-occurrences ( s ) = s < ! 1 + crush ( ! 0 , add , om-occurrences ( s )) The om-occurrences strategy counts the outermost subterms satisfying s . That is, the strategy stops counting as soon as it finds a subterm for which s succeeds. The following strategy counts all occurrences: occurrences ( s ) = < add >(< s < ! 1 + ! 0 >, < crush ( ! 0 , add , occurrences ( s ))>) It counts the current term if it satisfies s and adds that to the occurrences in the subterms. < om-occurrences ( ? Int ( _ ))> Plus ( Int ( \"1\" ), Plus ( Int ( \"34\" ), Var ( \"2\" ))) => 2 < om-occurrences ( ? Plus ( _ , _ ))> Plus ( Int ( \"1\" ), Plus ( Int ( \"34\" ), Var ( \"2\" ))) => 1 < occurrences ( ? Plus ( _ , _ ))> Plus ( Int ( \"1\" ), Plus ( Int ( \"34\" ), Var ( \"2\" ))) => 2 Collect \u00b6 Collecting the subterms that satisfy a predicate is similar to counting, but now a list of subterms is produced. The collect(s) strategy collects all outermost occurrences satisfying s . collect ( s ) = ! [< s >] < + crush ( ! [], union , collect ( s )) When encountering a subterm for which s succeeds, a singleton list is produced. For other terms, the matching subterms are collected for each direct subterm, and the resulting lists are combined with union to remove duplicates. A typical application of collect is the collection of all variables in an expression, which can be defined as follows: get-vars = collect ( ? Var ( _ )) Applying get-vars to an expression AST produces the list of all subterms matching Var(_) . The collect-all(s) strategy collects all occurrences satisfying s. collect-all ( s ) = ! [< s > | < crush ( ! [], union , collect ( s ))>] < + crush ( ! [], union , collect ( s )) If s succeeds for the subject term combines the subject term with the collected terms from the subterms. Free Variables \u00b6 Collecting the variables in an expression is easy, as we saw above. However, when dealing with languages with variable bindings, a common operation is to extract only the free variables in an expression or block of statements. That is, the occurrences of variables that are not bound by a variable declaration. For example, in the expression x + let var y := x + 1 in f ( y , a + x + b ) end the free variables are {x, a, b} , but not y , since it is bound by the declaration in the let. Similarly, in the function definition function f ( x : int ) = let var y := h ( x ) in x + g ( z ) * y end the only free variable is z since x and y are declared. Here is a free variable extraction strategy for Tiger expressions. The crush alternative takes care of the non-special constructors, while ExpVars and FreeVars deal with the special cases, i.e. variables and variable binding constructs: free-vars = ExpVars < + FreeVars ( free-vars ) < + crush ( ! [], union , free-vars ) ExpVars : Var ( x ) - > [ x ] FreeVars ( fv ) : Let ([ VarDec ( x , t , e1 )], e2 ) - > < union >(< fv > e1 , < diff >(< fv > e2 , [ x ])) FreeVars ( fv ) : Let ([ FunctionDec ( fdecs )], e2 ) - > < diff >(< union >(< fv > fdecs , < fv > e2 ), fs ) where < map ( ? FunDec (< id >, _ , _ , _ ))> fdecs => fs FreeVars ( fv ) : FunDec ( f , xs , t , e ) - > < diff >(< fv > e , xs ) where < map ( Fst )> xs => xs The FreeVars rules for binding constructs use their fv parameter to recursively get the free variables from subterms, and they subtract the bound variables from any free variables found using diff. We can even capture the pattern exhibited here in a generic collection algorithm with support for special cases: collect-exc ( base , special : ( a - > b ) * a - > b ) = base < + special ( collect-exc ( base , special )) < + crush ( ! [], union , collect-exc ( base , special )) The special parameter is a strategy parameterized with a recursive call to the collection strategy. The original definition of free-vars above, can now be replaced with free-vars = collect-exc ( ExpVars , FreeVars ) Generic Term Construction \u00b6 It can also be useful to construct terms generically. For example, in parse tree implosion, application nodes should be reduced to constructor applications. Hence build operators can also use the # operator. In a strategy !p1#(p2) , the current subject term is replaced by a constructor application, where the constructor name is provided by p1 and the list of subterms by p2 . So, if p1 evaluates to \"C\" and p2 evaluates to [t1,...,tn] , the expression !p1#(p2) build the term C(t1,...,tn) . Imploding Parse Trees \u00b6 A typical application of generic term construction is the implosion of parse trees to abstract syntax trees performed by implode-asfix . Parse trees produced by sglr have the form: appl ( prod ( sorts , sort , attrs ([ cons ( \"C\" )])),[ t1 , ... , tn ]) That is, a node in a parse tree consists of an encoding of the original production from the syntax definition, and a list with subtrees. The production includes a constructor annotation cons(\"C\") with the name of the abstract syntax tree constructor. Such a tree node should be imploded to an abstract syntax tree node of the form C(t1,...,tn) . Thus, this requires the construction of a term with constructor C given the string with its name. The following implosion strategy achieves this using generic term construction: implode = appl ( id , map ( implode )); Implode Implode : appl ( prod ( sorts , sort , attrs ([ cons ( c )])), ts ) - > c # ( ts ) The Implode rule rewrites an appl term to a constructor application, by extracting the constructor name from the production and then using generic term construction to apply the constructor. Note that this is a gross over simplification of the actual implementation of implode-asfix . See the source code for the full strategy. Generic term construction and deconstruction support the definition of generic analysis and generic translation problems. The generic solutions for the example problems term size, number of occurrences, and subterm collection demonstrate the general approach to solving these types of problems.","title":"Type Unifying Traversal"},{"location":"background/stratego/strategy-combinators/type-unifying/#type-unifying-traversal","text":"In this section we consider the class of type unifying strategies, in which terms of different types are mapped onto one type. The application area for this type of strategy is analysis of expressions with examples such as free variables collection and call-graph extraction. We consider the following example problems: term-size : Count the number of nodes in a term occurrences : Count number of occurrences of a subterm in a term collect-vars : Collect all variables in expression free-vars : Collect all free variables in expression These problems have in common that they reduce a structure to a single value or to a collection of derived values. The structure of the original term is usually lost. We start with examining these problems in the context of lists, and then generalize the solutions we find there to arbitrary terms using generic term deconstruction , which allows concise implementation of generic type unifying strategies.","title":"Type Unifying Traversal"},{"location":"background/stratego/strategy-combinators/type-unifying/#type-unifying-list-transformations","text":"We start with considering type-unifying operations on lists.","title":"Type Unifying List Transformations"},{"location":"background/stratego/strategy-combinators/type-unifying/#sum","text":"Reducing a list to a value can be conveniently expressed by means of a fold , which has as parameters operations for reducing the list constructors. The foldr/2 strategy reduces a list by replacing each Cons by an application of s2 , and the empty list by s1 . foldr ( s1 , s2 ) = []; s1 < + \\ [ y | ys ] - > < s2 >( y , < foldr ( s1 , s2 )> ys ) \\ Thus, when applied to a list with three terms the result is < foldr ( s1 , s2 )> [ t1 , t2 , t3 ] => < s2 >( t1 , < s2 >( t2 , < s2 >( t3 , < s1 > []))) A typical application of foldr/2 is sum , which reduces a list to the sum of its elements. It sums the elements of a list of integers, using 0 for the empty list and add to combine the head of a list and the result of folding the tail. sum = foldr ( ! 0 , add ) The effect of sum is illustrated by the following application: < foldr ( ! 0 , add )> [ 1 , 2 , 3 ] => < add >( 1 , < add >( 2 , < add >( 3 , < ! 0 > []))) => 6 Note the build operator for replacing the empty list with 0 ; writing foldr(0, add) would be wrong, since 0 by itself is a congruence operator, which basically matches the subject term with the term 0 (rather than replacing it).","title":"Sum"},{"location":"background/stratego/strategy-combinators/type-unifying/#size","text":"The foldr/2 strategy does not touch the elements of a list. The foldr/3 strategy is a combination of fold and map that extends foldr/2 with a parameter that is applied to the elements of the list. foldr ( s1 , s2 , f ) = []; s1 < + \\ [ y | ys ] - > < s2 >(< f > y , < foldr ( s1 , s2 , f )> ys ) \\ Thus, when applying it to a list with three elements, we get: < foldr ( s1 , s2 )> [ t1 , t2 , t3 ] => < s2 >(< f > t1 , < s2 >(< f > t2 , < s2 >(< f > t3 , < s1 > []))) Now we can solve our first example problem term-size . The size of a list is its length, which corresponds to the sum of the list with the elements replaced by 1 . length = foldr ( ! 0 , add , ! 1 )","title":"Size"},{"location":"background/stratego/strategy-combinators/type-unifying/#number-of-occurrences","text":"The number of occurrences in a list of terms that satisfy some predicate, entails only counting those elements in the list for which the predicate succeeds. (Where a predicate is implemented with a strategy that succeeds only for the elements in the domain of the predicate.) This follows the same pattern as counting the length of a list, but now only counting the elements for which s succeeds. list-occurrences ( s ) = foldr ( ! 0 , add , s < ! 1 + ! 0 ) Using list-occurrences and a match strategy we can count the number of variables in a list: list-occurrences ( ? Var ( _ ))","title":"Number of occurrences"},{"location":"background/stratego/strategy-combinators/type-unifying/#collect","text":"The next problem is to collect all terms for which a strategy succeeds. We have already seen how to do this for lists. The filter strategy reduces a list to the elements for which its argument strategy succeeds. filter ( s ) = [] < + [ s | filter ( s )] < + ? [ |< filter ( s )>] Collecting the variables in a list is a matter of filtering with the ?Var(_) match. filter ( ? Var ( _ )) The final problem, collecting the free variables in a term, does not really have a counter part in lists, but we can mimick this if we consider having two lists; where the second list is the one with the bound variables that should be excluded. ( filter ( ? Var ( _ )), id ); diff This collects the variables in the first list and subtracts the variables in the second list.","title":"Collect"},{"location":"background/stratego/strategy-combinators/type-unifying/#extending-fold-to-expressions","text":"We have seen how to do typical analysis transformations on lists. How can we generalize this to arbitrary terms? The general idea of a folding operator is that it replaces the constructors of a data-type by applying a function to combine the reduced arguments of constructor applications. For example, the following definition is a sketch for a fold over abstract syntax trees: fold-exp ( binop , assign , if , ... ) = rec f ( fold-binop ( f , binop ) < + fold-assign ( f , assign ) < + fold-if ( f , if ) < + ... ) fold-binop ( f , s ) : BinOp ( op , e1 , e2 ) - > < s >( op , < f > e1 , < f > e2 ) fold-assign ( f , s ) : Assign ( e1 , e2 ) - > < s >(< f > e1 , < f > e2 ) fold-if ( f , s ) : If ( e1 , e2 , e3 ) - > < s >(< f > e1 , < f > e2 , < f > e3 ) For each constructor of the data-type the fold has an argument strategy and a rule that matches applications of the constructor, which it replaces with an application of the strategy to the tuple of subterms reduced by a recursive invocation of the fold. Instantiation of this strategy requires a rule for each constructor of the data-type. For instance, the following instantiation defines term-size using fold-exp by providing rules that sum up the sizes of the subterms and add one ( inc ) to account for the node itself. term-size = fold-exp ( BinOpSize , AssignSize , IfSize , ... ) BinOpSize : ( Plus (), e1 , e2 ) - > < add ; inc >( e1 , e2 ) AssignSize : ( e1 , e2 ) - > < add ; inc >( e1 , e2 ) IfSize : ( e1 , e2 , e3 ) - > < add ; inc >( e1 , < add >( e2 , e3 )) This looks suspiciously like the traversal with rules pattern. Defining folds in this manner has several limitations. In the definition of fold, one parameter for each constructor is provided and traversal is defined explicitly for each constructor. Furthermore, in the instantiation of fold, one rule for each constructor is needed, and the default behaviour is not generically specified. One solution would be to use the generic traversal strategy bottomup to deal with fold: fold-exp ( s ) = bottomup ( s ) term-size = fold-exp ( BinOpSize < + AssignSize < + IfSize < + ... ) BinOpSize : BinOp ( Plus (), e1 , e2 ) - > < add >( 1 , < add >( e1 , e2 )) AssignSize : Assign ( e1 , e2 ) - > < add >( e1 , e2 ) IfSize : If ( e1 , e2 , e3 ) - > < add >( e1 , < add >( e2 , e3 )) Although the recursive application to subterms is now defined generically, one still has to specify rules for the default behavior.","title":"Extending Fold to Expressions"},{"location":"background/stratego/strategy-combinators/type-unifying/#generic-term-deconstruction","text":"Instead of having folding rules that are specific to a data type, such as BinOpSize : BinOp ( op , e1 , e2 ) - > < add >( 1 , < add >( e1 , e2 )) AssignSize : Assign ( e1 , e2 ) - > < add >( 1 , < add >( e1 , e2 )) we would like to have a generic definition of the form CSize : c ( e1 , e2 , ... ) - > < add >( e1 , < add >( e2 , ... )) This requires generic decomposition of a constructor application into its constructor and the list with children. This can be done using the # operator. The match strategy ?p1#(p2) decomposes a constructor application into its constructor name and the list of direct subterms. Matching such a pattern against a term of the form C(t1,...,tn) results in a match of \"C\" against p1 and a match of [t1,...,tn] against p2 . < ? c # ( xs )> Plus ( Int ( \"1\" ), Var ( \"2\" )) // variable c bound to \"Plus\" // variable xs bound to [Int(\"1\"), Var(\"2\")]","title":"Generic Term Deconstruction"},{"location":"background/stratego/strategy-combinators/type-unifying/#crush","text":"Using generic term deconstruction we can generalize the type unifying operations on lists to arbitrary terms. In analogy with the generic traversal operators we need a generic one-level reduction operator. The crush/3 strategy reduces a constructor application by folding the list of its subterms using foldr/3 . crush ( nul , sum , s ) : c # ( xs ) - > < foldr ( nul , sum , s )> xs Thus, crush performs a fold-map over the direct subterms of a term as illustrated by the following application: < crush ( s1 , s2 , f )> C ( t1 , t2 ) => < s2 >(< f > t1 , < s2 >(< f > t2 , < s1 >[])) The following application instantiates this application in two ways: < crush ( id , id , id )> Plus ( Int ( \"1\" ), Var ( \"2\" )) => ( Int ( \"1\" ),( Var ( \"2\" ),[])) < crush ( ! Tail (< id >), ! Sum (< Fst >,< Snd >), ! Arg (< id >))> Plus ( Int ( \"1\" ), Var ( \"2\" )) => Sum ( Arg ( Int ( \"1\" )), Sum ( Arg ( Var ( \"2\" )), Tail ([]))) The crush strategy is the tool we need to implement solutions for the example problems above.","title":"Crush"},{"location":"background/stratego/strategy-combinators/type-unifying/#size_1","text":"Counting the number of direct subterms of a term is similar to counting the number of elements of a list. The definition of node-size is the same as the definition of length , except that it uses crush instead of foldr : node-size = crush ( ! 0 , add , ! 1 ) Counting the number of subterms (nodes) in a term is a similar problem. But, instead of counting each direct subterm as 1 , we need to count its subterms. term-size = crush ( ! 1 , add , term-size ) The term-size strategy achieves this simply with a recursive call to itself. < node-size > Plus ( Int ( \"1\" ), Var ( \"2\" )) => 2 < term-size > Plus ( Int ( \"1\" ), Var ( \"2\" )) => 5","title":"Size"},{"location":"background/stratego/strategy-combinators/type-unifying/#occurrences","text":"Counting the number of occurrences of a certain term in another term, or more generally, counting the number of subterms that satisfy some predicate is similar to counting the term size. However, only those terms satisfying the predicate should be counted. The solution is again similar to the solution for lists, but now using crush. om-occurrences ( s ) = s < ! 1 + crush ( ! 0 , add , om-occurrences ( s )) The om-occurrences strategy counts the outermost subterms satisfying s . That is, the strategy stops counting as soon as it finds a subterm for which s succeeds. The following strategy counts all occurrences: occurrences ( s ) = < add >(< s < ! 1 + ! 0 >, < crush ( ! 0 , add , occurrences ( s ))>) It counts the current term if it satisfies s and adds that to the occurrences in the subterms. < om-occurrences ( ? Int ( _ ))> Plus ( Int ( \"1\" ), Plus ( Int ( \"34\" ), Var ( \"2\" ))) => 2 < om-occurrences ( ? Plus ( _ , _ ))> Plus ( Int ( \"1\" ), Plus ( Int ( \"34\" ), Var ( \"2\" ))) => 1 < occurrences ( ? Plus ( _ , _ ))> Plus ( Int ( \"1\" ), Plus ( Int ( \"34\" ), Var ( \"2\" ))) => 2","title":"Occurrences"},{"location":"background/stratego/strategy-combinators/type-unifying/#collect_1","text":"Collecting the subterms that satisfy a predicate is similar to counting, but now a list of subterms is produced. The collect(s) strategy collects all outermost occurrences satisfying s . collect ( s ) = ! [< s >] < + crush ( ! [], union , collect ( s )) When encountering a subterm for which s succeeds, a singleton list is produced. For other terms, the matching subterms are collected for each direct subterm, and the resulting lists are combined with union to remove duplicates. A typical application of collect is the collection of all variables in an expression, which can be defined as follows: get-vars = collect ( ? Var ( _ )) Applying get-vars to an expression AST produces the list of all subterms matching Var(_) . The collect-all(s) strategy collects all occurrences satisfying s. collect-all ( s ) = ! [< s > | < crush ( ! [], union , collect ( s ))>] < + crush ( ! [], union , collect ( s )) If s succeeds for the subject term combines the subject term with the collected terms from the subterms.","title":"Collect"},{"location":"background/stratego/strategy-combinators/type-unifying/#free-variables","text":"Collecting the variables in an expression is easy, as we saw above. However, when dealing with languages with variable bindings, a common operation is to extract only the free variables in an expression or block of statements. That is, the occurrences of variables that are not bound by a variable declaration. For example, in the expression x + let var y := x + 1 in f ( y , a + x + b ) end the free variables are {x, a, b} , but not y , since it is bound by the declaration in the let. Similarly, in the function definition function f ( x : int ) = let var y := h ( x ) in x + g ( z ) * y end the only free variable is z since x and y are declared. Here is a free variable extraction strategy for Tiger expressions. The crush alternative takes care of the non-special constructors, while ExpVars and FreeVars deal with the special cases, i.e. variables and variable binding constructs: free-vars = ExpVars < + FreeVars ( free-vars ) < + crush ( ! [], union , free-vars ) ExpVars : Var ( x ) - > [ x ] FreeVars ( fv ) : Let ([ VarDec ( x , t , e1 )], e2 ) - > < union >(< fv > e1 , < diff >(< fv > e2 , [ x ])) FreeVars ( fv ) : Let ([ FunctionDec ( fdecs )], e2 ) - > < diff >(< union >(< fv > fdecs , < fv > e2 ), fs ) where < map ( ? FunDec (< id >, _ , _ , _ ))> fdecs => fs FreeVars ( fv ) : FunDec ( f , xs , t , e ) - > < diff >(< fv > e , xs ) where < map ( Fst )> xs => xs The FreeVars rules for binding constructs use their fv parameter to recursively get the free variables from subterms, and they subtract the bound variables from any free variables found using diff. We can even capture the pattern exhibited here in a generic collection algorithm with support for special cases: collect-exc ( base , special : ( a - > b ) * a - > b ) = base < + special ( collect-exc ( base , special )) < + crush ( ! [], union , collect-exc ( base , special )) The special parameter is a strategy parameterized with a recursive call to the collection strategy. The original definition of free-vars above, can now be replaced with free-vars = collect-exc ( ExpVars , FreeVars )","title":"Free Variables"},{"location":"background/stratego/strategy-combinators/type-unifying/#generic-term-construction","text":"It can also be useful to construct terms generically. For example, in parse tree implosion, application nodes should be reduced to constructor applications. Hence build operators can also use the # operator. In a strategy !p1#(p2) , the current subject term is replaced by a constructor application, where the constructor name is provided by p1 and the list of subterms by p2 . So, if p1 evaluates to \"C\" and p2 evaluates to [t1,...,tn] , the expression !p1#(p2) build the term C(t1,...,tn) .","title":"Generic Term Construction"},{"location":"background/stratego/strategy-combinators/type-unifying/#imploding-parse-trees","text":"A typical application of generic term construction is the implosion of parse trees to abstract syntax trees performed by implode-asfix . Parse trees produced by sglr have the form: appl ( prod ( sorts , sort , attrs ([ cons ( \"C\" )])),[ t1 , ... , tn ]) That is, a node in a parse tree consists of an encoding of the original production from the syntax definition, and a list with subtrees. The production includes a constructor annotation cons(\"C\") with the name of the abstract syntax tree constructor. Such a tree node should be imploded to an abstract syntax tree node of the form C(t1,...,tn) . Thus, this requires the construction of a term with constructor C given the string with its name. The following implosion strategy achieves this using generic term construction: implode = appl ( id , map ( implode )); Implode Implode : appl ( prod ( sorts , sort , attrs ([ cons ( c )])), ts ) - > c # ( ts ) The Implode rule rewrites an appl term to a constructor application, by extracting the constructor name from the production and then using generic term construction to apply the constructor. Note that this is a gross over simplification of the actual implementation of implode-asfix . See the source code for the full strategy. Generic term construction and deconstruction support the definition of generic analysis and generic translation problems. The generic solutions for the example problems term size, number of occurrences, and subterm collection demonstrate the general approach to solving these types of problems.","title":"Imploding Parse Trees"},{"location":"howtos/","text":"How-To's \u00b6 These are some How-To's that help you to get to a specific goal or result with Spoofax. For hands-on tutorials on learning Spoofax, see the Tutorials section. For the Spoofax languages references, see the References section. Spoofax Installation \u00b6 Install the Eclipse with Spoofax Plugin Bundle Install the Spoofax Eclipse Plugin Manually Install Spoofax from Source Statix \u00b6 Debugging Statix Specifications Stratego \u00b6 How to generate Stratego signatures How to run Stratego programs How to generate pretty-printers How to debug Stratego programs Exchange Terms Inspect Terms How to upgrade from Stratego 1 to Stratego 2 Editor Services \u00b6 Add Rename Refactoring to an Existing Project Spoofax Development \u00b6 Install Requirements for Spoofax Install Maven for Spoofax Build Spoofax Develop Spoofax Release Spoofax","title":"How-To's"},{"location":"howtos/#how-tos","text":"These are some How-To's that help you to get to a specific goal or result with Spoofax. For hands-on tutorials on learning Spoofax, see the Tutorials section. For the Spoofax languages references, see the References section.","title":"How-To's"},{"location":"howtos/#spoofax-installation","text":"Install the Eclipse with Spoofax Plugin Bundle Install the Spoofax Eclipse Plugin Manually Install Spoofax from Source","title":"Spoofax Installation"},{"location":"howtos/#statix","text":"Debugging Statix Specifications","title":"Statix"},{"location":"howtos/#stratego","text":"How to generate Stratego signatures How to run Stratego programs How to generate pretty-printers How to debug Stratego programs Exchange Terms Inspect Terms How to upgrade from Stratego 1 to Stratego 2","title":"Stratego"},{"location":"howtos/#editor-services","text":"Add Rename Refactoring to an Existing Project","title":"Editor Services"},{"location":"howtos/#spoofax-development","text":"Install Requirements for Spoofax Install Maven for Spoofax Build Spoofax Develop Spoofax Release Spoofax","title":"Spoofax Development"},{"location":"howtos/development/","text":"Spoofax Development \u00b6 This is the reference manual for building and developing Spoofax, as well as information about its internals. Introduction \u00b6 Spoofax is the integration of many different tools, compilers, (meta-)languages, (meta-)libraries, and runtime components. This integration is made concrete in the spoofax-releng Git repository on GitHub. This repository contains all components via Git submodules , which are updated by our build farm that builds Spoofax whenever one of its components in a submodule changes. Spoofax currently contains the following subcomponents as submodules: releng - Release engineering scripts for managing and building the spoofax-releng repostory. Java libraries and runtimes mb-rep \u2014 Libraries for program representation such as abstract terms mb-exec \u2014 Stratego interpreter and utilities jsglr \u2014 JSGLR parser spoofax \u2014 Spoofax Core, a cross platform API to Spoofax languages spoofax-maven \u2014 Maven integration for Spoofax Core spoofax-sunshine \u2014 Command-line integration for Spoofax Core spoofax-eclipse \u2014 Eclipse plugin for Spoofax Core spoofax-intellij \u2014 IntelliJ plugin for Spoofax Core Meta-languages and libraries esv \u2014 Editor service language sdf \u2014 Syntax Definition Formalisms, containing the SDF2 and SDF 3 languages stratego and strategoxt \u2014 Stratego compiler, runtime, and editor nabl \u2014 Name binding languages, containing the NaBL and NaBL2 languages, and support libraries for NaBL2 ts \u2014 Type system language dynsem \u2014 Dynamic semantics language metaborg-coq \u2014 Coq signatures and syntax definition spt \u2014 Spoofax testing language runtime-libraries \u2014 NaBL support libraries, incremental task engine for incremental name and type analysis Furthermore, this repository contains a Bash script ./b that redirects into the Python release engineering scripts in the releng submodule. These scripts support managing this Git repository, version management, generation of Eclipse instances, building Spoofax, and releasing new versions of Spoofax. The following how-tos explain how to set up Maven and other required tools for building and developing Spoofax, how to build and develop Spoofax, how to write this documentation, and explains some of the internals of Spoofax components. Requirements Maven Building Developing Releasing","title":"Spoofax Development"},{"location":"howtos/development/#spoofax-development","text":"This is the reference manual for building and developing Spoofax, as well as information about its internals.","title":"Spoofax Development"},{"location":"howtos/development/#introduction","text":"Spoofax is the integration of many different tools, compilers, (meta-)languages, (meta-)libraries, and runtime components. This integration is made concrete in the spoofax-releng Git repository on GitHub. This repository contains all components via Git submodules , which are updated by our build farm that builds Spoofax whenever one of its components in a submodule changes. Spoofax currently contains the following subcomponents as submodules: releng - Release engineering scripts for managing and building the spoofax-releng repostory. Java libraries and runtimes mb-rep \u2014 Libraries for program representation such as abstract terms mb-exec \u2014 Stratego interpreter and utilities jsglr \u2014 JSGLR parser spoofax \u2014 Spoofax Core, a cross platform API to Spoofax languages spoofax-maven \u2014 Maven integration for Spoofax Core spoofax-sunshine \u2014 Command-line integration for Spoofax Core spoofax-eclipse \u2014 Eclipse plugin for Spoofax Core spoofax-intellij \u2014 IntelliJ plugin for Spoofax Core Meta-languages and libraries esv \u2014 Editor service language sdf \u2014 Syntax Definition Formalisms, containing the SDF2 and SDF 3 languages stratego and strategoxt \u2014 Stratego compiler, runtime, and editor nabl \u2014 Name binding languages, containing the NaBL and NaBL2 languages, and support libraries for NaBL2 ts \u2014 Type system language dynsem \u2014 Dynamic semantics language metaborg-coq \u2014 Coq signatures and syntax definition spt \u2014 Spoofax testing language runtime-libraries \u2014 NaBL support libraries, incremental task engine for incremental name and type analysis Furthermore, this repository contains a Bash script ./b that redirects into the Python release engineering scripts in the releng submodule. These scripts support managing this Git repository, version management, generation of Eclipse instances, building Spoofax, and releasing new versions of Spoofax. The following how-tos explain how to set up Maven and other required tools for building and developing Spoofax, how to build and develop Spoofax, how to write this documentation, and explains some of the internals of Spoofax components. Requirements Maven Building Developing Releasing","title":"Introduction"},{"location":"howtos/development/building/","text":"Building Spoofax \u00b6 This how-to guides you on how to build Spoofax from scratch, via the command-line. Cloning the Source Code \u00b6 Clone the source code from the spoofax-releng repository with the following commands: macOS git clone --recursive https://github.com/metaborg/spoofax-releng.git cd spoofax-releng macOS Catalina, Big Sur, or newer On macOS Catalina, Big Sur, or newer, you have to install coreutils and Docker to be able to build Spoofax. This is temporary, until the 32-bit binaries for sdf2table and implodePT have been phased out. See the requirements for Spoofax Development for more information. Linux git clone --recursive https://github.com/metaborg/spoofax-releng.git cd spoofax-releng Windows git clone - -recursive https :// github . com / metaborg / spoofax-releng . git cd spoofax-releng cd releng \\ releng py -m pip install -r .\\ requirements . txt Cloning and updating submodules can take a while, since we have many submodules and some have a large history. Start a Build \u00b6 To build Spoofax, simply execute: macOS ./b build all Linux ./b build all Windows .\\ bd . bat build all This downloads the latest Stratego/XT, and builds Spoofax. If you also want to build Stratego/XT from scratch, execute: macOS ./b build -st all Linux ./b build -st all Windows .\\ bd . bat build -st all The -s flag build Stratego/XT instead of downloading it, and -t skips the Stratego/XT tests since they are very lengthy. The all part of the command indicates that we want to build all components. For example, if you would only like to build the Java components of Spoofax, and skip the Eclipse plugins, execute: macOS ./b build java Use ./b build to get a list of components available for building, and ./b build --help for help on all the command-line flags and switches. Linux ./b build java Use ./b build to get a list of components available for building, and ./b build --help for help on all the command-line flags and switches. Windows .\\ bd . bat build java Use .\\bd.bat build to get a list of components available for building, and .\\bd.bat build --help for help on all the command-line flags and switches. If you have opened a project in the repository in Eclipse, you must turn off Project \u2023 Build Automatically in Eclipse, otherwise the Maven and Eclipse compilers will interfere and possibly fail the build. After the Maven build is finished, enable Build Automatically again. Updating the Source Code \u00b6 If you want to update the repository and submodules, execute: macOS git pull --rebase ./b checkout ./b update Linux git pull --rebase ./b checkout ./b update Windows git pull - -rebase .\\ bd . bat checkout .\\ bd . bat update The git pull command will update any changes in the main repository. The ./b checkout command will check out the correct branches in all submodules, because Git does not do this automatically. The ./b update command will update all submodules. Switching to a Different Branch \u00b6 Switching to a different branch, for example the spoofax-release branch, is done with the following commands: macOS git checkout spoofax-release git pull --rebase git submodule update --init --remote --recursive ./b checkout ./b update Linux git checkout spoofax-release git pull --rebase git submodule update --init --remote --recursive ./b checkout ./b update Windows git checkout spoofax-release git pull - -rebase git submodule update - -init - -remote - -recursive .\\ bd . bat checkout .\\ bd . bat update Troubleshooting \u00b6 Resetting and Cleaning \u00b6 If updating or checking out a branch of submodule fails (because of unstaged or conflicting changes), you can try to resolve it yourself, or you can reset and clean everything. Reset and clean all submodules using: macOS ./b reset ./b clean Linux ./b checkout ./b update Windows .\\ bd . bat reset .\\ bd . bat clean Risk of loss of data Resetting and cleaning deletes uncommitted and unpushed changes , which can cause permanent data loss . Make sure all your changes are committed and pushed! Weird Compilation Errors \u00b6 If you get any weird compilation errors during the command-line build, make sure that Project \u2023 Build Automatically is turned off in Eclipse.","title":"Building"},{"location":"howtos/development/building/#building-spoofax","text":"This how-to guides you on how to build Spoofax from scratch, via the command-line.","title":"Building Spoofax"},{"location":"howtos/development/building/#cloning-the-source-code","text":"Clone the source code from the spoofax-releng repository with the following commands: macOS git clone --recursive https://github.com/metaborg/spoofax-releng.git cd spoofax-releng macOS Catalina, Big Sur, or newer On macOS Catalina, Big Sur, or newer, you have to install coreutils and Docker to be able to build Spoofax. This is temporary, until the 32-bit binaries for sdf2table and implodePT have been phased out. See the requirements for Spoofax Development for more information. Linux git clone --recursive https://github.com/metaborg/spoofax-releng.git cd spoofax-releng Windows git clone - -recursive https :// github . com / metaborg / spoofax-releng . git cd spoofax-releng cd releng \\ releng py -m pip install -r .\\ requirements . txt Cloning and updating submodules can take a while, since we have many submodules and some have a large history.","title":"Cloning the Source Code"},{"location":"howtos/development/building/#start-a-build","text":"To build Spoofax, simply execute: macOS ./b build all Linux ./b build all Windows .\\ bd . bat build all This downloads the latest Stratego/XT, and builds Spoofax. If you also want to build Stratego/XT from scratch, execute: macOS ./b build -st all Linux ./b build -st all Windows .\\ bd . bat build -st all The -s flag build Stratego/XT instead of downloading it, and -t skips the Stratego/XT tests since they are very lengthy. The all part of the command indicates that we want to build all components. For example, if you would only like to build the Java components of Spoofax, and skip the Eclipse plugins, execute: macOS ./b build java Use ./b build to get a list of components available for building, and ./b build --help for help on all the command-line flags and switches. Linux ./b build java Use ./b build to get a list of components available for building, and ./b build --help for help on all the command-line flags and switches. Windows .\\ bd . bat build java Use .\\bd.bat build to get a list of components available for building, and .\\bd.bat build --help for help on all the command-line flags and switches. If you have opened a project in the repository in Eclipse, you must turn off Project \u2023 Build Automatically in Eclipse, otherwise the Maven and Eclipse compilers will interfere and possibly fail the build. After the Maven build is finished, enable Build Automatically again.","title":"Start a Build"},{"location":"howtos/development/building/#updating-the-source-code","text":"If you want to update the repository and submodules, execute: macOS git pull --rebase ./b checkout ./b update Linux git pull --rebase ./b checkout ./b update Windows git pull - -rebase .\\ bd . bat checkout .\\ bd . bat update The git pull command will update any changes in the main repository. The ./b checkout command will check out the correct branches in all submodules, because Git does not do this automatically. The ./b update command will update all submodules.","title":"Updating the Source Code"},{"location":"howtos/development/building/#switching-to-a-different-branch","text":"Switching to a different branch, for example the spoofax-release branch, is done with the following commands: macOS git checkout spoofax-release git pull --rebase git submodule update --init --remote --recursive ./b checkout ./b update Linux git checkout spoofax-release git pull --rebase git submodule update --init --remote --recursive ./b checkout ./b update Windows git checkout spoofax-release git pull - -rebase git submodule update - -init - -remote - -recursive .\\ bd . bat checkout .\\ bd . bat update","title":"Switching to a Different Branch"},{"location":"howtos/development/building/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"howtos/development/building/#resetting-and-cleaning","text":"If updating or checking out a branch of submodule fails (because of unstaged or conflicting changes), you can try to resolve it yourself, or you can reset and clean everything. Reset and clean all submodules using: macOS ./b reset ./b clean Linux ./b checkout ./b update Windows .\\ bd . bat reset .\\ bd . bat clean Risk of loss of data Resetting and cleaning deletes uncommitted and unpushed changes , which can cause permanent data loss . Make sure all your changes are committed and pushed!","title":"Resetting and Cleaning"},{"location":"howtos/development/building/#weird-compilation-errors","text":"If you get any weird compilation errors during the command-line build, make sure that Project \u2023 Build Automatically is turned off in Eclipse.","title":"Weird Compilation Errors"},{"location":"howtos/development/developing/","text":"Developing Spoofax \u00b6 If you are developing a project that is included in Spoofax, it is recommended to set up a development environment. This how-to describes how to set up such a development environment. A working Spoofax build is required before being able to develop. You must be able to successfully build Spoofax by running ./b build all . Do not continue if this does not work. See the instructions on how to build Spoofax . Eclipse \u00b6 Currently, an Eclipse development environment is the most supported environment. An Eclipse development environment can be generated with our scripts. Generating an Eclipse Instance \u00b6 The ./b script in the spoofax-releng repository can generate an Eclipse installation for you. Change directory into the spoofax-releng repository and run: ./b gen-spoofax -l -d ~/eclipse/spoofax-dev This will download and install Eclipse into ~/eclipse/spoofax-dev with the right plugins and eclipse.ini for Spoofax development. The locally built version of the Spoofax plugin will be installed into that Eclipse. Generating an Eclipse installation can take several minutes. After it\u2019s done generating, open the Eclipse installation and confirm that it works by creating a Spoofax project. Installation failed. Cannot complete the install because of a conflicting dependency If you get an error \"Installation failed. Cannot complete the install because of a conflicting dependency.\" , then make sure there is not an existing Eclipse instance at the destination. macOS: To open \u201cEclipse\u201d you need to install the legacy Java SE 6 runtime If upon starting Eclipse you get the error \"To open \u201cEclipse\u201d you need to install the legacy Java SE 6 runtime\" , then you should install a Java JDK 8 or newer for Eclipse to use. If you installed one through SDKMAN! then you have to point Eclipse to it. To do this, edit the Contents/Eclipse/eclipse.ini file in the Eclipse application package content. Add the following lines at the start of the file, where is your username: -vm /Users/<USERNAME>/.sdkman/candidates/java/current/jre/lib/jli/libjli.dylib Fixing Eclipse Settings \u00b6 Some Eclipse settings unfortunately have sub-optimal defaults. Go to the Eclipse preferences and set these options: General Enable : Keep next/previous editor, view and perspectives dialog open General \u2023 Startup and Shutdown Enable : Refresh workspace on startup General \u2023 Workspace Enable : Refresh using native hooks or polling Maven Enable : Do not automatically update dependencies from remote repositories Enable : Download Artifact Sources Enable : Download Artifact JavaDoc Maven \u2023 Annotation Processing Enable : Automatically configure JDT APT Maven \u2023 User Interface Enable : Open XML page in the POM editor by default Run/Debug \u2023 Launching Disable : Build (if required) before launching Developing \u00b6 Import the projects you\u2019d like to develop. To import Java and language projects, use Import \u2023 Maven \u2023 Existing Maven Projects . Eclipse plugins are still imported with Import \u2023 General \u2023 Existing Projects into Workspace . Running \u00b6 To test your changes in the Spoofax Eclipse plugin, import the org.metaborg.spoofax.eclipse project from the spoofax-eclipse repository, which provides launch configurations for starting new Eclipse instances (a \u201cguest\u201d Eclipse). Press the little down arrow next to the bug icon (next to the play icon) and choose Spoofax Eclipse Plugin to start a new Eclipse instance that contains your changes. If it is not in the list of recently used configurations, click Debug configurations... , it should be under Eclipse Application configurations . Some tricks: If you change a (meta-)language and want to test it in a new Eclipse instance, import that language\u2019s corresponding Eclipse plugin project. For example, org.metaborg.meta.lang.nabl has Eclipse plugin project org.metaborg.meta.lang.nabl.eclipse . Then compile both those projects from the command-line (don\u2019t forget to turn off Build Automatically in Eclipse), and start a new Eclipse instance. A different way to test the (meta-)language change is to import that language project into the workspace of the guest Eclipse. Because we use Maven snapshot versions, the built-in version will be overridden when you build the language in the guest eclipse. Troubleshooting \u00b6 If there are many errors in a project, try updating the Maven project. Right click the project and choose Maven \u2023 Update Project... , uncheck Clean projects in the new dialog and press OK . This will update the project from the POM file, update any dependencies, and trigger a build. If this does not solve the problems, try it again but this time with Clean projects checked. Note that if you clean a language project, it has to be rebuilt from the command-line. Restarting Eclipse and repeating these steps may also help. Multiple projects can be updated by selecting multiple projects in the package/project explorer, or by checking projects in the update dialog. If you have particular trouble with org.eclipse.* plugins in the MANIFEST.MF file that do not resolve, try the following. Go to Preferences \u2023 Plug-in Development \u2023 Target Platform , most likely there will not be an active Running Platform there. You can use Add... to add a new one if there isn\u2019t one already. Select the Default option, click Next , then click Finish . Check the box next to the platform to activate it. Advanced: Developing from Scratch \u00b6 In some cases it can be beneficial to have full control over all projects, instead of relying on Maven artifacts and the installed Spoofax plugin. To develop completely from scratch, uninstall Spoofax from Eclipse, and import all projects by importing releng/eclipse/import/pom.xml , which will import all relevant projects automatically. If you change a language project, build them on the command-line, because languages cannot be built inside Eclipse without the Spoofax plugin. IntelliJ \u00b6 Easiest is to install the latest release of the Spoofax plugin in an installation of IntelliJ IDEA. Otherwise, you may want to build it from source, and to run the built plugin inside a special sandbox-instance of IntelliJ IDEA, execute the following command: ./gradlew runIdea Alternatively, in IntelliJ IDEA you can invoke the IntelliJ Plugin run/debug configuration. You can use this to run or debug the IntelliJ IDEA plugin code. However, this cannot be used to debug the JPS Spoofax build process. To debug the JPS Spoofax build process, you need to execute the following command: ./gradlew debugJps ...or invoke the IntelliJ Plugin (Debug JPS) run configuration ( not debug ) from IntelliJ. Then in the sandbox IntelliJ IDEA instance you enable the Debug Build Process action ( Ctrl + Shift + A ). Then you start a build. IntelliJ will wait for a debugger to be attached to port 5005. Attach a debugger, and the build will continue. From the Spoofax plugin\u2019s IntelliJ IDEA project, you can invoke the JPS Plugin remote debug configuration to attach the debugger. Logging \u00b6 To get debug logging in IntelliJ, locate the bin/log.xml file in the IntelliJ folder and add the following snippet in the <log4j:configuration> element, just above the <root> element: <category name= \"#org.metaborg\" additivity= \"true\" > <priority value= \"DEBUG\" /> <appender-ref ref= \"CONSOLE-DEBUG\" /> <appender-ref ref= \"FILE\" /> </category>","title":"Developing"},{"location":"howtos/development/developing/#developing-spoofax","text":"If you are developing a project that is included in Spoofax, it is recommended to set up a development environment. This how-to describes how to set up such a development environment. A working Spoofax build is required before being able to develop. You must be able to successfully build Spoofax by running ./b build all . Do not continue if this does not work. See the instructions on how to build Spoofax .","title":"Developing Spoofax"},{"location":"howtos/development/developing/#eclipse","text":"Currently, an Eclipse development environment is the most supported environment. An Eclipse development environment can be generated with our scripts.","title":"Eclipse"},{"location":"howtos/development/developing/#generating-an-eclipse-instance","text":"The ./b script in the spoofax-releng repository can generate an Eclipse installation for you. Change directory into the spoofax-releng repository and run: ./b gen-spoofax -l -d ~/eclipse/spoofax-dev This will download and install Eclipse into ~/eclipse/spoofax-dev with the right plugins and eclipse.ini for Spoofax development. The locally built version of the Spoofax plugin will be installed into that Eclipse. Generating an Eclipse installation can take several minutes. After it\u2019s done generating, open the Eclipse installation and confirm that it works by creating a Spoofax project. Installation failed. Cannot complete the install because of a conflicting dependency If you get an error \"Installation failed. Cannot complete the install because of a conflicting dependency.\" , then make sure there is not an existing Eclipse instance at the destination. macOS: To open \u201cEclipse\u201d you need to install the legacy Java SE 6 runtime If upon starting Eclipse you get the error \"To open \u201cEclipse\u201d you need to install the legacy Java SE 6 runtime\" , then you should install a Java JDK 8 or newer for Eclipse to use. If you installed one through SDKMAN! then you have to point Eclipse to it. To do this, edit the Contents/Eclipse/eclipse.ini file in the Eclipse application package content. Add the following lines at the start of the file, where is your username: -vm /Users/<USERNAME>/.sdkman/candidates/java/current/jre/lib/jli/libjli.dylib","title":"Generating an Eclipse Instance"},{"location":"howtos/development/developing/#fixing-eclipse-settings","text":"Some Eclipse settings unfortunately have sub-optimal defaults. Go to the Eclipse preferences and set these options: General Enable : Keep next/previous editor, view and perspectives dialog open General \u2023 Startup and Shutdown Enable : Refresh workspace on startup General \u2023 Workspace Enable : Refresh using native hooks or polling Maven Enable : Do not automatically update dependencies from remote repositories Enable : Download Artifact Sources Enable : Download Artifact JavaDoc Maven \u2023 Annotation Processing Enable : Automatically configure JDT APT Maven \u2023 User Interface Enable : Open XML page in the POM editor by default Run/Debug \u2023 Launching Disable : Build (if required) before launching","title":"Fixing Eclipse Settings"},{"location":"howtos/development/developing/#developing","text":"Import the projects you\u2019d like to develop. To import Java and language projects, use Import \u2023 Maven \u2023 Existing Maven Projects . Eclipse plugins are still imported with Import \u2023 General \u2023 Existing Projects into Workspace .","title":"Developing"},{"location":"howtos/development/developing/#running","text":"To test your changes in the Spoofax Eclipse plugin, import the org.metaborg.spoofax.eclipse project from the spoofax-eclipse repository, which provides launch configurations for starting new Eclipse instances (a \u201cguest\u201d Eclipse). Press the little down arrow next to the bug icon (next to the play icon) and choose Spoofax Eclipse Plugin to start a new Eclipse instance that contains your changes. If it is not in the list of recently used configurations, click Debug configurations... , it should be under Eclipse Application configurations . Some tricks: If you change a (meta-)language and want to test it in a new Eclipse instance, import that language\u2019s corresponding Eclipse plugin project. For example, org.metaborg.meta.lang.nabl has Eclipse plugin project org.metaborg.meta.lang.nabl.eclipse . Then compile both those projects from the command-line (don\u2019t forget to turn off Build Automatically in Eclipse), and start a new Eclipse instance. A different way to test the (meta-)language change is to import that language project into the workspace of the guest Eclipse. Because we use Maven snapshot versions, the built-in version will be overridden when you build the language in the guest eclipse.","title":"Running"},{"location":"howtos/development/developing/#troubleshooting","text":"If there are many errors in a project, try updating the Maven project. Right click the project and choose Maven \u2023 Update Project... , uncheck Clean projects in the new dialog and press OK . This will update the project from the POM file, update any dependencies, and trigger a build. If this does not solve the problems, try it again but this time with Clean projects checked. Note that if you clean a language project, it has to be rebuilt from the command-line. Restarting Eclipse and repeating these steps may also help. Multiple projects can be updated by selecting multiple projects in the package/project explorer, or by checking projects in the update dialog. If you have particular trouble with org.eclipse.* plugins in the MANIFEST.MF file that do not resolve, try the following. Go to Preferences \u2023 Plug-in Development \u2023 Target Platform , most likely there will not be an active Running Platform there. You can use Add... to add a new one if there isn\u2019t one already. Select the Default option, click Next , then click Finish . Check the box next to the platform to activate it.","title":"Troubleshooting"},{"location":"howtos/development/developing/#advanced-developing-from-scratch","text":"In some cases it can be beneficial to have full control over all projects, instead of relying on Maven artifacts and the installed Spoofax plugin. To develop completely from scratch, uninstall Spoofax from Eclipse, and import all projects by importing releng/eclipse/import/pom.xml , which will import all relevant projects automatically. If you change a language project, build them on the command-line, because languages cannot be built inside Eclipse without the Spoofax plugin.","title":"Advanced: Developing from Scratch"},{"location":"howtos/development/developing/#intellij","text":"Easiest is to install the latest release of the Spoofax plugin in an installation of IntelliJ IDEA. Otherwise, you may want to build it from source, and to run the built plugin inside a special sandbox-instance of IntelliJ IDEA, execute the following command: ./gradlew runIdea Alternatively, in IntelliJ IDEA you can invoke the IntelliJ Plugin run/debug configuration. You can use this to run or debug the IntelliJ IDEA plugin code. However, this cannot be used to debug the JPS Spoofax build process. To debug the JPS Spoofax build process, you need to execute the following command: ./gradlew debugJps ...or invoke the IntelliJ Plugin (Debug JPS) run configuration ( not debug ) from IntelliJ. Then in the sandbox IntelliJ IDEA instance you enable the Debug Build Process action ( Ctrl + Shift + A ). Then you start a build. IntelliJ will wait for a debugger to be attached to port 5005. Attach a debugger, and the build will continue. From the Spoofax plugin\u2019s IntelliJ IDEA project, you can invoke the JPS Plugin remote debug configuration to attach the debugger.","title":"IntelliJ"},{"location":"howtos/development/developing/#logging","text":"To get debug logging in IntelliJ, locate the bin/log.xml file in the IntelliJ folder and add the following snippet in the <log4j:configuration> element, just above the <root> element: <category name= \"#org.metaborg\" additivity= \"true\" > <priority value= \"DEBUG\" /> <appender-ref ref= \"CONSOLE-DEBUG\" /> <appender-ref ref= \"FILE\" /> </category>","title":"Logging"},{"location":"howtos/development/maven/","text":"Spoofax Development Maven Setup \u00b6 Maven is a project management and build tool for software projects. Most components in Spoofax are built with Maven. This how-to will guide you to setup Maven for Spoofax 2 development. Installation \u00b6 Maven can be downloaded and installed from https://maven.apache.org/download.cgi . We require Maven 3.5.4 or newer (except Maven 3.6.1 and 3.6.2). On macOs, Maven can be easily installed with Homebrew by executing: brew install maven Confirm the installation was successful and the version is supported by running mvn --version . Memory Allocation \u00b6 By default, Maven does not assign a lot of memory to the JVM that it runs in, which may lead to out-of-memory exceptions during builds. To increase the allocated memory, execute before building: export MAVEN_OPTS = \"-Xms512m -Xmx1024m -Xss16m\" Such an export is not permanent. To make it permanent, add that line to ~/.bashrc or equivalent for your OS/shell (create the file if it does not exist), which will execute it whenever a new shell is opened. Proxy Settings \u00b6 If you are behind a proxy, please put the proxy settings in your ~/.m2/settings.xml file. When you use the ./b script to build Spoofax, the MAVEN_OPTS environment variable is overridden to ensure the memory options above are supplied, so using command-line options in the environment variable for the proxy settings does not work. Spoofax Maven Artifacts \u00b6 Spoofax\u2019s Maven artifacts are hosted on our artifact server at artifacts.metaborg.org . To use these artifacts, repositories have to be added to your Maven configuration. This configuration is required when building and developing Spoofax. Repositories can be added to your local Maven settings file (which is recommended), or to a project\u2019s POM file. Simple: Local Settings File \u00b6 The recommended approach is to add repositories to your local Maven settings file, located at ~/.m2/settings.xml . If you have not created this file yet, or want to completely replace it, simply create it with the following content: ~/.m2/settings.xml <?xml version=\"1.0\" ?> <settings xmlns= \"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\" > <profiles> <profile> <id> add-metaborg-release-repos </id> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <id> metaborg-release-repo </id> <url> https://artifacts.metaborg.org/content/repositories/releases/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> metaborg-release-repo </id> <url> https://artifacts.metaborg.org/content/repositories/releases/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> <profile> <id> add-metaborg-snapshot-repos </id> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <id> metaborg-snapshot-repo </id> <url> https://artifacts.metaborg.org/content/repositories/snapshots/ </url> <releases> <enabled> false </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> metaborg-snapshot-repo </id> <url> https://artifacts.metaborg.org/content/repositories/snapshots/ </url> <releases> <enabled> false </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> </profiles> <mirrors> <mirror> <id> metaborg-central-mirror </id> <url> https://artifacts.metaborg.org/content/repositories/central/ </url> <mirrorOf> central </mirrorOf> </mirror> </mirrors> </settings> If you\u2019ve already created a settings file before and want to add the repositories, just add the profile element (and the profiles element if it does not exist yet) to the settings file. Advanced: Project POM File \u00b6 Repositories can also be added directly to a project\u2019s POM file, which only set the repositories for that particular project. This is not recommended, because it makes repositories harder to change by users, and duplicates the configuration. But it can be convenient, because it does not require an external settings file. To do this, just add the the following content to the POM file: ~/.m2/settings.xml <repositories> <repository> <id> metaborg-release-repo </id> <url> https://artifacts.metaborg.org/content/repositories/releases/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </repository> <repository> <id> metaborg-snapshot-repo </id> <url> https://artifacts.metaborg.org/content/repositories/snapshots/ </url> <releases> <enabled> false </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> metaborg-release-repo </id> <url> https://artifacts.metaborg.org/content/repositories/releases/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </pluginRepository> <pluginRepository> <id> metaborg-snapshot-repo </id> <url> https://artifacts.metaborg.org/content/repositories/snapshots/ </url> <releases> <enabled> false </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </pluginRepository> </pluginRepositories> Maven Central Repository Mirror \u00b6 Artifacts of most open source projects are hosted in the Maven Central Repository . If you are building any project using Maven, many artifacts will be downloaded from that server. While it is a fast server, it can still take a while to download all required artifacts for big projects. If you are on the TU Delft network, you can use our local mirror of Maven Central to speed things up. Using the mirroring requires a change in your local ~/.m2/settings.xml file. If this file does not exist, create it with the following content: ~/.m2/settings.xml <?xml version=\"1.0\" ?> <settings xmlns= \"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\" > <mirrors> <mirror> <id> metaborg-central-mirror </id> <url> https://artifacts.metaborg.org/content/repositories/central/ </url> <mirrorOf> central </mirrorOf> </mirror> </mirrors> </settings> If you\u2019ve already created a settings file before and want to add the mirror configuration, just add the mirror element (and the mirrors element if it does not exist yet) to the settings file.","title":"Maven"},{"location":"howtos/development/maven/#spoofax-development-maven-setup","text":"Maven is a project management and build tool for software projects. Most components in Spoofax are built with Maven. This how-to will guide you to setup Maven for Spoofax 2 development.","title":"Spoofax Development Maven Setup"},{"location":"howtos/development/maven/#installation","text":"Maven can be downloaded and installed from https://maven.apache.org/download.cgi . We require Maven 3.5.4 or newer (except Maven 3.6.1 and 3.6.2). On macOs, Maven can be easily installed with Homebrew by executing: brew install maven Confirm the installation was successful and the version is supported by running mvn --version .","title":"Installation"},{"location":"howtos/development/maven/#memory-allocation","text":"By default, Maven does not assign a lot of memory to the JVM that it runs in, which may lead to out-of-memory exceptions during builds. To increase the allocated memory, execute before building: export MAVEN_OPTS = \"-Xms512m -Xmx1024m -Xss16m\" Such an export is not permanent. To make it permanent, add that line to ~/.bashrc or equivalent for your OS/shell (create the file if it does not exist), which will execute it whenever a new shell is opened.","title":"Memory Allocation"},{"location":"howtos/development/maven/#proxy-settings","text":"If you are behind a proxy, please put the proxy settings in your ~/.m2/settings.xml file. When you use the ./b script to build Spoofax, the MAVEN_OPTS environment variable is overridden to ensure the memory options above are supplied, so using command-line options in the environment variable for the proxy settings does not work.","title":"Proxy Settings"},{"location":"howtos/development/maven/#spoofax-maven-artifacts","text":"Spoofax\u2019s Maven artifacts are hosted on our artifact server at artifacts.metaborg.org . To use these artifacts, repositories have to be added to your Maven configuration. This configuration is required when building and developing Spoofax. Repositories can be added to your local Maven settings file (which is recommended), or to a project\u2019s POM file.","title":"Spoofax Maven Artifacts"},{"location":"howtos/development/maven/#simple-local-settings-file","text":"The recommended approach is to add repositories to your local Maven settings file, located at ~/.m2/settings.xml . If you have not created this file yet, or want to completely replace it, simply create it with the following content: ~/.m2/settings.xml <?xml version=\"1.0\" ?> <settings xmlns= \"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\" > <profiles> <profile> <id> add-metaborg-release-repos </id> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <id> metaborg-release-repo </id> <url> https://artifacts.metaborg.org/content/repositories/releases/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> metaborg-release-repo </id> <url> https://artifacts.metaborg.org/content/repositories/releases/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> <profile> <id> add-metaborg-snapshot-repos </id> <activation> <activeByDefault> true </activeByDefault> </activation> <repositories> <repository> <id> metaborg-snapshot-repo </id> <url> https://artifacts.metaborg.org/content/repositories/snapshots/ </url> <releases> <enabled> false </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> metaborg-snapshot-repo </id> <url> https://artifacts.metaborg.org/content/repositories/snapshots/ </url> <releases> <enabled> false </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </pluginRepository> </pluginRepositories> </profile> </profiles> <mirrors> <mirror> <id> metaborg-central-mirror </id> <url> https://artifacts.metaborg.org/content/repositories/central/ </url> <mirrorOf> central </mirrorOf> </mirror> </mirrors> </settings> If you\u2019ve already created a settings file before and want to add the repositories, just add the profile element (and the profiles element if it does not exist yet) to the settings file.","title":"Simple: Local Settings File"},{"location":"howtos/development/maven/#advanced-project-pom-file","text":"Repositories can also be added directly to a project\u2019s POM file, which only set the repositories for that particular project. This is not recommended, because it makes repositories harder to change by users, and duplicates the configuration. But it can be convenient, because it does not require an external settings file. To do this, just add the the following content to the POM file: ~/.m2/settings.xml <repositories> <repository> <id> metaborg-release-repo </id> <url> https://artifacts.metaborg.org/content/repositories/releases/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </repository> <repository> <id> metaborg-snapshot-repo </id> <url> https://artifacts.metaborg.org/content/repositories/snapshots/ </url> <releases> <enabled> false </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </repository> </repositories> <pluginRepositories> <pluginRepository> <id> metaborg-release-repo </id> <url> https://artifacts.metaborg.org/content/repositories/releases/ </url> <releases> <enabled> true </enabled> </releases> <snapshots> <enabled> false </enabled> </snapshots> </pluginRepository> <pluginRepository> <id> metaborg-snapshot-repo </id> <url> https://artifacts.metaborg.org/content/repositories/snapshots/ </url> <releases> <enabled> false </enabled> </releases> <snapshots> <enabled> true </enabled> </snapshots> </pluginRepository> </pluginRepositories>","title":"Advanced: Project POM File"},{"location":"howtos/development/maven/#maven-central-repository-mirror","text":"Artifacts of most open source projects are hosted in the Maven Central Repository . If you are building any project using Maven, many artifacts will be downloaded from that server. While it is a fast server, it can still take a while to download all required artifacts for big projects. If you are on the TU Delft network, you can use our local mirror of Maven Central to speed things up. Using the mirroring requires a change in your local ~/.m2/settings.xml file. If this file does not exist, create it with the following content: ~/.m2/settings.xml <?xml version=\"1.0\" ?> <settings xmlns= \"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\" > <mirrors> <mirror> <id> metaborg-central-mirror </id> <url> https://artifacts.metaborg.org/content/repositories/central/ </url> <mirrorOf> central </mirrorOf> </mirror> </mirrors> </settings> If you\u2019ve already created a settings file before and want to add the mirror configuration, just add the mirror element (and the mirrors element if it does not exist yet) to the settings file.","title":"Maven Central Repository Mirror"},{"location":"howtos/development/releasing/","text":"Releasing Spoofax \u00b6 This how-to describes how to release Spoofax. Requirements \u00b6 To release Spoofax, you must first be able to build Spoofax. Follow the Maven and Building guides first. To publish releases, you will need write access to the spoofax-releng repository, to all submodule repositories in that repository, and to this documentation repository. An account with deploy access to our artifact server is required. Ask an administrator of the Programming Languages group to get access to the repositories and artifact server. Instructions \u00b6 Prepare Maven deploy settings. Open your ~/.m2/settings.xml file. Add a <servers></servers> section if it does not exist. Add a server to the servers section with id metaborg-nexus that contains your username and password to our artifact server: <server> <id> metaborg-nexus </id> <username> myusername </username> <password> mypassword </password> </server> Optionally encrypt your password by following the Password Encryption guide . Prepare the repository containing the build scripts. Clone or re-use an existing clone of spoofax-releng on the master branch. See Cloning the source code . Update it to the latest commit with: git pull --rebase && ./b checkout -y && ./b update To enable Git pushing without having to supply a username and password via Git over HTTPS, run the following command to set the submodule remotes to SSH URLs: ./b set-remote -s Prepare the source code repository. Make a separate clone (or re-use an existing one if you have released Spoofax before) of the spoofax-release branch of spoofax-releng . This must be a separate clone in a different directory from the first one. See Cloning the source code . Why two separate clones of spoofax-releng ? The reason for two separate clones of spoofax-releng is that the release script will modify the files in the repository, which could include files of the release script itself. Therefore, we make a separate clone which the release script acts upon, so that it does not interfere with itself. If reusing an existing clone, ensure that it is checked out to spoofax-release with: git checkout spoofax-release ...and update it to the latest commit with: git pull --rebase && ./b checkout -y && ./b update If there are new submodules repositories, follow the steps for preparing new submodules below. To enable Git pushing without having to supply a username and password via Git over HTTPS, run the following command to set the submodule remotes to SSH URLs: ./b set-remote -s Perform the release. Change directory into the repository cloned in step 2. For example: cd /Users/gohla/spoofax/master/spoofax-releng Get an absolute path to the repository cloned in step 3. For example: /Users/gohla/spoofax/release/spoofax-releng Determine whether the release will be patch or minor/major . For a patch release, we do not bump the development version. For a minor or major release, we do. Figure out what the current development version of Spoofax is, what the next release version should be, and if doing a non-patch release, what the next development version should be. The release script will change the current development version into the next release version, deploy that, and then change the current development version to the next development version, and commit that. Setting the next development version is optional. Execute the release script with the parameters you gathered: ./b --repo <release-repository> release \\ spoofax-release <release-version> \\ master <current-development-version> \\ --non-interactive \\ --maven-deploy \\ --maven-deploy-identifier metaborg-nexus \\ --maven-deploy-url http://artifacts.metaborg.org/content/repositories/releases/ \\ --nexus-deploy \\ --nexus-username <artifact-server-username> \\ --nexus-password <artifact-server-password> \\ --nexus-repo releases ...or for a major version, with --next-develop-version : ./b --repo <release-repository> release \\ spoofax-release <release-version> \\ master <current-development-version> \\ --next-develop-version <next-development-version> \\ --non-interactive \\ --maven-deploy \\ --maven-deploy-identifier metaborg-nexus \\ --maven-deploy-url http://artifacts.metaborg.org/content/repositories/releases/ \\ --nexus-deploy \\ --nexus-username <artifact-server-username> \\ --nexus-password <artifact-server-password> \\ --nexus-repo releases For example, if we currently are at development version 2.6.0-SNAPSHOT , and would like to release minor version 2.6.0 , and update the development version to 2.7.0-SNAPSHOT , we would execute the following command: cd /Users/gohla/spoofax/master/spoofax-releng ./b --repo /Users/gohla/spoofax/release/spoofax-releng release \\ spoofax-release 2 .6.0 \\ master 2 .6.0-SNAPSHOT \\ --next-develop-version 2 .7.0-SNAPSHOT \\ --non-interactive \\ --maven-deploy \\ --maven-deploy-identifier metaborg-nexus \\ --maven-deploy-url http://artifacts.metaborg.org/content/repositories/releases/ \\ --nexus-deploy \\ --nexus-username myusername \\ --nexus-password mypassword \\ --nexus-repo releases Unfortunately, it is currently not possible to encrypt the artifact server password passed to the build script. New spoofax-releng Submodules \u00b6 When adding a new submodule to the spoofax-releng repository, the following steps must be performed before starting the automated release process: Add a spoofax-release branch to the submodule (pointing to the current master branch), and push that branch. Add the submodule to the .gitmodule file in the spoofax-release branch of the spoofax-releng repository. Make sure that the branch of the submodule is set to spoofax-release , and that the remote is using an https URL. Commit and push this change. Updating the Release Archive \u00b6 To update the release archive of this documentation site, perform the following steps after a release: Update include files: Copy include/hyperlink/download-<current-release-version>.rst to new file include/hyperlink/download-<release-version>.rst , replace all instances of <current-release-version> in that new file with <release-version> , and update the date to the current date. In include/hyperlink/download-rel.rst , replace all instances of <current-release-version> with <release-version> . In include/hyperlink/download-dev.rst , update the development version to <next-development-version> . In include/_all.rst , add a new line to include the newly copied file: .. include:: /include/hyperlink/download-<release-version>.rst. Update source/release/migrate/<release-version>.rst (only if migrations are necessary): Remove stub notice. Update source/release/note/<release-version>.rst : Remove stub notice. Add small summary of the release as an introduction. Include download links, which can be copied and have their versions replaced from a previous release. Create new stub files for the next release: Create a new migration guide stub file. Create a new release notes stub file. Update source/release/note/index.rst : Move stub for this release to the top of the notes. Add new stub file at the bottom of the notes. Update source/release/migrate/index.rst : Move stub for this release to the top of the migration guides. Add new stub file at the bottom of the migration guides. Update conf.py : Update version variable. Update copyright variable with new year, if needed.","title":"Releasing"},{"location":"howtos/development/releasing/#releasing-spoofax","text":"This how-to describes how to release Spoofax.","title":"Releasing Spoofax"},{"location":"howtos/development/releasing/#requirements","text":"To release Spoofax, you must first be able to build Spoofax. Follow the Maven and Building guides first. To publish releases, you will need write access to the spoofax-releng repository, to all submodule repositories in that repository, and to this documentation repository. An account with deploy access to our artifact server is required. Ask an administrator of the Programming Languages group to get access to the repositories and artifact server.","title":"Requirements"},{"location":"howtos/development/releasing/#instructions","text":"Prepare Maven deploy settings. Open your ~/.m2/settings.xml file. Add a <servers></servers> section if it does not exist. Add a server to the servers section with id metaborg-nexus that contains your username and password to our artifact server: <server> <id> metaborg-nexus </id> <username> myusername </username> <password> mypassword </password> </server> Optionally encrypt your password by following the Password Encryption guide . Prepare the repository containing the build scripts. Clone or re-use an existing clone of spoofax-releng on the master branch. See Cloning the source code . Update it to the latest commit with: git pull --rebase && ./b checkout -y && ./b update To enable Git pushing without having to supply a username and password via Git over HTTPS, run the following command to set the submodule remotes to SSH URLs: ./b set-remote -s Prepare the source code repository. Make a separate clone (or re-use an existing one if you have released Spoofax before) of the spoofax-release branch of spoofax-releng . This must be a separate clone in a different directory from the first one. See Cloning the source code . Why two separate clones of spoofax-releng ? The reason for two separate clones of spoofax-releng is that the release script will modify the files in the repository, which could include files of the release script itself. Therefore, we make a separate clone which the release script acts upon, so that it does not interfere with itself. If reusing an existing clone, ensure that it is checked out to spoofax-release with: git checkout spoofax-release ...and update it to the latest commit with: git pull --rebase && ./b checkout -y && ./b update If there are new submodules repositories, follow the steps for preparing new submodules below. To enable Git pushing without having to supply a username and password via Git over HTTPS, run the following command to set the submodule remotes to SSH URLs: ./b set-remote -s Perform the release. Change directory into the repository cloned in step 2. For example: cd /Users/gohla/spoofax/master/spoofax-releng Get an absolute path to the repository cloned in step 3. For example: /Users/gohla/spoofax/release/spoofax-releng Determine whether the release will be patch or minor/major . For a patch release, we do not bump the development version. For a minor or major release, we do. Figure out what the current development version of Spoofax is, what the next release version should be, and if doing a non-patch release, what the next development version should be. The release script will change the current development version into the next release version, deploy that, and then change the current development version to the next development version, and commit that. Setting the next development version is optional. Execute the release script with the parameters you gathered: ./b --repo <release-repository> release \\ spoofax-release <release-version> \\ master <current-development-version> \\ --non-interactive \\ --maven-deploy \\ --maven-deploy-identifier metaborg-nexus \\ --maven-deploy-url http://artifacts.metaborg.org/content/repositories/releases/ \\ --nexus-deploy \\ --nexus-username <artifact-server-username> \\ --nexus-password <artifact-server-password> \\ --nexus-repo releases ...or for a major version, with --next-develop-version : ./b --repo <release-repository> release \\ spoofax-release <release-version> \\ master <current-development-version> \\ --next-develop-version <next-development-version> \\ --non-interactive \\ --maven-deploy \\ --maven-deploy-identifier metaborg-nexus \\ --maven-deploy-url http://artifacts.metaborg.org/content/repositories/releases/ \\ --nexus-deploy \\ --nexus-username <artifact-server-username> \\ --nexus-password <artifact-server-password> \\ --nexus-repo releases For example, if we currently are at development version 2.6.0-SNAPSHOT , and would like to release minor version 2.6.0 , and update the development version to 2.7.0-SNAPSHOT , we would execute the following command: cd /Users/gohla/spoofax/master/spoofax-releng ./b --repo /Users/gohla/spoofax/release/spoofax-releng release \\ spoofax-release 2 .6.0 \\ master 2 .6.0-SNAPSHOT \\ --next-develop-version 2 .7.0-SNAPSHOT \\ --non-interactive \\ --maven-deploy \\ --maven-deploy-identifier metaborg-nexus \\ --maven-deploy-url http://artifacts.metaborg.org/content/repositories/releases/ \\ --nexus-deploy \\ --nexus-username myusername \\ --nexus-password mypassword \\ --nexus-repo releases Unfortunately, it is currently not possible to encrypt the artifact server password passed to the build script.","title":"Instructions"},{"location":"howtos/development/releasing/#new-spoofax-releng-submodules","text":"When adding a new submodule to the spoofax-releng repository, the following steps must be performed before starting the automated release process: Add a spoofax-release branch to the submodule (pointing to the current master branch), and push that branch. Add the submodule to the .gitmodule file in the spoofax-release branch of the spoofax-releng repository. Make sure that the branch of the submodule is set to spoofax-release , and that the remote is using an https URL. Commit and push this change.","title":"New spoofax-releng Submodules"},{"location":"howtos/development/releasing/#updating-the-release-archive","text":"To update the release archive of this documentation site, perform the following steps after a release: Update include files: Copy include/hyperlink/download-<current-release-version>.rst to new file include/hyperlink/download-<release-version>.rst , replace all instances of <current-release-version> in that new file with <release-version> , and update the date to the current date. In include/hyperlink/download-rel.rst , replace all instances of <current-release-version> with <release-version> . In include/hyperlink/download-dev.rst , update the development version to <next-development-version> . In include/_all.rst , add a new line to include the newly copied file: .. include:: /include/hyperlink/download-<release-version>.rst. Update source/release/migrate/<release-version>.rst (only if migrations are necessary): Remove stub notice. Update source/release/note/<release-version>.rst : Remove stub notice. Add small summary of the release as an introduction. Include download links, which can be copied and have their versions replaced from a previous release. Create new stub files for the next release: Create a new migration guide stub file. Create a new release notes stub file. Update source/release/note/index.rst : Move stub for this release to the top of the notes. Add new stub file at the bottom of the notes. Update source/release/migrate/index.rst : Move stub for this release to the top of the migration guides. Add new stub file at the bottom of the migration guides. Update conf.py : Update version variable. Update copyright variable with new year, if needed.","title":"Updating the Release Archive"},{"location":"howtos/development/requirements/","text":"Spoofax Development Requirements \u00b6 This how-to will instruct you which requirements you need to install to do Spoofax 2 development. Spoofax can be run on macOS, Linux, and Windows. Building is directly supported on macOS and Linux. Building on Windows is supported through the Windows Subsystem for Linux (Bash on Windows) . The following tools are required to build and develop Spoofax: Git 1.8.2 or newer Required to check out the source code from our GitHub repositories. Instructions on how to install Git for your platform can be found here: https://git-scm.com/downloads . If you run macOs and have Homebrew installed, you can install Git by executing brew install git . Confirm your Git installation by executing git version . Java JDK 8 or newer Required to build and run Java components. The latest JDK can be downloaded and installed from: https://www.oracle.com/technetwork/java/javase/downloads/index.html . On macOs, it can be a bit tricky to use the installed JDK, because Apple by default installs JRE 6. To check which version of Java you are running, execute the java -version command. If this tells you that the Java version is 1.8 or newer, or Java 9 or newer, everything is fine. If not, you can either install a newer Java version through Homebrew ( brew install --cask adoptopenjdk8 ), or use a JDK manager such as SDKMAN! . Python 3.4 or newer Python scripts are used to orchestrate the build. Instructions on how to install Python for your platform can be found here: https://www.python.org/downloads/ . If you run macOs and have Homebrew installed, you can install Python by executing brew install python3 . Confirm your Python installation by executing python3 --version or python --version , depending on how your package manager sets up Python. During a build of Spoofax, Pip will install some Python dependencies into a virtual environment. No extra Python dependencies are required for this (with one small exception, see the note below). The latest version of Pip will automatically be installed inside the virtual environment. Debian and derivatives (like Ubuntu) do not include the full standard library when installing Python ( bug 1290847 ), so you will need to install python3-venv to ensure the virtual environment can be created. Maven 3.5.4 or newer (except Maven 3.6.1 and 3.6.2) Maven is required to build most components of Spoofax. Our Maven artifact server must also be registered with Maven since the build depends on artifacts from previous builds for bootstrapping purposes. We explain how to install and set up Maven in this how-to . Spoofax cannot be built using Maven 3.6.1 or 3.6.2 due to bugs MNG-6642 and MNG-6765 . Docker Required on macOS Catalina, Big Sur, Monterey, and newer to be able to run the sdf2table and implodePT legacy binaries. On macOS, install it though the Docker for Mac website. Coreutils Required on macOS to be able to run the sdf2table and implodePT legacy binaries. On macOS with Homebrew installed, you can install them by running brew install coreutils .","title":"Requirements"},{"location":"howtos/development/requirements/#spoofax-development-requirements","text":"This how-to will instruct you which requirements you need to install to do Spoofax 2 development. Spoofax can be run on macOS, Linux, and Windows. Building is directly supported on macOS and Linux. Building on Windows is supported through the Windows Subsystem for Linux (Bash on Windows) . The following tools are required to build and develop Spoofax: Git 1.8.2 or newer Required to check out the source code from our GitHub repositories. Instructions on how to install Git for your platform can be found here: https://git-scm.com/downloads . If you run macOs and have Homebrew installed, you can install Git by executing brew install git . Confirm your Git installation by executing git version . Java JDK 8 or newer Required to build and run Java components. The latest JDK can be downloaded and installed from: https://www.oracle.com/technetwork/java/javase/downloads/index.html . On macOs, it can be a bit tricky to use the installed JDK, because Apple by default installs JRE 6. To check which version of Java you are running, execute the java -version command. If this tells you that the Java version is 1.8 or newer, or Java 9 or newer, everything is fine. If not, you can either install a newer Java version through Homebrew ( brew install --cask adoptopenjdk8 ), or use a JDK manager such as SDKMAN! . Python 3.4 or newer Python scripts are used to orchestrate the build. Instructions on how to install Python for your platform can be found here: https://www.python.org/downloads/ . If you run macOs and have Homebrew installed, you can install Python by executing brew install python3 . Confirm your Python installation by executing python3 --version or python --version , depending on how your package manager sets up Python. During a build of Spoofax, Pip will install some Python dependencies into a virtual environment. No extra Python dependencies are required for this (with one small exception, see the note below). The latest version of Pip will automatically be installed inside the virtual environment. Debian and derivatives (like Ubuntu) do not include the full standard library when installing Python ( bug 1290847 ), so you will need to install python3-venv to ensure the virtual environment can be created. Maven 3.5.4 or newer (except Maven 3.6.1 and 3.6.2) Maven is required to build most components of Spoofax. Our Maven artifact server must also be registered with Maven since the build depends on artifacts from previous builds for bootstrapping purposes. We explain how to install and set up Maven in this how-to . Spoofax cannot be built using Maven 3.6.1 or 3.6.2 due to bugs MNG-6642 and MNG-6765 . Docker Required on macOS Catalina, Big Sur, Monterey, and newer to be able to run the sdf2table and implodePT legacy binaries. On macOS, install it though the Docker for Mac website. Coreutils Required on macOS to be able to run the sdf2table and implodePT legacy binaries. On macOS with Homebrew installed, you can install them by running brew install coreutils .","title":"Spoofax Development Requirements"},{"location":"howtos/development/troubleshooting/","text":"Development Troubleshooting \u00b6 This page provides some troubleshooting information. Developing on macOS Catalina and newer \u00b6 Due to some legacy 32-bit binaries ( sdf2table and implodePT ) that are required to build some of the Spoofax meta-languages, building on macOS Catalina or newer (Big Sur, Monterey) requires the following additional components to be installed: coreutils Docker for Mac Refer to the Requirements page for more information and installation instructions. The following subsections detail some problems that can occur when developing on macOS Catalina or newer. realpath: command not found \u00b6 You should install coreutils . Refer to the Requirements page for installation instructions. Invoke the following command to test it (it should return the full path to the current directory): realpath -s . Mounts denied: The path is not shared the host and is not known to Docker \u00b6 Error response from daemon: Mounts denied: The path /var/folders/a_/foo0000bar/T/vfs_cache-345/tmp_123_macosx/sdf2table-macosx is not shared from the host and is not known to Docker. Ensure the root of the path ( /var/folders in this example) is shared in Docker for Mac. Go to Docker for Mac Preferences \u2023 Resources \u2023 File Sharing and add the path to the list. The default list should contain: /Users /Volumes /private /tmp /var/folders invalid mount config for type \"bind\": bind source path does not exist \u00b6 Error response from daemon: invalid mount config for type \"bind\": bind source path does not exist: /var/folders/a_/foo0000bar/T/vfs_cache-345/tmp_123_macosx Ensure that both Docker and the terminal from which you are invoking the build have full-disk access . Go to macOS Preferences \u2023 Security and Privacy \u2023 Full Disk Access and check the checkboxes next to Docker and your terminal ( Terminal and/or iTerm ).","title":"Troubleshooting"},{"location":"howtos/development/troubleshooting/#development-troubleshooting","text":"This page provides some troubleshooting information.","title":"Development Troubleshooting"},{"location":"howtos/development/troubleshooting/#developing-on-macos-catalina-and-newer","text":"Due to some legacy 32-bit binaries ( sdf2table and implodePT ) that are required to build some of the Spoofax meta-languages, building on macOS Catalina or newer (Big Sur, Monterey) requires the following additional components to be installed: coreutils Docker for Mac Refer to the Requirements page for more information and installation instructions. The following subsections detail some problems that can occur when developing on macOS Catalina or newer.","title":"Developing on macOS Catalina and newer"},{"location":"howtos/development/troubleshooting/#realpath-command-not-found","text":"You should install coreutils . Refer to the Requirements page for installation instructions. Invoke the following command to test it (it should return the full path to the current directory): realpath -s .","title":"realpath: command not found"},{"location":"howtos/development/troubleshooting/#mounts-denied-the-path-is-not-shared-the-host-and-is-not-known-to-docker","text":"Error response from daemon: Mounts denied: The path /var/folders/a_/foo0000bar/T/vfs_cache-345/tmp_123_macosx/sdf2table-macosx is not shared from the host and is not known to Docker. Ensure the root of the path ( /var/folders in this example) is shared in Docker for Mac. Go to Docker for Mac Preferences \u2023 Resources \u2023 File Sharing and add the path to the list. The default list should contain: /Users /Volumes /private /tmp /var/folders","title":"Mounts denied: The path is not shared the host and is not known to Docker"},{"location":"howtos/development/troubleshooting/#invalid-mount-config-for-type-bind-bind-source-path-does-not-exist","text":"Error response from daemon: invalid mount config for type \"bind\": bind source path does not exist: /var/folders/a_/foo0000bar/T/vfs_cache-345/tmp_123_macosx Ensure that both Docker and the terminal from which you are invoking the build have full-disk access . Go to macOS Preferences \u2023 Security and Privacy \u2023 Full Disk Access and check the checkboxes next to Docker and your terminal ( Terminal and/or iTerm ).","title":"invalid mount config for type \"bind\": bind source path does not exist"},{"location":"howtos/editor-services/rename-refactoring/","text":"Add Rename Refactoring to an Existing Project \u00b6 Rename Refactoring is the ability for the user to select a reference or declaration and rename it to across the whole program while not introducing errors and not touching syntactically equal names. Renaming in Statix \u00b6 To enable the Rename Refactoring for an existing Spoofax Language project that uses Statix, create an action that calls the rename-action strategy from the statixruntime library. The parameters are explained in the reference . For example: module renaming imports statixruntime statix / runtime / renaming pp analysis rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , id ) Renaming in NaBL2 \u00b6 There also exists a version of the Rename refactoring that works with languages using NaBL2. It can be added with a Stratego module like this: module renaming imports nabl2 / runtime pp analysis rules rename-menu-action = nabl2-rename-action ( construct-textual-change , editor-analyze , id ) Menu Action \u00b6 The rename refactoring is triggered from an entry in the Spoofax menu. To add it to an existing project a menu like the following can be implemented in an ESV file : module Refactoring menus menu : \"Refactoring\" action : \"Rename\" = re name - menu - action See Also \u00b6 Reference: Rename Refactoring","title":"Add Rename Refactoring to an Existing Project"},{"location":"howtos/editor-services/rename-refactoring/#add-rename-refactoring-to-an-existing-project","text":"Rename Refactoring is the ability for the user to select a reference or declaration and rename it to across the whole program while not introducing errors and not touching syntactically equal names.","title":"Add Rename Refactoring to an Existing Project"},{"location":"howtos/editor-services/rename-refactoring/#renaming-in-statix","text":"To enable the Rename Refactoring for an existing Spoofax Language project that uses Statix, create an action that calls the rename-action strategy from the statixruntime library. The parameters are explained in the reference . For example: module renaming imports statixruntime statix / runtime / renaming pp analysis rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , id )","title":"Renaming in Statix"},{"location":"howtos/editor-services/rename-refactoring/#renaming-in-nabl2","text":"There also exists a version of the Rename refactoring that works with languages using NaBL2. It can be added with a Stratego module like this: module renaming imports nabl2 / runtime pp analysis rules rename-menu-action = nabl2-rename-action ( construct-textual-change , editor-analyze , id )","title":"Renaming in NaBL2"},{"location":"howtos/editor-services/rename-refactoring/#menu-action","text":"The rename refactoring is triggered from an entry in the Spoofax menu. To add it to an existing project a menu like the following can be implemented in an ESV file : module Refactoring menus menu : \"Refactoring\" action : \"Rename\" = re name - menu - action","title":"Menu Action"},{"location":"howtos/editor-services/rename-refactoring/#see-also","text":"Reference: Rename Refactoring","title":"See Also"},{"location":"howtos/installation/install-eclipse-bundle/","text":"Install the Eclipse with Spoofax Plugin Bundle \u00b6 Install an Eclipse instance with the latest stable release of the Spoofax plugin pre-installed for your platform: Eclipse with JRE (recommended) Eclipse bundle including the Spoofax plugin with embedded Java Runtime Environment (JRE) (recommended): + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Eclipse Eclipse bundle including the Spoofax plugin ( no embedded JRE ): macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Development releases . Troubleshooting \u00b6 macOS: \"Eclipse\" cannot be opened because the developer could not be verified \u00b6 macOS puts unverified binaries in 'quarantine' and disallows their execution. To remove the com.apple.quarantine attribute, do: xattr -rc Eclipse.app Eclipse does not start, or complains about missing Java \u00b6 Download the Eclipse bundle with embedded JRE . Otherwise, ensure you have a distribution of Java installed. Then in eclipse.ini , add a -vm line at the top of the file, followed by the path to the Java installation. For example, with SDKMan! on macOS: -vm /Users/myusername/.sdkman/candidates/java/current/jre/lib/jli/libjli.dylib","title":"Install the Eclipse with Spoofax Plugin Bundle"},{"location":"howtos/installation/install-eclipse-bundle/#install-the-eclipse-with-spoofax-plugin-bundle","text":"Install an Eclipse instance with the latest stable release of the Spoofax plugin pre-installed for your platform: Eclipse with JRE (recommended) Eclipse bundle including the Spoofax plugin with embedded Java Runtime Environment (JRE) (recommended): + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Eclipse Eclipse bundle including the Spoofax plugin ( no embedded JRE ): macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Development releases .","title":"Install the Eclipse with Spoofax Plugin Bundle"},{"location":"howtos/installation/install-eclipse-bundle/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"howtos/installation/install-eclipse-bundle/#macos-eclipse-cannot-be-opened-because-the-developer-could-not-be-verified","text":"macOS puts unverified binaries in 'quarantine' and disallows their execution. To remove the com.apple.quarantine attribute, do: xattr -rc Eclipse.app","title":" macOS: \"Eclipse\" cannot be opened because the developer could not be verified"},{"location":"howtos/installation/install-eclipse-bundle/#eclipse-does-not-start-or-complains-about-missing-java","text":"Download the Eclipse bundle with embedded JRE . Otherwise, ensure you have a distribution of Java installed. Then in eclipse.ini , add a -vm line at the top of the file, followed by the path to the Java installation. For example, with SDKMan! on macOS: -vm /Users/myusername/.sdkman/candidates/java/current/jre/lib/jli/libjli.dylib","title":"Eclipse does not start, or complains about missing Java"},{"location":"howtos/installation/install-eclipse-plugin-manually/","text":"Install the Spoofax Eclipse Plugin Manually \u00b6 Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer. In Eclipse, go to menu Help \u2192 Install New Software . In the Work with: text area, type: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ ( Development releases ). Uncheck Group items by category to make the plugin visible. Check Spoofax Eclipse meta-tooling , Spoofax Eclipse meta-tooling M2E integration and Spoofax Eclipse runtime . Click Install and go through the remaining steps. Restart Eclipse.","title":"Install the Spoofax Eclipse Plugin Manually"},{"location":"howtos/installation/install-eclipse-plugin-manually/#install-the-spoofax-eclipse-plugin-manually","text":"Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer. In Eclipse, go to menu Help \u2192 Install New Software . In the Work with: text area, type: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ ( Development releases ). Uncheck Group items by category to make the plugin visible. Check Spoofax Eclipse meta-tooling , Spoofax Eclipse meta-tooling M2E integration and Spoofax Eclipse runtime . Click Install and go through the remaining steps. Restart Eclipse.","title":"Install the Spoofax Eclipse Plugin Manually"},{"location":"howtos/installation/install-from-source/","text":"Install Spoofax from Source \u00b6 Perform a manual build and installation of cutting-edge Spoofax from source, by first cloning the Git repository: HTTPS git clone https://github.com/metaborg/spoofax-releng.git HTTPS git clone git@github.com:metaborg/spoofax-releng.git GitHub CLI gh repo clone metaborg/spoofax-releng Then: Using a terminal, navigate to the root of the spoofax-releng repository. (Optional.) Generate a new Maven ~/.m2/settings.xml with the Spoofax repository information. ./b gen-mvn-settings This will overwrite your existing ~/.m2/settings.xml file! Invoke the following command to build Spoofax and its submodules and meta-languages: ./b build all (Optional.) Generate a new Eclipse instance with the Spoofax plugin embedded into it: ./b gen-eclipse --destination Spoofax.app","title":"Install Spoofax from Source"},{"location":"howtos/installation/install-from-source/#install-spoofax-from-source","text":"Perform a manual build and installation of cutting-edge Spoofax from source, by first cloning the Git repository: HTTPS git clone https://github.com/metaborg/spoofax-releng.git HTTPS git clone git@github.com:metaborg/spoofax-releng.git GitHub CLI gh repo clone metaborg/spoofax-releng Then: Using a terminal, navigate to the root of the spoofax-releng repository. (Optional.) Generate a new Maven ~/.m2/settings.xml with the Spoofax repository information. ./b gen-mvn-settings This will overwrite your existing ~/.m2/settings.xml file! Invoke the following command to build Spoofax and its submodules and meta-languages: ./b build all (Optional.) Generate a new Eclipse instance with the Spoofax plugin embedded into it: ./b gen-eclipse --destination Spoofax.app","title":"Install Spoofax from Source"},{"location":"howtos/statix/debugging/","text":"Debugging \u00b6 This section describes several techniques that can be used to debug your Statix specification if it does not work as you expect. Important The single most useful thing you can do when debugging is to make the problem as small as possible! All the techniques you can use for debugging are more effective when the problem is as small as possible. Try to find the smallest example program and Statix specification that still exhibits your problem, and focus on those. There are three main categories of problems you may encounter: Errors reported in your Statix files. These may come from syntax errors, unresolved names for modules or predicates, type errors, or problems with illegal scope extension. When these errors are unexpected and not mentioned in Some Common Problems , follow the steps in Basic Checklist and Creating Minimal Examples . If that does not help out, please follow the steps in Getting Help and Reporting Issues . Unexpected behavior when running Statix on files of your language. This is the most common kind of problems. All debugging techniques after the Basic Checklist are focused on finding and debugging problems in the definitions in your specification. Note that it is useless to try to solve problems of this kind when your specification still has errors! Analysis fails altogether. This usually results in Analysis failed errors at the top of files or on the project in the Package Explorer . Check the Console and Error Log for reported Java exceptions that can be included in a bug report. Basic Checklist \u00b6 These are some basic things that should always be checked when you encounter a problem: See if the disappears after a clean build (run Project > Clean... and then Project > Build Project on your project). If the problem disappears after a clean build, but then consistently reappears after subsequent editing or building, it should be reported as a potential bug. Are there errors on any Statix files? The behavior of running Statix on your language files is undefined when the specification itself has errors. Check the Package Explorer as well as the Problems and Console views to make sure none of the Statix files have errors on them. Fix such errors before debugging any other issues! Check for errors in the Package Explorer and in open editors, as well as in the Console , Problems , and Error Log views. See whether the section on Some Common Problems or the remainder of the documentation answers your question already. Checking AST Traversal \u00b6 Ensure that your Statix rules are applied to the AST nodes. It is easy to forget to apply a predicate to a subterm, especially for bigger languages. If you are not sure if the rules are applied to a certain AST node, add a forced note (e.g. try { false } | note \"text\" ) to that AST node as follows: extendsOk ( s_lex , Extends ( m ), s_mod ) : - { s_mod' } try { false } | note \"extendsOK applied\" , resolveMod ( s_lex , m ) == s_mod' , s_mod -EXT-> s_mod' . Build your project and check if the note appears where you expect it. If it does not appear, find the places where those AST nodes may appear and ensure the predicate is applied. Checking Reference Resolution \u00b6 Setting ref attributes on references allows you to check reference resolution interactively in example programs of your language. The following rule shows how to do that using @x.ref := x' : resolveVar ( s , x ) = T : - { x' } query typeOfDecl filter P * and { x' : - x' == x } min $ < P and true in s |-> [ ( _ , ( x' , T )) ] , @ x . ref := x' . This requires that x and x' are both names from the AST. Now write some example programs and check if references resolve to the definitions you expect, by Ctrl + Click / Cmd + Right Click on the reference. Note that statix/References must be included in one of your ESV files for this to work. This is by default the case for generated projects that use Statix. Interpreting Error Messages \u00b6 Statix can be configured to contain a trace as part of the error messages for failing constraints, which can be a great help for debugging. Two parameters control message formatting. The first, message-trace-length controls whether a trace is included in the message, and how long it will be. A value of 0 means no trace (if no custom message is provided, the failing constraint itself is still shown), -1 means the full trace. The default is set to 0 , as showing traces is mostly helpful for debugging when writing the spec and it slows down message formatting. The second, message-term-depth controls the depth up to which terms in messages are formatted. A value of -1 means full terms. The default is set to 3 , which is usually enough to understand the terms involved, without choking up Eclipse or the console with large error messages. It is not recommended to set both settings to -1 , because then every message will contain the full AST. The configuration settings are part of the metaborg.yaml file of the project containing the language files (not the project containing the specification!), and look as follows: runtime : statix : message-trace-length : 5 # default: 0, full trace: -1 message-term-depth : 3 # -1 = max A typical error message including a trace may look as follows: [(?q.unit-wld61-10,(?q.unit-x'-11,?q.unit-T-5))] == [] > query filter ((Label(\"units/name-resolution/interface!EXT\"))* Label(\"units/name-resolution/default-impl!var\")) and { (?x',_) :- ?x' == \"q\" } min irrefl trans anti-sym { <edge>Label(\"units/name-resolution/default-impl!var\") < <edge>Label(\"units/name-resolution/interface!EXT\"); } and { _, _ :- true } in #p.unit-s_mod_4-4 |-> [(?q.unit-wld61-10,(?q.unit-x'-11,?q.unit-T-5))] > units/name-resolution/interface!resolveVar(#q.unit-s_mod_2-4, QDefRef(QModInModRef(ModRef(\"P\"),\"B\"),\"q\"),?q.unit-T-5) > units/statics!typeOfExpr(#q.unit-s_mod_2-4, VarRef(QDefRef(QModInModRef(ModRef(\u2026),\"B\"),\"q\")), ?q.unit-T-5) > units/statics!defOk(#q.unit-s_mod_2-4, VarDef(\"e\",VarRef(QDefRef(QModInModRef(\u2026,\u2026),\"q\"))), #q.unit-s_mod_2-4) > ... trace truncated ... As this looks daunting at first, we break it down. At the top is the constraint that failed; in this case an equality constraint. Below that are several lines prefixed with > that show where the constraint above it originated. We see that the equality originated from a query , which itself originated from one of the rules of resolveVar , which was applied in one of the rules of typeOfExpr etc. As these traces can get very long, they are truncated to five entries. Now we explain some more details of what we can see here: Errors may contain unification variables of the form ?FILENAME-VARNAME-NUM or ?VARNAME-NUM . These are instantiations of the meta-variables in the specification. The variable name VARNAME corresponds to the name of the meta-variable that was instantiated, and can be helpful in reasoning about the origin of a unification variable. When the name corresponds to a functional predicate name, it is a return value from that predicate. The file name is the file that was being checked when the unification variable was created. Due to Statix's operation, this can sometimes be the project root instead of the actual file. Scope values are shown as #FILENAME-VARNAME-NUM or #VARNAME-NUM . (Rarely they appear in the exploded form Scope(\"FILENAME\", \"VARNAME-NUM\") ). Predicate names are prefixed with the name of the module they are defined in. For example, defOk is defined in units/statics and therefore appears as units/statics!defOk in the trace. Note that the predicate name is prefixed with the Statix module that defines the predicate. (The rules for the predicate may be defined in other modules.) The trace shows which predicates were applied, and to which arguments. It does not show which predicate rule was chosen! This can often be deduced from the line above it in the trace, but if unsure, use a forced note (see Inspecting Variables ) to check your expectation. Error messages are fully instantiated with the final result. This means that variables that appear in error messages are free in the final result of this Statix execution. Therefore, we do not have to consider the order of execution or the moment when the error message was generated when interpreting error messages! Since Spoofax 2.5.17, error messages for unsolved constraints will additionally display information on why they are delayed, and which critical edges these constraints prevented to be closed. Examples of such messages are: (unsolved) statics/modules/imports!declareImportedValues(#p.unit-imps_42-8, ?p.unit-imp-49) delayed on vars {?p.unit-imp-49} preventing completion of { #p.unit-imps_42-8: {<edge>Label(\"statics/names/relations!value\"): 1} } and (unsolved) query filter ((Label(\"statics/main!EXT\"))* Label(\"statics/names/relations!value\")) and { (?x',_) :- ?x' == \"q\" } min irrefl trans anti-sym { <edge>Label(\"statics/names/relations!value\") < <edge>Label(\"statics/main!EXT\"); } and { _, _ :- true } in #p.unit-imps_42-8 |-> [(?q.unit-wld61-10,(?q.unit-x'-11,?q.unit-T-5))] delayed on critial edges { #p.unit-imps_42-8-<edge>Label(\"statics/names/relations!value\") } In this first message, the delayed on vars {?p.unit-imp-49} fragment indicates that progress on this constraint could be made when ?p.unit-imp-49 would be refined to a particular value. Because the unfolding of this constraint migth create declarations for the value relation in #p.unit-imps_42-8 , the tail of the message indicates that edge cannot be closed, and hence queries over such an edge cannot proceed. The second message is delayed precisely because that edge cannot be closed. This is indicated by the delayed on critial edges { #p.unit-imps_42-8-<edge>Label(\"statics/names/relations!value\") } part of the second message. In such cases, the way to proceed is trying to rewrite the specification in such a way the constraint that prevents the critical edge from being closed can make progress. When it turnes out that this constraint is dependent on the result of a query that is delayed on that edge, there is a 'cyclic dependency' on these constraint. Statix cannot handle those, and therefore a redesign of the name binding model is required. If there is a constraint that is delayed on a critical edge, but no constraint preventing that edge from being closed is displayed, this should be reported as an error. Tip The section on Common Problems contains tips on how to deal with many error messages. Inspecting Variables \u00b6 Inspecting the values assigned to meta-variables can be very helpful to debug a specification. Variables cannot be automatically inspected, but we can show their values by forcing a note in the rule where the variable appears. The following rule shows how to do this for the intermediate type T of the assigned variable: stmtOk ( s , Assign ( x , e )) : - { T U } T == resolveVar ( s , x ), try { false } | note $[ assignee has type [ T ]], U == typeOfExp ( s , e ), subtype ( U , T ). Inspecting the Scope Graph \u00b6 Inspecting the scope graph that is constructed by Statix can be very helpful in debugging problems with scoping and name resolution queries. After type checking, view the scope graph of a file using the Spoofax > Statix > Show scope graph menu. Note that in multi-file mode, the scope graph is always the graph of the whole project. Therefore, creating a small example project with only a few files can be very helpful (see also Creating Minimal Examples ). Here is an example of such a scope graph: scope graph #q.unit-s_mod_2-4 { relations { units/name-resolution/default-impl!var : (\"e\", UNIT()) } edges { units/name-resolution/interface!LEX : #s_1-1 } } #p.unit-s_mod_4-4 { relations { units/name-resolution/default-impl!var : (\"b\", UNIT()) } edges { units/name-resolution/interface!LEX : #p.unit-s_mod_2-6 } } #p.unit-s_mod_2-6 { relations { units/name-resolution/default-impl!mod : (\"B\", #p.unit-s_mod_4-4) } edges { units/name-resolution/interface!LEX : #s_1-1 } } #s_1-1 { relations { units/name-resolution/default-impl!mod : (\"E\", #q.unit-s_mod_2-4) (\"P\", #p.unit-s_mod_2-6) } } The scope graph is presented as a list of scopes, with the relation entries and outgoing edges from that scope. Remember that the names of the scopes match the names of the meta-variables in the specification! For example, #p.unit-s_mod_4-4 originated from a meta-variable s_mod . Paying attention to this is very helpful in figuring out the structure of the graph. Some useful questions you can ask yourself when inspecting the scope graph for debugging: Does the graph have the structure I expect from the current example program? Are all the scopes that I expect there, and are all the scopes that are there expected? Do all scopes have the expected relations in them? Do the have the expected outgoing edges? When you are debugging a certain query, consider the scope in which the query starts, and execute the query in the given graph. Are the necessary edges present? Does the regular expression allow those edges to be traversed? Are you querying the correct relation, and is the filter predicate correct for the data you want to match? When considering these questions, it can be helpful to use the ideas from Inspecting Variables to verify the scope a query is executed in, or to show the scope that is created for a definition, and match those with what you see in the scope graph. Testing Predicates \u00b6 Sometimes creating a minimal example program in your language is not enough to fix a problem. In such cases writing Statix tests is a great way to test your definitions in even more detail. In a Statix test you can specify a constraint and evaluate it to see how it behaves. For example, if you suspect a bug in the definition of the subtype predicate, you could test it as follows: // file: debug.stxtest resolve { T } T == typeOfExp ( Int ( \"42\" )), subtype ( T , LONG ()) imports statics The .stxtest file starts with resolve and a constraint, which can be anything that can appear in a rule body. After that, the test may specify imports , signature and rules sections like a regular Statix module. A test is executed using the Spoofax > Evaluate > Evaluate Test menu. Evaluation outputs a .stxresult file, which looks as follows: substitution T |-> INT() analysis scope graph errors * INT() == LONG() > statics!subtype(INT(), LONG()) > ... trace truncated ... warnings notes The test result shows the value of top-level variables from the resolve block (in this case T ), the scope graph that was constructed (in this case empty), and any messages that were generated (in this case one error). These tests are a great way to verify that the predicate definitions work as you expect. Apply your predicates to different arguments to check their behavior. Even more complicated mechanisms such as queries can be debugged this way. Simply construct a scope graph in the resolve block (using new , edges, and declarations), and execute your querying predicate on the scopes you have created. As a starting point, you can take the AST of your example program (using the Spoofax > Syntax > Show parse AST menu), and use that as an argument to your top-level predicate. Creating a self-contained Statix test is a good way to isolate a problem. Instead of importing all your definitions, copy the relevant definitions to the test (in a rules section), and try to create the smallest set of rules and predicate arguments that still exhibit the problem you are debugging. A self-contained test is also very helpful when asking others for help, as it is much easier to review and run than having to setup and build a complete language project. Creating Minimal Examples \u00b6 Creating a minimal example is one of the most useful things you can do when debugging. It helps you to get to the core of the problem, but it also benefits all of the other techniques we have discussed so far. Having a smaller example makes it easier to inspect the scope graph, makes it easier to inspect variables as there are fewer, and reduced the number of error messages to review. An example is a file, or a set of files, in your language, where Statix does not behave as you expect. A minimal example is usually created by starting from a big example that exhibits the problem. Try to eliminate files and simplify the example program while keeping the unexpected behavior. The smaller the program and the fewer rules in your specification are used for this program, the easier it is to debug. Creating Self-Containted Tests from Examples \u00b6 While a small example in an example language is already helpful to debug Statix issues, a self-contained Statix test file is even more useful. The recommended approach to create a self-contained Statix test from an example is as follows: Create a test with all ASTs from the example Manually unfold user-defined constraints in the test. Copy imported definitions in the test file. These steps will now be discussed in more detail below First, create a Statix test file that mimics the example literally. Assuming that AST ? are placeholders for real ASTs (which can be obtained using the Spoofax > Analysis > Show Pre-Analysis AST menu option), such test files roughly look as follows: resolve { s } new s , projectOk ( s ), fileOk ( s , < AST 1 > ), fileOk ( s , < AST 2 > ), ... fileOk ( s , < AST N > ) imports mylang / base Before going on, first check whether this test still exhibits the issue at hand. Second, simplify this test by manually reducing its size. This step consists of repeated application of the following substeps: Discard irrelevant constraints Inline constraints Manually apply rules Constraints that do not have to do with the issue can sometimes be removed completely. A common example is the projectOk constraint, but also declaration name uniqueness checks and such can often just be omitted. In the second substep we try to reduce the number of constraint by simplification. Examples of simplification include inlining unification with variables (e.g. replacing { T } T == INT (), subtype ( T , T ) with subtype ( INT (), INT ()) ), or solving deterministic queries (e.g. replacing ! var [ \"x\" , INT () ] in s , query var ... in s |-> [ ( _ , ( _ , T )) ] with T == INT () ). The third substep entails choosing the correct rule for a user-defined constraint, and replacing the user-defined constraint with the properly applied body. For example, when the specification constains a rule fileOk ( s , File ( imp , decls )) : - importsOk ( s , imp ), declsOk ( decls ) , the fileOk ( s , < AST 1 > ) constraint in the test above can be replaced with importsOk ( s , < IMPs 1 > ), declsOk ( s , < DECLS 1 > ) . Be aware to properly introduce new names for variables from the rule head. Additionally, it is required to update the imported modules when new user-defined constraints are introduced by application. Warning At each step, execute your test to verify whether it still exposes the issue. When it does not, you have either discarded a relevant constraint, or made a mistake when simplifying. Try to apply these steps exhaustively, until only built-in constraints remain. Now that we have a minimal test case, the third step is to make it self-contained by removing imports. This just involved copying the relevant constraint declarations and rules in a rules section in the test, and removing the import. Some Common Problems \u00b6 1. Predicates fail with amb(...) terms as arguments. These terms indicate parsing ambiguities, which should be fixed in the grammar (SDF3) files. 2. Errors in your specification appear at incorrect places (e.g. sort or constructor declarations). In such cases, the declaration is referenced from an invalid position anywhere in your specification, but due to the non-deterministic order of constraint solving the error appears at the observed position. The best approach to solve these issues is to comment away all usages, until the error disappears. Then, in the last commented position, the declaration is used incorrectly. 3. One or both of the fileOk(...) or projectOk(...) predicates fail immediately, for example with the error messages: statics!fileOk(#s_1-1,Test([Prog(\"A.mod\",Decls(\u2026)),Prog(\"B.mod\",Decls(\u2026)),Prog(\"C.mod\",Decls(\u2026))])) (no origin information) statics!projectOk(#s_1-1) (no origin information) In such cases, you have probably renamed the top-level file, or moved the declarations of these predicates to another file that is imported. Assuming the predicates are now defined in the module statics/mylang as follows: // file: trans/statics/mylang.stx module statics/mylang imports statics/mylang/program rules projectOk : scope projectOk(s). fileOk : scope * Start fileOk(s, p) :- programOk(s, p). If this module is the top-level module of your specification, then you have to change the call to stx-editor-analyze in trans/analysis.str such that the first term argument (which specifies the module to use, by default \"statics\" ) is the new module name (in this case statics/mylang ). On the other hand, if you kept statics as the top-level module and have it import the module statics/mylang , then you have to change the call to stx-editor-analyze in trans/analysis.str such that the second and third term argument (which specify the predicates to apply to projects and files, respectively) are qualified by the module name (in this case \"statics/mylang!projectOk\" and \"\"statics/mylang!fileOk , respectively). 4. Files of your language are only analyzed by Statix after they are opened in an editor in Eclipse. There are several reasons why this may be hapening: The project containing the files is not a Spoofax project. A spoofax project must contain a metaborg.yaml . If it is a Maven project, the packaging must be one of spoofax-{language,test,project} . The project containing the files does not have a dependency on your language. Spoofax only analyzes files of your language if the metaborg.yaml configuration contains a compile dependency on the language definition. This should look similar to the following: dependencies : compile : - org.example:your-language:1.0-SNAPSHOT The language is missing. If a language dependency is missing, this is reported with errors on the console. Make sure your language definition project is open in Eclipse and that is is successfully built. Eclipse is not configured to automatically build files. This can be enabled by selecting Project > Build automatically from the Eclipse menu. The project in Eclipse did not get the Spoofax nature. Imported Maven projects with one of the spoofax-* packagings normally get the Spoofax nature automatically, but sometimes this doesn't work correctly. Non-Maven projects always have to be assigned the Spoofax nature manually. This can be done with Spoofax > Add Spoofax nature in the context-menu of the project containing the files. 5. A lot of errors are reported. It happens that a single problem in the type checked program leads to the failure of other constraints (cascading errors). For example, an unresolved name might lead to errors about subtype checks that cannot be solved, import edges that cannot be created, etc. Here are some tips to help you find the root cause of the probem: Differentiate between failed and unsolved constraints. The cause of a problem is usually found best by looking at the failed constraints. For example, an unresolved name might result in an error on the equality constraint between the expected and actual query result. Errors on unsolved constraints are marked as Unsolved . Unsolved errors are often the result of uninstantiated logical variables. Predicates remain unsolved if the uninstantiated variable prevents the selection of an applicable rule for the predicate. For example, an unsolved error subtype(INT(), ?T-1) is caused by the free variable ?T-1 which prevents selecting the appropriate rule of the subtype predicate. Queries remain unsolved if the query scope is not instantiated, or if variables used in the matching predicate (such as the name to resolve) remained free. For example, an unsolved error query filter (e Label(\"typeOfDecl\")) and { (?x',_) :- ?x' == ?x-5 } min irrefl trans anti-sym { <edge>Label(\"typeOfDecl\") < <edge>Label(\"P\"); } and { _, _ :- true } in ?s-3 |-> [(?wld0-1,(?x'-2,?T-4))] cannot be resolved because the scope variable ?s-3 is free, and the free variable ?x-5 would prevent matching declarations. Use of the variables ?x'-2 and ?T-4 might cause more unsolved constraints, since these also remain free when the query cannot be solved. Edge and declaration assertions remain unsolved if the scopes are not instantiated. For example, the edge assertion #s_2-1 -Label(\"P\")-> ?s'-5 cannot be solved because the variable for the target scope ?s'-5 is not instantiated. Unsolved edge constraints in particular can lead to lots of cascading errors, as they block all queries going through the source scope of the edge. If it is not immediately clear which error is the root of a problem, it helps to figure out the free variable dependencies between reported errors. Consider the following small example of three reported errors: subtype(?T-5, LONG) #s_3-1 -Label(\"P\")-> ?s'-6 query filter ((Label(\"P\"))* Label(\"typeOfDecl\")) and { (?x',_) :- ?x' == \"v\" } min irrefl trans anti-sym { <edge>Label(\"typeOfDecl\") < <edge>Label(\"P\"); } and { _, _ :- true } in #s_3-1 |-> [(?wld4-1,(?x'-2,?T-5))] For each of these we can see which variables are necessary for the constraint to be solved, and which they might instantiate when solved. The subtype predicate is blocked on the variable ?T-5 . The edge assertion is blocked on the scope variable ?s'-6 . The query does not seem blocked on a variable (both the scope and the filter predicate are instantiated), but would instantiate the variables ?x'-2 and ?T-5 when solved. We can conclude that the subtype constraint depends on solving the query, so we focus our attention on the query. Now we realize that we query in the scope of the unsolved edge assertion. So, the query depends on the edge assertion, and our task is to figure out why the scope variable in the edge target is not instantiated. Info Since Spoofax 2.5.17, many cascading errors should not be displayed anymore. If you encounter a message that is clearly a cascading error from another constraint, and not part of a cycle of unsolved constraints, consider reporting this as a bug. Also, the delays and unclosed critical edges should be displayed explicitly on the message of unsolved constraints. This helps figuring out dependencies between failed constraints. 6. Constraint solving does not terminate. This is caused by infinite recursion in a user-defined constraint. Three common causes of this problem are. First, the specification contains recursive predicates, such as rule(x, T) :- /* ... */, rule(x, T), /* ... */. . As can be trivially seen, the solver will simplify rule(x, T) infinitely many times. Second, the specification contains a rule that creates a declaration, queries it, and then calls itself on the query result. If the result contains the declaration made by the rule, it will instantiate itself infinitely. An example of such a specification is: declareTVar ( s , x , T ) : - { Tvs } ! tvar [ x , T ] in s , query tvar /* */ in s |-> Tvs , declareTVars ( s , Tvs ). Often, this pattern is not easily observable, since the recursion may be indirect, and the declarations, queries and recursive calls are specified in different rules. Info This pattern often occurs in incorrect specifications for parametric types with lazy substitution. Third, for a user-defined constraint on a existential variable, optimistic rule selection can cause infinite generation and refinement of new, unconstrained variables. Consider for example the following specification: rules ruleWithoutBaseCase : list ( int ) ruleWithoutBaseCase ( [ x | xs ] ) : - ruleWithoutBaseCase ( xs ). Although the recursive call seems to be on a strictly smaller term (namely, the tail of the list), infinite recursion can still happen when this rule is instantiated with a free variable, such as in this constraint: { x } ruleWithoutBaseCase ( x ), x == [] . While this constraint should fail, it can be that the solver decides to simplify ruleWithoutBaseCase(x) first. Due to optimistic rule selection, it will refine x to [x1 | xs] , and simplify to ruleWithoutBaseCase(xs) . When later the constraint x == [] is solved, it will simply fail. But since xs is free, the sequence of simplifying ruleWithoutBaseCase on a free variable repeats indefinitely. The Statix normalization often introduces new existential variables. Therefore it might not be completely obvious that a specification is susceptible to this behavior. Consider for example the following specification: rules ruleWithoutBaseCase : list ( int ) ruleWithoutBaseCase ( [ x | xs ] ) : - ruleWithoutBaseCase ( xs ). nil : -> list ( int ) nil () = [] . test : test () : - ruleWithoutBaseCase ( nil ()). Although this specification does not seem to have existential variables, its normalized equivalent (see below), actually does. rules ruleWithoutBaseCase : list ( int ) ruleWithoutBaseCase ( [ x | xs ] ) : - ruleWithoutBaseCase ( xs ). nil : list ( int ) nil ( [] ). test : test () : - { nil1 } ruleWithoutBaseCase ( nil1 ), nil ( nil1 ). Now, it can be seen that the normalized test rule is again susceptile to this type of infinite recursion. In order to debug non-terminating specifications, first add base cases like rule(_, _, ..) :- false for all user-defined constraints that do not yet have those. This prevents recursion by optimistic rule selection. Potential errors that pop up now demonstrate which rule was incorrectly selected optimistically. If that does not work out, the other techniques in this section should be applied to isolate the recursion. Getting Help and Reporting Issues \u00b6 If the techniques above did not help to solve your problem, you can ask us for help or report the issue you found. To make this process as smooth as possible, we ask you to follow the following template when asking a Statix related question: Single sentence description of the issue. Spoofax version. See About Eclipse ; Installation Details ; Features , and search for Spoofax . Statix configuration: single-file or multi-file mode. Multi-file mode is enabled when the observer setting in in your ESV looks like observer: editor-analyze (constraint) (multifile) . Steps to reproduce. Best is to include a small, self-contained test (see Testing Predicates above) so that others can easily run the test and reproduce the issue! If that is not possible, provide a (link to) a project, including an example file, that shows the problem. Keep the project and the example as small as possible, and be specific about the relevant parts of your program and of your specification. Description of the observed behavior. Also mention if the problem occurs consistently, or only sometimes? If only sometimes, does it occur always/never after a clean build, or does it occur always/never after editing and/or building without cleaning? Description of the expected behavior. Extra information that you think is relevant to the problem. For example, things you have tried already, pointers to the part of the rules you think are relevant to the problem etc. If you tried other examples that show some light on the issue, this is a good place to put those. Again, it is best if these also come as self-contained tests! An example bug report described using the format above: Issue: Spoofax version: 2.6.0.20210208-173259-master Statix setup: multi-file Steps to reproduce: Execute the test in `example1.stxtest`. Observed behavior: Sometimes an error is reported that the `query` failed. The problem does not occur consistently. On some runs, the error appears, but not on others. This does not seem related to cleaning or building the project. Expected behavior: The test is executed and no errors are reported. Scope `s1` is reachable from `s2`, so the query return a single result, and `ps != []` should therefore hold. Extra information: The test in `example2.stxtest` is very similar. The only difference is that the predicate `nonempty` has an extra rule for the empty list. The predicate is semantically the same, as the extra rule fails, just as the general rule would do on the empty list. However, this example never gives the unexpected error. The bug report is accompanied by two self-contained tests. One illustrates the problem, while the other shows a very similar variant that does not exhibit the problem. // example1.stxtest resolve { s1 s2 } new s1 , new s2 , s2 -I-> s1 , reachable ( s1 , s2 ) signature name-resolution labels I rules reachable : scope * scope reachable ( s1 , s2 ) : - { ps } query () filter I * and { s1' : - s1' == s1 } min and true in s2 |-> ps , nonempty ( ps ). nonempty : list (( path * scope )) nonempty ( ps ) : - ps != [] . // example2.stxtest resolve { s1 s2 } new s1 , new s2 , s2 -I-> s1 , reachable ( s1 , s2 ) signature name-resolution labels I rules reachable : scope * scope reachable ( s1 , s2 ) : - { ps } query () filter I * and { s1' : - s1' == s1 } min and true in s2 |-> ps , nonempty ( ps ). nonempty : list (( path * scope )) nonempty ( ps ) : - ps != [] . nonempty ( [] ) : - false .","title":"Debugging"},{"location":"howtos/statix/debugging/#debugging","text":"This section describes several techniques that can be used to debug your Statix specification if it does not work as you expect. Important The single most useful thing you can do when debugging is to make the problem as small as possible! All the techniques you can use for debugging are more effective when the problem is as small as possible. Try to find the smallest example program and Statix specification that still exhibits your problem, and focus on those. There are three main categories of problems you may encounter: Errors reported in your Statix files. These may come from syntax errors, unresolved names for modules or predicates, type errors, or problems with illegal scope extension. When these errors are unexpected and not mentioned in Some Common Problems , follow the steps in Basic Checklist and Creating Minimal Examples . If that does not help out, please follow the steps in Getting Help and Reporting Issues . Unexpected behavior when running Statix on files of your language. This is the most common kind of problems. All debugging techniques after the Basic Checklist are focused on finding and debugging problems in the definitions in your specification. Note that it is useless to try to solve problems of this kind when your specification still has errors! Analysis fails altogether. This usually results in Analysis failed errors at the top of files or on the project in the Package Explorer . Check the Console and Error Log for reported Java exceptions that can be included in a bug report.","title":"Debugging"},{"location":"howtos/statix/debugging/#basic-checklist","text":"These are some basic things that should always be checked when you encounter a problem: See if the disappears after a clean build (run Project > Clean... and then Project > Build Project on your project). If the problem disappears after a clean build, but then consistently reappears after subsequent editing or building, it should be reported as a potential bug. Are there errors on any Statix files? The behavior of running Statix on your language files is undefined when the specification itself has errors. Check the Package Explorer as well as the Problems and Console views to make sure none of the Statix files have errors on them. Fix such errors before debugging any other issues! Check for errors in the Package Explorer and in open editors, as well as in the Console , Problems , and Error Log views. See whether the section on Some Common Problems or the remainder of the documentation answers your question already.","title":"Basic Checklist"},{"location":"howtos/statix/debugging/#checking-ast-traversal","text":"Ensure that your Statix rules are applied to the AST nodes. It is easy to forget to apply a predicate to a subterm, especially for bigger languages. If you are not sure if the rules are applied to a certain AST node, add a forced note (e.g. try { false } | note \"text\" ) to that AST node as follows: extendsOk ( s_lex , Extends ( m ), s_mod ) : - { s_mod' } try { false } | note \"extendsOK applied\" , resolveMod ( s_lex , m ) == s_mod' , s_mod -EXT-> s_mod' . Build your project and check if the note appears where you expect it. If it does not appear, find the places where those AST nodes may appear and ensure the predicate is applied.","title":"Checking AST Traversal"},{"location":"howtos/statix/debugging/#checking-reference-resolution","text":"Setting ref attributes on references allows you to check reference resolution interactively in example programs of your language. The following rule shows how to do that using @x.ref := x' : resolveVar ( s , x ) = T : - { x' } query typeOfDecl filter P * and { x' : - x' == x } min $ < P and true in s |-> [ ( _ , ( x' , T )) ] , @ x . ref := x' . This requires that x and x' are both names from the AST. Now write some example programs and check if references resolve to the definitions you expect, by Ctrl + Click / Cmd + Right Click on the reference. Note that statix/References must be included in one of your ESV files for this to work. This is by default the case for generated projects that use Statix.","title":"Checking Reference Resolution"},{"location":"howtos/statix/debugging/#interpreting-error-messages","text":"Statix can be configured to contain a trace as part of the error messages for failing constraints, which can be a great help for debugging. Two parameters control message formatting. The first, message-trace-length controls whether a trace is included in the message, and how long it will be. A value of 0 means no trace (if no custom message is provided, the failing constraint itself is still shown), -1 means the full trace. The default is set to 0 , as showing traces is mostly helpful for debugging when writing the spec and it slows down message formatting. The second, message-term-depth controls the depth up to which terms in messages are formatted. A value of -1 means full terms. The default is set to 3 , which is usually enough to understand the terms involved, without choking up Eclipse or the console with large error messages. It is not recommended to set both settings to -1 , because then every message will contain the full AST. The configuration settings are part of the metaborg.yaml file of the project containing the language files (not the project containing the specification!), and look as follows: runtime : statix : message-trace-length : 5 # default: 0, full trace: -1 message-term-depth : 3 # -1 = max A typical error message including a trace may look as follows: [(?q.unit-wld61-10,(?q.unit-x'-11,?q.unit-T-5))] == [] > query filter ((Label(\"units/name-resolution/interface!EXT\"))* Label(\"units/name-resolution/default-impl!var\")) and { (?x',_) :- ?x' == \"q\" } min irrefl trans anti-sym { <edge>Label(\"units/name-resolution/default-impl!var\") < <edge>Label(\"units/name-resolution/interface!EXT\"); } and { _, _ :- true } in #p.unit-s_mod_4-4 |-> [(?q.unit-wld61-10,(?q.unit-x'-11,?q.unit-T-5))] > units/name-resolution/interface!resolveVar(#q.unit-s_mod_2-4, QDefRef(QModInModRef(ModRef(\"P\"),\"B\"),\"q\"),?q.unit-T-5) > units/statics!typeOfExpr(#q.unit-s_mod_2-4, VarRef(QDefRef(QModInModRef(ModRef(\u2026),\"B\"),\"q\")), ?q.unit-T-5) > units/statics!defOk(#q.unit-s_mod_2-4, VarDef(\"e\",VarRef(QDefRef(QModInModRef(\u2026,\u2026),\"q\"))), #q.unit-s_mod_2-4) > ... trace truncated ... As this looks daunting at first, we break it down. At the top is the constraint that failed; in this case an equality constraint. Below that are several lines prefixed with > that show where the constraint above it originated. We see that the equality originated from a query , which itself originated from one of the rules of resolveVar , which was applied in one of the rules of typeOfExpr etc. As these traces can get very long, they are truncated to five entries. Now we explain some more details of what we can see here: Errors may contain unification variables of the form ?FILENAME-VARNAME-NUM or ?VARNAME-NUM . These are instantiations of the meta-variables in the specification. The variable name VARNAME corresponds to the name of the meta-variable that was instantiated, and can be helpful in reasoning about the origin of a unification variable. When the name corresponds to a functional predicate name, it is a return value from that predicate. The file name is the file that was being checked when the unification variable was created. Due to Statix's operation, this can sometimes be the project root instead of the actual file. Scope values are shown as #FILENAME-VARNAME-NUM or #VARNAME-NUM . (Rarely they appear in the exploded form Scope(\"FILENAME\", \"VARNAME-NUM\") ). Predicate names are prefixed with the name of the module they are defined in. For example, defOk is defined in units/statics and therefore appears as units/statics!defOk in the trace. Note that the predicate name is prefixed with the Statix module that defines the predicate. (The rules for the predicate may be defined in other modules.) The trace shows which predicates were applied, and to which arguments. It does not show which predicate rule was chosen! This can often be deduced from the line above it in the trace, but if unsure, use a forced note (see Inspecting Variables ) to check your expectation. Error messages are fully instantiated with the final result. This means that variables that appear in error messages are free in the final result of this Statix execution. Therefore, we do not have to consider the order of execution or the moment when the error message was generated when interpreting error messages! Since Spoofax 2.5.17, error messages for unsolved constraints will additionally display information on why they are delayed, and which critical edges these constraints prevented to be closed. Examples of such messages are: (unsolved) statics/modules/imports!declareImportedValues(#p.unit-imps_42-8, ?p.unit-imp-49) delayed on vars {?p.unit-imp-49} preventing completion of { #p.unit-imps_42-8: {<edge>Label(\"statics/names/relations!value\"): 1} } and (unsolved) query filter ((Label(\"statics/main!EXT\"))* Label(\"statics/names/relations!value\")) and { (?x',_) :- ?x' == \"q\" } min irrefl trans anti-sym { <edge>Label(\"statics/names/relations!value\") < <edge>Label(\"statics/main!EXT\"); } and { _, _ :- true } in #p.unit-imps_42-8 |-> [(?q.unit-wld61-10,(?q.unit-x'-11,?q.unit-T-5))] delayed on critial edges { #p.unit-imps_42-8-<edge>Label(\"statics/names/relations!value\") } In this first message, the delayed on vars {?p.unit-imp-49} fragment indicates that progress on this constraint could be made when ?p.unit-imp-49 would be refined to a particular value. Because the unfolding of this constraint migth create declarations for the value relation in #p.unit-imps_42-8 , the tail of the message indicates that edge cannot be closed, and hence queries over such an edge cannot proceed. The second message is delayed precisely because that edge cannot be closed. This is indicated by the delayed on critial edges { #p.unit-imps_42-8-<edge>Label(\"statics/names/relations!value\") } part of the second message. In such cases, the way to proceed is trying to rewrite the specification in such a way the constraint that prevents the critical edge from being closed can make progress. When it turnes out that this constraint is dependent on the result of a query that is delayed on that edge, there is a 'cyclic dependency' on these constraint. Statix cannot handle those, and therefore a redesign of the name binding model is required. If there is a constraint that is delayed on a critical edge, but no constraint preventing that edge from being closed is displayed, this should be reported as an error. Tip The section on Common Problems contains tips on how to deal with many error messages.","title":"Interpreting Error Messages"},{"location":"howtos/statix/debugging/#inspecting-variables","text":"Inspecting the values assigned to meta-variables can be very helpful to debug a specification. Variables cannot be automatically inspected, but we can show their values by forcing a note in the rule where the variable appears. The following rule shows how to do this for the intermediate type T of the assigned variable: stmtOk ( s , Assign ( x , e )) : - { T U } T == resolveVar ( s , x ), try { false } | note $[ assignee has type [ T ]], U == typeOfExp ( s , e ), subtype ( U , T ).","title":"Inspecting Variables"},{"location":"howtos/statix/debugging/#inspecting-the-scope-graph","text":"Inspecting the scope graph that is constructed by Statix can be very helpful in debugging problems with scoping and name resolution queries. After type checking, view the scope graph of a file using the Spoofax > Statix > Show scope graph menu. Note that in multi-file mode, the scope graph is always the graph of the whole project. Therefore, creating a small example project with only a few files can be very helpful (see also Creating Minimal Examples ). Here is an example of such a scope graph: scope graph #q.unit-s_mod_2-4 { relations { units/name-resolution/default-impl!var : (\"e\", UNIT()) } edges { units/name-resolution/interface!LEX : #s_1-1 } } #p.unit-s_mod_4-4 { relations { units/name-resolution/default-impl!var : (\"b\", UNIT()) } edges { units/name-resolution/interface!LEX : #p.unit-s_mod_2-6 } } #p.unit-s_mod_2-6 { relations { units/name-resolution/default-impl!mod : (\"B\", #p.unit-s_mod_4-4) } edges { units/name-resolution/interface!LEX : #s_1-1 } } #s_1-1 { relations { units/name-resolution/default-impl!mod : (\"E\", #q.unit-s_mod_2-4) (\"P\", #p.unit-s_mod_2-6) } } The scope graph is presented as a list of scopes, with the relation entries and outgoing edges from that scope. Remember that the names of the scopes match the names of the meta-variables in the specification! For example, #p.unit-s_mod_4-4 originated from a meta-variable s_mod . Paying attention to this is very helpful in figuring out the structure of the graph. Some useful questions you can ask yourself when inspecting the scope graph for debugging: Does the graph have the structure I expect from the current example program? Are all the scopes that I expect there, and are all the scopes that are there expected? Do all scopes have the expected relations in them? Do the have the expected outgoing edges? When you are debugging a certain query, consider the scope in which the query starts, and execute the query in the given graph. Are the necessary edges present? Does the regular expression allow those edges to be traversed? Are you querying the correct relation, and is the filter predicate correct for the data you want to match? When considering these questions, it can be helpful to use the ideas from Inspecting Variables to verify the scope a query is executed in, or to show the scope that is created for a definition, and match those with what you see in the scope graph.","title":"Inspecting the Scope Graph"},{"location":"howtos/statix/debugging/#testing-predicates","text":"Sometimes creating a minimal example program in your language is not enough to fix a problem. In such cases writing Statix tests is a great way to test your definitions in even more detail. In a Statix test you can specify a constraint and evaluate it to see how it behaves. For example, if you suspect a bug in the definition of the subtype predicate, you could test it as follows: // file: debug.stxtest resolve { T } T == typeOfExp ( Int ( \"42\" )), subtype ( T , LONG ()) imports statics The .stxtest file starts with resolve and a constraint, which can be anything that can appear in a rule body. After that, the test may specify imports , signature and rules sections like a regular Statix module. A test is executed using the Spoofax > Evaluate > Evaluate Test menu. Evaluation outputs a .stxresult file, which looks as follows: substitution T |-> INT() analysis scope graph errors * INT() == LONG() > statics!subtype(INT(), LONG()) > ... trace truncated ... warnings notes The test result shows the value of top-level variables from the resolve block (in this case T ), the scope graph that was constructed (in this case empty), and any messages that were generated (in this case one error). These tests are a great way to verify that the predicate definitions work as you expect. Apply your predicates to different arguments to check their behavior. Even more complicated mechanisms such as queries can be debugged this way. Simply construct a scope graph in the resolve block (using new , edges, and declarations), and execute your querying predicate on the scopes you have created. As a starting point, you can take the AST of your example program (using the Spoofax > Syntax > Show parse AST menu), and use that as an argument to your top-level predicate. Creating a self-contained Statix test is a good way to isolate a problem. Instead of importing all your definitions, copy the relevant definitions to the test (in a rules section), and try to create the smallest set of rules and predicate arguments that still exhibit the problem you are debugging. A self-contained test is also very helpful when asking others for help, as it is much easier to review and run than having to setup and build a complete language project.","title":"Testing Predicates"},{"location":"howtos/statix/debugging/#creating-minimal-examples","text":"Creating a minimal example is one of the most useful things you can do when debugging. It helps you to get to the core of the problem, but it also benefits all of the other techniques we have discussed so far. Having a smaller example makes it easier to inspect the scope graph, makes it easier to inspect variables as there are fewer, and reduced the number of error messages to review. An example is a file, or a set of files, in your language, where Statix does not behave as you expect. A minimal example is usually created by starting from a big example that exhibits the problem. Try to eliminate files and simplify the example program while keeping the unexpected behavior. The smaller the program and the fewer rules in your specification are used for this program, the easier it is to debug.","title":"Creating Minimal Examples"},{"location":"howtos/statix/debugging/#creating-self-containted-tests-from-examples","text":"While a small example in an example language is already helpful to debug Statix issues, a self-contained Statix test file is even more useful. The recommended approach to create a self-contained Statix test from an example is as follows: Create a test with all ASTs from the example Manually unfold user-defined constraints in the test. Copy imported definitions in the test file. These steps will now be discussed in more detail below First, create a Statix test file that mimics the example literally. Assuming that AST ? are placeholders for real ASTs (which can be obtained using the Spoofax > Analysis > Show Pre-Analysis AST menu option), such test files roughly look as follows: resolve { s } new s , projectOk ( s ), fileOk ( s , < AST 1 > ), fileOk ( s , < AST 2 > ), ... fileOk ( s , < AST N > ) imports mylang / base Before going on, first check whether this test still exhibits the issue at hand. Second, simplify this test by manually reducing its size. This step consists of repeated application of the following substeps: Discard irrelevant constraints Inline constraints Manually apply rules Constraints that do not have to do with the issue can sometimes be removed completely. A common example is the projectOk constraint, but also declaration name uniqueness checks and such can often just be omitted. In the second substep we try to reduce the number of constraint by simplification. Examples of simplification include inlining unification with variables (e.g. replacing { T } T == INT (), subtype ( T , T ) with subtype ( INT (), INT ()) ), or solving deterministic queries (e.g. replacing ! var [ \"x\" , INT () ] in s , query var ... in s |-> [ ( _ , ( _ , T )) ] with T == INT () ). The third substep entails choosing the correct rule for a user-defined constraint, and replacing the user-defined constraint with the properly applied body. For example, when the specification constains a rule fileOk ( s , File ( imp , decls )) : - importsOk ( s , imp ), declsOk ( decls ) , the fileOk ( s , < AST 1 > ) constraint in the test above can be replaced with importsOk ( s , < IMPs 1 > ), declsOk ( s , < DECLS 1 > ) . Be aware to properly introduce new names for variables from the rule head. Additionally, it is required to update the imported modules when new user-defined constraints are introduced by application. Warning At each step, execute your test to verify whether it still exposes the issue. When it does not, you have either discarded a relevant constraint, or made a mistake when simplifying. Try to apply these steps exhaustively, until only built-in constraints remain. Now that we have a minimal test case, the third step is to make it self-contained by removing imports. This just involved copying the relevant constraint declarations and rules in a rules section in the test, and removing the import.","title":"Creating Self-Containted Tests from Examples"},{"location":"howtos/statix/debugging/#some-common-problems","text":"1. Predicates fail with amb(...) terms as arguments. These terms indicate parsing ambiguities, which should be fixed in the grammar (SDF3) files. 2. Errors in your specification appear at incorrect places (e.g. sort or constructor declarations). In such cases, the declaration is referenced from an invalid position anywhere in your specification, but due to the non-deterministic order of constraint solving the error appears at the observed position. The best approach to solve these issues is to comment away all usages, until the error disappears. Then, in the last commented position, the declaration is used incorrectly. 3. One or both of the fileOk(...) or projectOk(...) predicates fail immediately, for example with the error messages: statics!fileOk(#s_1-1,Test([Prog(\"A.mod\",Decls(\u2026)),Prog(\"B.mod\",Decls(\u2026)),Prog(\"C.mod\",Decls(\u2026))])) (no origin information) statics!projectOk(#s_1-1) (no origin information) In such cases, you have probably renamed the top-level file, or moved the declarations of these predicates to another file that is imported. Assuming the predicates are now defined in the module statics/mylang as follows: // file: trans/statics/mylang.stx module statics/mylang imports statics/mylang/program rules projectOk : scope projectOk(s). fileOk : scope * Start fileOk(s, p) :- programOk(s, p). If this module is the top-level module of your specification, then you have to change the call to stx-editor-analyze in trans/analysis.str such that the first term argument (which specifies the module to use, by default \"statics\" ) is the new module name (in this case statics/mylang ). On the other hand, if you kept statics as the top-level module and have it import the module statics/mylang , then you have to change the call to stx-editor-analyze in trans/analysis.str such that the second and third term argument (which specify the predicates to apply to projects and files, respectively) are qualified by the module name (in this case \"statics/mylang!projectOk\" and \"\"statics/mylang!fileOk , respectively). 4. Files of your language are only analyzed by Statix after they are opened in an editor in Eclipse. There are several reasons why this may be hapening: The project containing the files is not a Spoofax project. A spoofax project must contain a metaborg.yaml . If it is a Maven project, the packaging must be one of spoofax-{language,test,project} . The project containing the files does not have a dependency on your language. Spoofax only analyzes files of your language if the metaborg.yaml configuration contains a compile dependency on the language definition. This should look similar to the following: dependencies : compile : - org.example:your-language:1.0-SNAPSHOT The language is missing. If a language dependency is missing, this is reported with errors on the console. Make sure your language definition project is open in Eclipse and that is is successfully built. Eclipse is not configured to automatically build files. This can be enabled by selecting Project > Build automatically from the Eclipse menu. The project in Eclipse did not get the Spoofax nature. Imported Maven projects with one of the spoofax-* packagings normally get the Spoofax nature automatically, but sometimes this doesn't work correctly. Non-Maven projects always have to be assigned the Spoofax nature manually. This can be done with Spoofax > Add Spoofax nature in the context-menu of the project containing the files. 5. A lot of errors are reported. It happens that a single problem in the type checked program leads to the failure of other constraints (cascading errors). For example, an unresolved name might lead to errors about subtype checks that cannot be solved, import edges that cannot be created, etc. Here are some tips to help you find the root cause of the probem: Differentiate between failed and unsolved constraints. The cause of a problem is usually found best by looking at the failed constraints. For example, an unresolved name might result in an error on the equality constraint between the expected and actual query result. Errors on unsolved constraints are marked as Unsolved . Unsolved errors are often the result of uninstantiated logical variables. Predicates remain unsolved if the uninstantiated variable prevents the selection of an applicable rule for the predicate. For example, an unsolved error subtype(INT(), ?T-1) is caused by the free variable ?T-1 which prevents selecting the appropriate rule of the subtype predicate. Queries remain unsolved if the query scope is not instantiated, or if variables used in the matching predicate (such as the name to resolve) remained free. For example, an unsolved error query filter (e Label(\"typeOfDecl\")) and { (?x',_) :- ?x' == ?x-5 } min irrefl trans anti-sym { <edge>Label(\"typeOfDecl\") < <edge>Label(\"P\"); } and { _, _ :- true } in ?s-3 |-> [(?wld0-1,(?x'-2,?T-4))] cannot be resolved because the scope variable ?s-3 is free, and the free variable ?x-5 would prevent matching declarations. Use of the variables ?x'-2 and ?T-4 might cause more unsolved constraints, since these also remain free when the query cannot be solved. Edge and declaration assertions remain unsolved if the scopes are not instantiated. For example, the edge assertion #s_2-1 -Label(\"P\")-> ?s'-5 cannot be solved because the variable for the target scope ?s'-5 is not instantiated. Unsolved edge constraints in particular can lead to lots of cascading errors, as they block all queries going through the source scope of the edge. If it is not immediately clear which error is the root of a problem, it helps to figure out the free variable dependencies between reported errors. Consider the following small example of three reported errors: subtype(?T-5, LONG) #s_3-1 -Label(\"P\")-> ?s'-6 query filter ((Label(\"P\"))* Label(\"typeOfDecl\")) and { (?x',_) :- ?x' == \"v\" } min irrefl trans anti-sym { <edge>Label(\"typeOfDecl\") < <edge>Label(\"P\"); } and { _, _ :- true } in #s_3-1 |-> [(?wld4-1,(?x'-2,?T-5))] For each of these we can see which variables are necessary for the constraint to be solved, and which they might instantiate when solved. The subtype predicate is blocked on the variable ?T-5 . The edge assertion is blocked on the scope variable ?s'-6 . The query does not seem blocked on a variable (both the scope and the filter predicate are instantiated), but would instantiate the variables ?x'-2 and ?T-5 when solved. We can conclude that the subtype constraint depends on solving the query, so we focus our attention on the query. Now we realize that we query in the scope of the unsolved edge assertion. So, the query depends on the edge assertion, and our task is to figure out why the scope variable in the edge target is not instantiated. Info Since Spoofax 2.5.17, many cascading errors should not be displayed anymore. If you encounter a message that is clearly a cascading error from another constraint, and not part of a cycle of unsolved constraints, consider reporting this as a bug. Also, the delays and unclosed critical edges should be displayed explicitly on the message of unsolved constraints. This helps figuring out dependencies between failed constraints. 6. Constraint solving does not terminate. This is caused by infinite recursion in a user-defined constraint. Three common causes of this problem are. First, the specification contains recursive predicates, such as rule(x, T) :- /* ... */, rule(x, T), /* ... */. . As can be trivially seen, the solver will simplify rule(x, T) infinitely many times. Second, the specification contains a rule that creates a declaration, queries it, and then calls itself on the query result. If the result contains the declaration made by the rule, it will instantiate itself infinitely. An example of such a specification is: declareTVar ( s , x , T ) : - { Tvs } ! tvar [ x , T ] in s , query tvar /* */ in s |-> Tvs , declareTVars ( s , Tvs ). Often, this pattern is not easily observable, since the recursion may be indirect, and the declarations, queries and recursive calls are specified in different rules. Info This pattern often occurs in incorrect specifications for parametric types with lazy substitution. Third, for a user-defined constraint on a existential variable, optimistic rule selection can cause infinite generation and refinement of new, unconstrained variables. Consider for example the following specification: rules ruleWithoutBaseCase : list ( int ) ruleWithoutBaseCase ( [ x | xs ] ) : - ruleWithoutBaseCase ( xs ). Although the recursive call seems to be on a strictly smaller term (namely, the tail of the list), infinite recursion can still happen when this rule is instantiated with a free variable, such as in this constraint: { x } ruleWithoutBaseCase ( x ), x == [] . While this constraint should fail, it can be that the solver decides to simplify ruleWithoutBaseCase(x) first. Due to optimistic rule selection, it will refine x to [x1 | xs] , and simplify to ruleWithoutBaseCase(xs) . When later the constraint x == [] is solved, it will simply fail. But since xs is free, the sequence of simplifying ruleWithoutBaseCase on a free variable repeats indefinitely. The Statix normalization often introduces new existential variables. Therefore it might not be completely obvious that a specification is susceptible to this behavior. Consider for example the following specification: rules ruleWithoutBaseCase : list ( int ) ruleWithoutBaseCase ( [ x | xs ] ) : - ruleWithoutBaseCase ( xs ). nil : -> list ( int ) nil () = [] . test : test () : - ruleWithoutBaseCase ( nil ()). Although this specification does not seem to have existential variables, its normalized equivalent (see below), actually does. rules ruleWithoutBaseCase : list ( int ) ruleWithoutBaseCase ( [ x | xs ] ) : - ruleWithoutBaseCase ( xs ). nil : list ( int ) nil ( [] ). test : test () : - { nil1 } ruleWithoutBaseCase ( nil1 ), nil ( nil1 ). Now, it can be seen that the normalized test rule is again susceptile to this type of infinite recursion. In order to debug non-terminating specifications, first add base cases like rule(_, _, ..) :- false for all user-defined constraints that do not yet have those. This prevents recursion by optimistic rule selection. Potential errors that pop up now demonstrate which rule was incorrectly selected optimistically. If that does not work out, the other techniques in this section should be applied to isolate the recursion.","title":"Some Common Problems"},{"location":"howtos/statix/debugging/#getting-help-and-reporting-issues","text":"If the techniques above did not help to solve your problem, you can ask us for help or report the issue you found. To make this process as smooth as possible, we ask you to follow the following template when asking a Statix related question: Single sentence description of the issue. Spoofax version. See About Eclipse ; Installation Details ; Features , and search for Spoofax . Statix configuration: single-file or multi-file mode. Multi-file mode is enabled when the observer setting in in your ESV looks like observer: editor-analyze (constraint) (multifile) . Steps to reproduce. Best is to include a small, self-contained test (see Testing Predicates above) so that others can easily run the test and reproduce the issue! If that is not possible, provide a (link to) a project, including an example file, that shows the problem. Keep the project and the example as small as possible, and be specific about the relevant parts of your program and of your specification. Description of the observed behavior. Also mention if the problem occurs consistently, or only sometimes? If only sometimes, does it occur always/never after a clean build, or does it occur always/never after editing and/or building without cleaning? Description of the expected behavior. Extra information that you think is relevant to the problem. For example, things you have tried already, pointers to the part of the rules you think are relevant to the problem etc. If you tried other examples that show some light on the issue, this is a good place to put those. Again, it is best if these also come as self-contained tests! An example bug report described using the format above: Issue: Spoofax version: 2.6.0.20210208-173259-master Statix setup: multi-file Steps to reproduce: Execute the test in `example1.stxtest`. Observed behavior: Sometimes an error is reported that the `query` failed. The problem does not occur consistently. On some runs, the error appears, but not on others. This does not seem related to cleaning or building the project. Expected behavior: The test is executed and no errors are reported. Scope `s1` is reachable from `s2`, so the query return a single result, and `ps != []` should therefore hold. Extra information: The test in `example2.stxtest` is very similar. The only difference is that the predicate `nonempty` has an extra rule for the empty list. The predicate is semantically the same, as the extra rule fails, just as the general rule would do on the empty list. However, this example never gives the unexpected error. The bug report is accompanied by two self-contained tests. One illustrates the problem, while the other shows a very similar variant that does not exhibit the problem. // example1.stxtest resolve { s1 s2 } new s1 , new s2 , s2 -I-> s1 , reachable ( s1 , s2 ) signature name-resolution labels I rules reachable : scope * scope reachable ( s1 , s2 ) : - { ps } query () filter I * and { s1' : - s1' == s1 } min and true in s2 |-> ps , nonempty ( ps ). nonempty : list (( path * scope )) nonempty ( ps ) : - ps != [] . // example2.stxtest resolve { s1 s2 } new s1 , new s2 , s2 -I-> s1 , reachable ( s1 , s2 ) signature name-resolution labels I rules reachable : scope * scope reachable ( s1 , s2 ) : - { ps } query () filter I * and { s1' : - s1' == s1 } min and true in s2 |-> ps , nonempty ( ps ). nonempty : list (( path * scope )) nonempty ( ps ) : - ps != [] . nonempty ( [] ) : - false .","title":"Getting Help and Reporting Issues"},{"location":"howtos/statix/migrating-from-nabl2/","text":"Migrate to Statix from NaBL2 \u00b6 Signature \u00b6 All sorts and constructors must be explicitly defined in Statix in sorts and constructors signatures. Sorts in Statix are mostly similar to terms in NaBL2. Notable differences: There is no catch-all sort term in Statix. There are no sort variables in Statix. List sorts in Statix are written as list ( X ) for some sort X . Statix signatures for language syntax can be generated from SDF3 definitions with the signature generator . Name-resolution \u00b6 Name resolution in NaBL2 heavily relies on occurrences and their unique identity. In Statix, the notion of a stand-alone reference is replaced by the notion of a query. Therefore, the use of occurrences is now discouraged in favour of regular terms, relations, and and predicates for the different namespaces. signature namespaces Var name resolution labels P well-formedness P * order D < P rules [[ Def ( x , T ) ^ ( s ) ]] := Var { x } <- s , Var { x } : T . [[ Var ( x ) ^ ( s ) : T ]] := Var { x } -> s , Var { x } | -> d , d : T . signature relations var : string * TYPE name-resolution labels P rules declareVar : scope * string * TYPE declareVar ( s , x , T ) : - ! var [ x , T ] in s . resolveVar : scope * string -> TYPE resolveVar ( s , x ) = T : - { x' } query var filter P * and { x' : - x' == x } min $ < P and true in s |-> [ ( _ , ( x , T )) ] , @ x . ref := x' . rules stmtOk : scope * Stmt stmtOk ( s , Def ( x , T )) : - declareVar ( s , x , T ); typeOfExp : scope * Exp -> TYPE typeOfExp ( s , Var ( x )) = T : - T == resolveVar ( s , x ). Things to note: Each namespace gets its own relation, and set of predicates to declare and resolve in that namespace ( declareXXX and resolveXXX ). The regular expression and order on labels is not global anymore, but part of the query in the resolveXXX rules. If a declaration should have a type associated with it, it is now part of the relation. The fact that it appears after the arrow -> indicates that each declaration has a single type. As a result, declareXXX combines the constraints XXX{...} <- s, XXX{...} : T . Similarly, resolveXXX combines the constraints XXX{...} -> s, XXX{...} |-> d, d : T . The end-of-path label, called D in NaBL2, now has a special symbol $ , instead of the reserved name. Functions \u00b6 NaBL2 functions can be translated to Statix predicates in a straight-forward manner. Note that if the function was used overloaded,it is necessary to defined different predicates for the different argument types. signature functions plusType : ( Type * Type ) -> Type { ( IntTy () , IntTy () ) -> IntTy (), ( StrTy () , _ ) -> StrTy (), ( ListTy ( a ), a ) -> ListTy ( a ), ( ListTy ( a ), ListTy ( a )) -> ListTy ( a ) } plusType : Type * Type -> Type plusType ( IntTy () , IntTy () ) = IntTy (). plusType ( StrTy () , _ ) = StrTy (). plusType ( ListTy ( a ), a ) = ListTy ( a ). plusType ( ListTy ( a ), ListTy ( a )) = ListTy ( a ). Relations \u00b6 Relations as they exist in NaBL2 are not supported in Statix. An example of a subtyping relation in NaBl2 would translate as follows: signature relations reflexive, transitive, anti-symmetric sub : Type * Type { FunT(-sub, +sub), ListT(+sub) } rules [[ Class(x, superX, _) ^ (s) ]] := ... more constraints ..., ClassT(x) <sub! ClassT(superX). [[ Def(x, T, e) ^ (s) ]] := [[ e ^ (s) : T' ]], T1 <sub? T2. rules subType : TYPE * TYPE subType ( FunT ( T1 , T2 ), FunT ( U1 , U2 )) : - subType ( U1 , T1 ), subType ( T2 , T1 ). subType ( ListT ( T ), ListT ( U )) : - subType ( T , U ). subType ( ClassT ( s1 ), ClassT ( s2 )) : - ... check connectivity of s1 and s2 in the scope graph ... In this case implementing the subType rule for ClassT requires changing the encoding of class types. Instead of using names, we use the class scope to identify the class type. This pattern is know as Scopes as Types . Subtyping between class scopes can be checked by checking if one scope is reachable from the other. Rules \u00b6 NaBL2 constraint generation rules must be translated to Statix predicates and corresponding rules. Predicates in Statix are explcitly typed, and a predicate has to be defined for each sort for which constraint generation rules are defined. Here are some example rules for expressions in NaBL2: [[ Let ( binds , body ) ^ ( s ) : T ]] := new s_let , s_let -P-> s , Map1 [[ binds ^ ( s , s_let ) ]], [[ body ^ ( s_let ) : T ]]. [[ Bind ( x , e ) ^ ( s , s_let ) ]] : - [[ e ^ ( s ) : T ]], Var { x } <- s_let , Var { x } : T . In Statix these would be encoded as: typeOfExp : scope * Exp -> TYPE typeOfExp ( s , e @ Let ( binds , body )) = T : - { s_let } new s_let , s_let -P-> s , bindsOk ( s , binds , s_let ), T == typeOfExp ( s_let , body ), @ e . type := T . bindOk : scope * Bind * scope bindsOk maps bindOk ( * , list ( * )) bindOk ( s , Bind ( x , e ), s_let ) : - { T } T == typeOfExp ( s , e ), declareVar ( s_let , x , T ).","title":"Migrate from NaBL2"},{"location":"howtos/statix/migrating-from-nabl2/#migrate-to-statix-from-nabl2","text":"","title":"Migrate to Statix from NaBL2"},{"location":"howtos/statix/migrating-from-nabl2/#signature","text":"All sorts and constructors must be explicitly defined in Statix in sorts and constructors signatures. Sorts in Statix are mostly similar to terms in NaBL2. Notable differences: There is no catch-all sort term in Statix. There are no sort variables in Statix. List sorts in Statix are written as list ( X ) for some sort X . Statix signatures for language syntax can be generated from SDF3 definitions with the signature generator .","title":"Signature"},{"location":"howtos/statix/migrating-from-nabl2/#name-resolution","text":"Name resolution in NaBL2 heavily relies on occurrences and their unique identity. In Statix, the notion of a stand-alone reference is replaced by the notion of a query. Therefore, the use of occurrences is now discouraged in favour of regular terms, relations, and and predicates for the different namespaces. signature namespaces Var name resolution labels P well-formedness P * order D < P rules [[ Def ( x , T ) ^ ( s ) ]] := Var { x } <- s , Var { x } : T . [[ Var ( x ) ^ ( s ) : T ]] := Var { x } -> s , Var { x } | -> d , d : T . signature relations var : string * TYPE name-resolution labels P rules declareVar : scope * string * TYPE declareVar ( s , x , T ) : - ! var [ x , T ] in s . resolveVar : scope * string -> TYPE resolveVar ( s , x ) = T : - { x' } query var filter P * and { x' : - x' == x } min $ < P and true in s |-> [ ( _ , ( x , T )) ] , @ x . ref := x' . rules stmtOk : scope * Stmt stmtOk ( s , Def ( x , T )) : - declareVar ( s , x , T ); typeOfExp : scope * Exp -> TYPE typeOfExp ( s , Var ( x )) = T : - T == resolveVar ( s , x ). Things to note: Each namespace gets its own relation, and set of predicates to declare and resolve in that namespace ( declareXXX and resolveXXX ). The regular expression and order on labels is not global anymore, but part of the query in the resolveXXX rules. If a declaration should have a type associated with it, it is now part of the relation. The fact that it appears after the arrow -> indicates that each declaration has a single type. As a result, declareXXX combines the constraints XXX{...} <- s, XXX{...} : T . Similarly, resolveXXX combines the constraints XXX{...} -> s, XXX{...} |-> d, d : T . The end-of-path label, called D in NaBL2, now has a special symbol $ , instead of the reserved name.","title":"Name-resolution"},{"location":"howtos/statix/migrating-from-nabl2/#functions","text":"NaBL2 functions can be translated to Statix predicates in a straight-forward manner. Note that if the function was used overloaded,it is necessary to defined different predicates for the different argument types. signature functions plusType : ( Type * Type ) -> Type { ( IntTy () , IntTy () ) -> IntTy (), ( StrTy () , _ ) -> StrTy (), ( ListTy ( a ), a ) -> ListTy ( a ), ( ListTy ( a ), ListTy ( a )) -> ListTy ( a ) } plusType : Type * Type -> Type plusType ( IntTy () , IntTy () ) = IntTy (). plusType ( StrTy () , _ ) = StrTy (). plusType ( ListTy ( a ), a ) = ListTy ( a ). plusType ( ListTy ( a ), ListTy ( a )) = ListTy ( a ).","title":"Functions"},{"location":"howtos/statix/migrating-from-nabl2/#relations","text":"Relations as they exist in NaBL2 are not supported in Statix. An example of a subtyping relation in NaBl2 would translate as follows: signature relations reflexive, transitive, anti-symmetric sub : Type * Type { FunT(-sub, +sub), ListT(+sub) } rules [[ Class(x, superX, _) ^ (s) ]] := ... more constraints ..., ClassT(x) <sub! ClassT(superX). [[ Def(x, T, e) ^ (s) ]] := [[ e ^ (s) : T' ]], T1 <sub? T2. rules subType : TYPE * TYPE subType ( FunT ( T1 , T2 ), FunT ( U1 , U2 )) : - subType ( U1 , T1 ), subType ( T2 , T1 ). subType ( ListT ( T ), ListT ( U )) : - subType ( T , U ). subType ( ClassT ( s1 ), ClassT ( s2 )) : - ... check connectivity of s1 and s2 in the scope graph ... In this case implementing the subType rule for ClassT requires changing the encoding of class types. Instead of using names, we use the class scope to identify the class type. This pattern is know as Scopes as Types . Subtyping between class scopes can be checked by checking if one scope is reachable from the other.","title":"Relations"},{"location":"howtos/statix/migrating-from-nabl2/#rules","text":"NaBL2 constraint generation rules must be translated to Statix predicates and corresponding rules. Predicates in Statix are explcitly typed, and a predicate has to be defined for each sort for which constraint generation rules are defined. Here are some example rules for expressions in NaBL2: [[ Let ( binds , body ) ^ ( s ) : T ]] := new s_let , s_let -P-> s , Map1 [[ binds ^ ( s , s_let ) ]], [[ body ^ ( s_let ) : T ]]. [[ Bind ( x , e ) ^ ( s , s_let ) ]] : - [[ e ^ ( s ) : T ]], Var { x } <- s_let , Var { x } : T . In Statix these would be encoded as: typeOfExp : scope * Exp -> TYPE typeOfExp ( s , e @ Let ( binds , body )) = T : - { s_let } new s_let , s_let -P-> s , bindsOk ( s , binds , s_let ), T == typeOfExp ( s_let , body ), @ e . type := T . bindOk : scope * Bind * scope bindsOk maps bindOk ( * , list ( * )) bindOk ( s , Bind ( x , e ), s_let ) : - { T } T == typeOfExp ( s , e ), declareVar ( s_let , x , T ).","title":"Rules"},{"location":"howtos/statix/migrating-to-concurrent-solver/","text":"Migrating to the Concurrent Solver \u00b6 In this how-to guide, we explain what changes should be made to enable the concurrent solver for a Statix project. Enabling the Concurrent solver for a Language \u00b6 To enable the concurrent solver for a language, set the language.statix.concurrent property in the metaborg.yaml file to true . This ensures that the concurrent solver is used for all sources in the language. id : org.example:mylang:0.1.0-SNAPSHOT name : mylang language : statix : mode : concurrent Enabling the Concurrent solver for an Example Project only \u00b6 To enable the concurrent solver for a particular project only, set the runtime.statix.modes property in the metaborg.yaml file to a map that contains all names of the languages for which you want to use the concurrent solver, and their corresponding modes. The name of the language should correspond to the name property in the metaborg.yaml of the language definition project. id : org.example:mylang.example:0.1.0-SNAPSHOT runtime : statix : modes : - mylang : concurrent Indirect Type Declaration \u00b6 Type checking with the concurrent solver might result in deadlock when type-checkers have mutual dependencies on their declarations. This problem can be solved by adding an intermediate declaration that splits the part of the declaration data that is filtered on (usually the declaration name ), and the part that is processed further by the querying unit (usually the type ). This pattern is best explained with an example: signature relations type : ID -> TYPE rules declareType : scope * ID * TYPE resolveType : scope * ID -> TYPE declareType ( s , x , T ) : - ! type [ x , T ] in s . resolveType ( s , x ) = T : - query type filter P * I * and { x' : - x' == x } in s |-> [ ( _ , ( _ , T )) ] . This specification needs to be changed in the following: signature relations type : ID -> scope typeOf : TYPE rules declareType : scope * ID * TYPE resolveType : scope * ID -> TYPE declareType ( s , x , T ) : - ! type [ x , withType ( T ) ] in s . resolveType ( s , x ) = typeOf ( T ) : - query type filter P * I * and { x' : - x' == x } in s |-> [ ( _ , ( _ , T )) ] . We now discuss the changes one-by-one. First, the signature of relation type is be changed to ID -> scope . In this scope, we store the type using the newly introduced typeOf relation. This relation only carries a single TYPE term. In this way, the original term is still indirectly present in the outer declaration. The withType and typeOf rules allow to convert between these representations. The withType rule creates a scope with a typeOf declaration that contains the type. In the adapted declareType rule, this constraint is used to convert the T argument to the representation that the type relation accepts. Likewise, the typeOf rule queries the typeOf declaration to extract the type from a scope. This rule is used in the resolveType rule to convert back to the term representation of a type. Performing this change should resolve potential deadlocks when executing your specifications. Because the signatures of the rules in the original specification did not change, and the new specification should have identical semantics, the remainder of the specification should not be affected. Using Grouping \u00b6 The traditional solver uses two special constraints as entry points: the project constraint (usually named projectOk ) was solved once for each project, while the file constraint (usually called fileOk ) was solved for each file in that project. The concurrent solver adds the concepts of groups in between these concepts. Files can be organized in groups, while groups can in addition contain subgroups. This gives rise to a tree-shaped hierarchy, where the project is the root node, the files are the leaf nodes, and all nodes in between are groups. Most often, this hierarchy follows the directory structure of a project. In order to use grouping, two steps need to be performed. First, a group constraint must be defined. The group constraint must have the signature scope * string * scope . For each group, this constraint will be instantiated with the parent group scope, group name and own group scope as arguments (in that order). A simple example of a group constraint can look as follows: signature sorts MODULE constructors MODULE : scope -> MODULE relations mod : string * MODULE rules groupOk : scope * string * scope groupOk ( s_prnt , name , s_grp ) : - ! mod [ name , MODULE ( s_grp ) ] in s_prnt . In this fragment, we define the groupOk constraint, which has the appropriate signature. The body of the rule for this constraint simply declares that a module with the appropriate name exists in the parent scope. Second, the builder needs to be adapted in two ways: the name of the group constraint must be passed to the solver, and a strategy that determines the group of a file must be provided to the solver. Such a strategy should have type ( String * AST ) - > List ( String ) . The input arguments represent the file path and its AST, respectively. The output list should contain all group identifiers from project root to the respective file. For example, a Java specification should return [\"java\", \"lang\", \"Object\"] for the Object class in the java.lang package. Note that the file must be assigned a name, and that that name should be included in the output list. A project that uses the directory structure as its grouping structure could call the stx-editor-analyze as follows: rules editor-analyze = stx-editor-analyze ( pre-analyze , group-key , post-analyze | \"statics\" , \"projectOk\" , \"groupOk\" , \"fileOk\" ) group-key : ( resource , ast ) - > key with rel-path := < current-language-relative-source-or-include-path > resource ; key := < string-tokenize > ([ '/' , '\\' ], rel-path ) In this snippet, the \"groupOk\" argument between the file and project constraint names points to our newly defined group constraint. The group-key strategy, passed between the pre and post transformation strategies, is the strategy that performs the grouping. It first makes the path relative to the project root, and then splits it on each '/' or '\\' character. Using Libraries \u00b6 Secondly, the concurrent solver allows to export the scope graph of a project in a library. These libraries can be linked with other projects, potentially decreasing analysis times significantly. Statix libraries can be generated and linked by following three steps. As a first step, use the Spoofax \u2023 Statix \u2023 Make project library menu on a file in your library project to export its scope graph. A project.stxlib file will now appear in the root directory of that project. Project Scope Configuration Currently, the project scope in the project.stxlib file must still be configured manually. Some understanding of the Statix library format is helpful for that. The signature for Statix libraries looks roughly as follows: sorts Library constructors Library : list ( Scope ) * list ( Scope ) * ScopeGraph -> Library sorts ScopeGraph constructors ScopeGraph : list (( Scope * Datum ? * list ( Edge ))) -> ScopeGraph The top-level Library term contains (in this order): the list of shared scopes, the list of all scopes of the library, and the actual scope graph. A scope graph consists of scope entries, which are defined as three-tuples of: scope, datum associated to that scope, and the list of outgoing edges for that scope. In order to configure the root scopes correctly, perform the following steps: Identify the project scope. Names of the project scope are: Scope(\"/.\", \"s_prj-0\") for project scopes generated by the concurrent solver, and Scope(\"\", \"s_1-1\") for project scopes generated by the traditional solver. Add the project scope to the list of shared scopes in the exported library. Instead of Library([], ...) , this term should now look like Library([Scope(\"/.\", \"s_prj-0\")], ...) . If it is present, remove the project root scope from the second list in the Library term. Remove the datum on the project scope. To do this, find the project scope entry in the scope graph, and change its second argument from Some(...) to None() . The entry should now roughly look like (Scope(\"/.\", \"s_prj-0\"), None(), [...]) . The second step is to copy this file in the lib/ directory of the project that will use the library. Additionally, you might rename the file to something more descriptive (e.g. stdlib ), but ensure that the .stxlib extension is preserved. Thirdly, the library must be enabled. To enable the library, create a lib/stxlibs file (no extension) that contains a list of enabled library names. Continuing our previous example, the content of that file would be [\"stdlib\"] . Project-level Declarations When the project constraint asserts declarations, these will be duplicated, because the project constraint is solved both when analyzing the library and when analyzing the project using the library. As a general principle, project constraints should not make declarations. Instead, libraries are meant as a replacement for the project constraint to declare built-in types. Multiple Libraries Using multiple libraries is supported by adding multple *.stxlib files in the lib/ directory, and having multiple entries in the stxlibs file. An example of such a file could look like [\"lib1\", \"lib2\"] . Note however that libraries will not link properly when different exports are used, due to the fact that scope identities are not deterministic. Incremental Solver \u00b6 Thirdly, there is experimental support for incremental analysis. To enable this, the following options for the mode / modes settings are available: In language projects: incremental-scope-graph-diff . In example projects: incremental-deadlock or incremental-scope-graph-diff .","title":"Migrating to the Concurrent Solver"},{"location":"howtos/statix/migrating-to-concurrent-solver/#migrating-to-the-concurrent-solver","text":"In this how-to guide, we explain what changes should be made to enable the concurrent solver for a Statix project.","title":"Migrating to the Concurrent Solver"},{"location":"howtos/statix/migrating-to-concurrent-solver/#enabling-the-concurrent-solver-for-a-language","text":"To enable the concurrent solver for a language, set the language.statix.concurrent property in the metaborg.yaml file to true . This ensures that the concurrent solver is used for all sources in the language. id : org.example:mylang:0.1.0-SNAPSHOT name : mylang language : statix : mode : concurrent","title":"Enabling the Concurrent solver for a Language"},{"location":"howtos/statix/migrating-to-concurrent-solver/#enabling-the-concurrent-solver-for-an-example-project-only","text":"To enable the concurrent solver for a particular project only, set the runtime.statix.modes property in the metaborg.yaml file to a map that contains all names of the languages for which you want to use the concurrent solver, and their corresponding modes. The name of the language should correspond to the name property in the metaborg.yaml of the language definition project. id : org.example:mylang.example:0.1.0-SNAPSHOT runtime : statix : modes : - mylang : concurrent","title":"Enabling the Concurrent solver for an Example Project only"},{"location":"howtos/statix/migrating-to-concurrent-solver/#indirect-type-declaration","text":"Type checking with the concurrent solver might result in deadlock when type-checkers have mutual dependencies on their declarations. This problem can be solved by adding an intermediate declaration that splits the part of the declaration data that is filtered on (usually the declaration name ), and the part that is processed further by the querying unit (usually the type ). This pattern is best explained with an example: signature relations type : ID -> TYPE rules declareType : scope * ID * TYPE resolveType : scope * ID -> TYPE declareType ( s , x , T ) : - ! type [ x , T ] in s . resolveType ( s , x ) = T : - query type filter P * I * and { x' : - x' == x } in s |-> [ ( _ , ( _ , T )) ] . This specification needs to be changed in the following: signature relations type : ID -> scope typeOf : TYPE rules declareType : scope * ID * TYPE resolveType : scope * ID -> TYPE declareType ( s , x , T ) : - ! type [ x , withType ( T ) ] in s . resolveType ( s , x ) = typeOf ( T ) : - query type filter P * I * and { x' : - x' == x } in s |-> [ ( _ , ( _ , T )) ] . We now discuss the changes one-by-one. First, the signature of relation type is be changed to ID -> scope . In this scope, we store the type using the newly introduced typeOf relation. This relation only carries a single TYPE term. In this way, the original term is still indirectly present in the outer declaration. The withType and typeOf rules allow to convert between these representations. The withType rule creates a scope with a typeOf declaration that contains the type. In the adapted declareType rule, this constraint is used to convert the T argument to the representation that the type relation accepts. Likewise, the typeOf rule queries the typeOf declaration to extract the type from a scope. This rule is used in the resolveType rule to convert back to the term representation of a type. Performing this change should resolve potential deadlocks when executing your specifications. Because the signatures of the rules in the original specification did not change, and the new specification should have identical semantics, the remainder of the specification should not be affected.","title":"Indirect Type Declaration"},{"location":"howtos/statix/migrating-to-concurrent-solver/#using-grouping","text":"The traditional solver uses two special constraints as entry points: the project constraint (usually named projectOk ) was solved once for each project, while the file constraint (usually called fileOk ) was solved for each file in that project. The concurrent solver adds the concepts of groups in between these concepts. Files can be organized in groups, while groups can in addition contain subgroups. This gives rise to a tree-shaped hierarchy, where the project is the root node, the files are the leaf nodes, and all nodes in between are groups. Most often, this hierarchy follows the directory structure of a project. In order to use grouping, two steps need to be performed. First, a group constraint must be defined. The group constraint must have the signature scope * string * scope . For each group, this constraint will be instantiated with the parent group scope, group name and own group scope as arguments (in that order). A simple example of a group constraint can look as follows: signature sorts MODULE constructors MODULE : scope -> MODULE relations mod : string * MODULE rules groupOk : scope * string * scope groupOk ( s_prnt , name , s_grp ) : - ! mod [ name , MODULE ( s_grp ) ] in s_prnt . In this fragment, we define the groupOk constraint, which has the appropriate signature. The body of the rule for this constraint simply declares that a module with the appropriate name exists in the parent scope. Second, the builder needs to be adapted in two ways: the name of the group constraint must be passed to the solver, and a strategy that determines the group of a file must be provided to the solver. Such a strategy should have type ( String * AST ) - > List ( String ) . The input arguments represent the file path and its AST, respectively. The output list should contain all group identifiers from project root to the respective file. For example, a Java specification should return [\"java\", \"lang\", \"Object\"] for the Object class in the java.lang package. Note that the file must be assigned a name, and that that name should be included in the output list. A project that uses the directory structure as its grouping structure could call the stx-editor-analyze as follows: rules editor-analyze = stx-editor-analyze ( pre-analyze , group-key , post-analyze | \"statics\" , \"projectOk\" , \"groupOk\" , \"fileOk\" ) group-key : ( resource , ast ) - > key with rel-path := < current-language-relative-source-or-include-path > resource ; key := < string-tokenize > ([ '/' , '\\' ], rel-path ) In this snippet, the \"groupOk\" argument between the file and project constraint names points to our newly defined group constraint. The group-key strategy, passed between the pre and post transformation strategies, is the strategy that performs the grouping. It first makes the path relative to the project root, and then splits it on each '/' or '\\' character.","title":"Using Grouping"},{"location":"howtos/statix/migrating-to-concurrent-solver/#using-libraries","text":"Secondly, the concurrent solver allows to export the scope graph of a project in a library. These libraries can be linked with other projects, potentially decreasing analysis times significantly. Statix libraries can be generated and linked by following three steps. As a first step, use the Spoofax \u2023 Statix \u2023 Make project library menu on a file in your library project to export its scope graph. A project.stxlib file will now appear in the root directory of that project. Project Scope Configuration Currently, the project scope in the project.stxlib file must still be configured manually. Some understanding of the Statix library format is helpful for that. The signature for Statix libraries looks roughly as follows: sorts Library constructors Library : list ( Scope ) * list ( Scope ) * ScopeGraph -> Library sorts ScopeGraph constructors ScopeGraph : list (( Scope * Datum ? * list ( Edge ))) -> ScopeGraph The top-level Library term contains (in this order): the list of shared scopes, the list of all scopes of the library, and the actual scope graph. A scope graph consists of scope entries, which are defined as three-tuples of: scope, datum associated to that scope, and the list of outgoing edges for that scope. In order to configure the root scopes correctly, perform the following steps: Identify the project scope. Names of the project scope are: Scope(\"/.\", \"s_prj-0\") for project scopes generated by the concurrent solver, and Scope(\"\", \"s_1-1\") for project scopes generated by the traditional solver. Add the project scope to the list of shared scopes in the exported library. Instead of Library([], ...) , this term should now look like Library([Scope(\"/.\", \"s_prj-0\")], ...) . If it is present, remove the project root scope from the second list in the Library term. Remove the datum on the project scope. To do this, find the project scope entry in the scope graph, and change its second argument from Some(...) to None() . The entry should now roughly look like (Scope(\"/.\", \"s_prj-0\"), None(), [...]) . The second step is to copy this file in the lib/ directory of the project that will use the library. Additionally, you might rename the file to something more descriptive (e.g. stdlib ), but ensure that the .stxlib extension is preserved. Thirdly, the library must be enabled. To enable the library, create a lib/stxlibs file (no extension) that contains a list of enabled library names. Continuing our previous example, the content of that file would be [\"stdlib\"] . Project-level Declarations When the project constraint asserts declarations, these will be duplicated, because the project constraint is solved both when analyzing the library and when analyzing the project using the library. As a general principle, project constraints should not make declarations. Instead, libraries are meant as a replacement for the project constraint to declare built-in types. Multiple Libraries Using multiple libraries is supported by adding multple *.stxlib files in the lib/ directory, and having multiple entries in the stxlibs file. An example of such a file could look like [\"lib1\", \"lib2\"] . Note however that libraries will not link properly when different exports are used, due to the fact that scope identities are not deterministic.","title":"Using Libraries"},{"location":"howtos/statix/migrating-to-concurrent-solver/#incremental-solver","text":"Thirdly, there is experimental support for incremental analysis. To enable this, the following options for the mode / modes settings are available: In language projects: incremental-scope-graph-diff . In example projects: incremental-deadlock or incremental-scope-graph-diff .","title":"Incremental Solver"},{"location":"howtos/statix/signature-generator/","text":"Use the Statix Signature Generator \u00b6 It is quite cumbersome to write Statix signatures. Thankfully, the sdf3.ext.statix project can generate these signatures for you. Well-Formed SDF3 Requirements \u00b6 For the generator to work correctly, your SDF3 must be well formed. In particular, you must: explicitly declare each sort exactly once in your project declare lexical sorts in a lexical sorts block declare context-free sorts in a context-free sorts block for every use of a sort: either have a local declaration of a sort, or an import of a file that declares the sort not declare sorts that are not used in any rules not use any implicitly declared sorts not use complex injections, such as Pair = Expr Expr . However, list injections without terminal syntax, such as List = Elem * , are allowed. constructors must start with an upper-case letter not use sdf2table : c The generator generates strategies and signatures for each explicit declaration of a sort in SDF3, which is why each sort must be declared exactly once. SDF3 does not generate Stratego signatures for placeholders for sorts that have no corresponding rules, causing errors in the generated Statix injection explication strategies. Complex injections are not supported across Spoofax. Optional sorts cannot be represented in Statix. Applying the Generator in Spoofax 2 \u00b6 In your language project's metaborg.yaml file, change your compile dependencies to include org.metaborg:sdf3.ext.statix . For example: dependencies : compile : - org.metaborg:org.metaborg.meta.lang.esv:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.template:${metaborgVersion} - org.metaborg:sdf3.ext.statix:${metaborgVersion} Clean the project and restart Eclipse when changing the metaborg.yaml file. Once you clean your project, the extension automatically generates the following: Statix signatures declarations (in src-gen/statix/signatures/ ) Stratego strategies for explicating and removing injections (in src-gen/injections/ ) Using the Generated Injection strategies \u00b6 The generator generates strategies for explicating and removing injections. This is unfortunately needed since Statix does not support injections directly. To use these strategies, import injections/- and call the explicate-injections-MyLang-Start and implicate-injections-MyLang-Start strategies for the analysis pre-processing and post-processing respectively, where MyLang is the name of your language and Start is your language's start symbol (as specified in Syntax.esv ). For example, in trans/analysis.str : module analysis imports libspoofax / sdf / pp statixruntime statix / api injections / - libspoofax / term / origin rules editor-analyze = stx-editor-analyze ( pre-analyze , post-analyze | \"static-semantics\" , \"programOk\" ) pre-analyze = origin-track-forced ( explicate-injections-MyLang-Start ) post-analyze = origin-track-forced ( implicate-injections-MyLang-Start ) Using the Generated Signatures \u00b6 Using the generated Statix signatures is quite simple: just import them into your Statix specification. Each SDF3 file gets an associated Statix file with the signatures. For example, if your syntax is defined across two files named MyLang.sdf3 and Common.sdf3 , then in Statix you should add the following imports: imports signatures / MyLang-sig signatures / Common-sig Because Statix does not support injections, you have to use explicit constructor names for injections. For example, the following SDF3 syntax: context-free sorts Stmt VarName lexical sorts ID context-free syntax Stmt . VarDecl = < var < VarName >;> VarName . Wildcard = < _ > VarName = ID lexical syntax ID = [ a-zA-Z ] [ a-zA-Z0-9 \\ _ ] * lexical restrictions ID - / - [ a-zA-Z0-9 \\ _ ] would approximately produce the following signatures: module signatures / Test-sig imports signature sorts Stmt VarName ID = string constructors Stmt-Plhdr : Stmt VarName-Plhdr : VarName signature constructors VarDecl : VarName -> Stmt Wildcard : VarName ID2VarName : ID -> VarName Now, in Statix if you just want to capture the term of sort VarName in the VarDecl constructor, this would suffice: VarDecl ( x ) But if you want to match the term only if it has the sort ID , then you have to use the explicit injection constructor name ID2VarName : VarDecl ( ID2VarName ( x )) In this example, ID is a lexical sort, so it is an alias for string in the Statix specification. Troubleshooting \u00b6 Calls non-existing \u00b6 Build fails with errors such as this: [ strj | error ] *** (\"is-MyLang-MySort-or-inj\",0,0) calls non-existing (\"is-MyLang-ID-or-inj\",0,0) [ strj | error ] *** (\"explicate-injections-MyLang-MySort\",0,0) calls non-existing (\"explicate-injections-MyLang-ID\",0,0) [ strj | error ] *** (\"implicate-injections-MyLang-MySort\",0,0) calls non-existing (\"implicate-injections-MyLang-ID\",0,0) Executing strj failed: {} Failing builder was required by \"Generate sources\". BUILD FAILED To solve this, ensure you have declared ID (in this example) as a lexical sort in your syntax, and make sure that the syntax file with rules for MySort that reference ID import the syntax file that declares ID . Transformation failed unexpectedly \u00b6 Clean or build fails with an error such as this: ERROR: Optional sorts are not supported by Statix: Opt(Sort(\"MySort\")) Transformation failed unexpectedly for eclipse:///mylang/syntax/mysyntax.sdf3 org.metaborg.core.transform.TransformException: Invoking Stratego strategy generate-statix failed at term: CfSignature(\"MySort\", Some(\"MyCons\"), [ Param(Opt(Sort(\"MySort\")), \"mySort\") ]) Stratego trace: generate_statix_0_0 generate_statix_abstract_0_0 geninj_generate_statix_0_0 geninj_module_to_sig_0_0 with_1_1 flatfilter_1_0 filter_1_0 with_1_1 <== map_1_0 geninj_symbol_to_stxsig_0_0 Internal error: 'with' clause failed unexpectedly in 'geninj-sig-to-stxsig' Note the first line with ERROR , it tells you that something is not supported. In this case, the use of optional sorts such as MySort? is not supported by Statix and the Statix signature generator. To solve this, rewrite a syntax rule with an optional sort such as: Stmt . VarDecl = << Type ? > < ID > = < Exp >> Into a rule with an explicit sort: Stmt . VarDecl = << Type-OPT > < ID > = < Exp >> Type-OPT . NoType = <> Type-OPT = Type Note that the -OPT suffix has no special meaning. You can name the sort differently, such as OptionalType . Constructor MySort-Plhdr/0 not declared \u00b6 Build fails with an error such as this: [ strj | error ] in rule explicate-injections-MyLang-MySort(0|0): constructor MySort-Plhdr/0 not declared - MySort-Plhdr() Executing strj failed: {} BUILD FAILED You have declared a sort for which you don't have any syntax rules. Remove the sort from the context-free sorts or sorts block. No pp entry found, cannot rewrite to box \u00b6 Clean fails with an error such as this: [ identity crisis | error ] No pp entry found for: (1,[\"declSortLex\"]) - [ identity crisis | error ] Cannot rewrite to box: - declSortLex(\"MySort\") You are using the old sdf2table : c . Change this in metaborg.yaml into sdf2table : java . SPT analysis tests calling Stratego strategies fail \u00b6 An SPT test can run an arbitrary Stratego strategy on an analyzed AST and compare the results with the expected AST. If the origin of the is not tracked properly, the root constructor of the resulting analyzed AST will be missing and the comparison will fail. To fix this, ensure the pre-analyze and post-analyze strategies in analysis.str call origin-track-forced : imports libspoofax / term / origin rules pre-analyze = origin-track-forced ( explicate-injections-MyLang-Start ) post-analyze = origin-track-forced ( implicate-injections-MyLang-Start )","title":"Signature Generator"},{"location":"howtos/statix/signature-generator/#use-the-statix-signature-generator","text":"It is quite cumbersome to write Statix signatures. Thankfully, the sdf3.ext.statix project can generate these signatures for you.","title":"Use the Statix Signature Generator"},{"location":"howtos/statix/signature-generator/#well-formed-sdf3-requirements","text":"For the generator to work correctly, your SDF3 must be well formed. In particular, you must: explicitly declare each sort exactly once in your project declare lexical sorts in a lexical sorts block declare context-free sorts in a context-free sorts block for every use of a sort: either have a local declaration of a sort, or an import of a file that declares the sort not declare sorts that are not used in any rules not use any implicitly declared sorts not use complex injections, such as Pair = Expr Expr . However, list injections without terminal syntax, such as List = Elem * , are allowed. constructors must start with an upper-case letter not use sdf2table : c The generator generates strategies and signatures for each explicit declaration of a sort in SDF3, which is why each sort must be declared exactly once. SDF3 does not generate Stratego signatures for placeholders for sorts that have no corresponding rules, causing errors in the generated Statix injection explication strategies. Complex injections are not supported across Spoofax. Optional sorts cannot be represented in Statix.","title":"Well-Formed SDF3 Requirements"},{"location":"howtos/statix/signature-generator/#applying-the-generator-in-spoofax-2","text":"In your language project's metaborg.yaml file, change your compile dependencies to include org.metaborg:sdf3.ext.statix . For example: dependencies : compile : - org.metaborg:org.metaborg.meta.lang.esv:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.template:${metaborgVersion} - org.metaborg:sdf3.ext.statix:${metaborgVersion} Clean the project and restart Eclipse when changing the metaborg.yaml file. Once you clean your project, the extension automatically generates the following: Statix signatures declarations (in src-gen/statix/signatures/ ) Stratego strategies for explicating and removing injections (in src-gen/injections/ )","title":"Applying the Generator in Spoofax 2"},{"location":"howtos/statix/signature-generator/#using-the-generated-injection-strategies","text":"The generator generates strategies for explicating and removing injections. This is unfortunately needed since Statix does not support injections directly. To use these strategies, import injections/- and call the explicate-injections-MyLang-Start and implicate-injections-MyLang-Start strategies for the analysis pre-processing and post-processing respectively, where MyLang is the name of your language and Start is your language's start symbol (as specified in Syntax.esv ). For example, in trans/analysis.str : module analysis imports libspoofax / sdf / pp statixruntime statix / api injections / - libspoofax / term / origin rules editor-analyze = stx-editor-analyze ( pre-analyze , post-analyze | \"static-semantics\" , \"programOk\" ) pre-analyze = origin-track-forced ( explicate-injections-MyLang-Start ) post-analyze = origin-track-forced ( implicate-injections-MyLang-Start )","title":"Using the Generated Injection strategies"},{"location":"howtos/statix/signature-generator/#using-the-generated-signatures","text":"Using the generated Statix signatures is quite simple: just import them into your Statix specification. Each SDF3 file gets an associated Statix file with the signatures. For example, if your syntax is defined across two files named MyLang.sdf3 and Common.sdf3 , then in Statix you should add the following imports: imports signatures / MyLang-sig signatures / Common-sig Because Statix does not support injections, you have to use explicit constructor names for injections. For example, the following SDF3 syntax: context-free sorts Stmt VarName lexical sorts ID context-free syntax Stmt . VarDecl = < var < VarName >;> VarName . Wildcard = < _ > VarName = ID lexical syntax ID = [ a-zA-Z ] [ a-zA-Z0-9 \\ _ ] * lexical restrictions ID - / - [ a-zA-Z0-9 \\ _ ] would approximately produce the following signatures: module signatures / Test-sig imports signature sorts Stmt VarName ID = string constructors Stmt-Plhdr : Stmt VarName-Plhdr : VarName signature constructors VarDecl : VarName -> Stmt Wildcard : VarName ID2VarName : ID -> VarName Now, in Statix if you just want to capture the term of sort VarName in the VarDecl constructor, this would suffice: VarDecl ( x ) But if you want to match the term only if it has the sort ID , then you have to use the explicit injection constructor name ID2VarName : VarDecl ( ID2VarName ( x )) In this example, ID is a lexical sort, so it is an alias for string in the Statix specification.","title":"Using the Generated Signatures"},{"location":"howtos/statix/signature-generator/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"howtos/statix/signature-generator/#calls-non-existing","text":"Build fails with errors such as this: [ strj | error ] *** (\"is-MyLang-MySort-or-inj\",0,0) calls non-existing (\"is-MyLang-ID-or-inj\",0,0) [ strj | error ] *** (\"explicate-injections-MyLang-MySort\",0,0) calls non-existing (\"explicate-injections-MyLang-ID\",0,0) [ strj | error ] *** (\"implicate-injections-MyLang-MySort\",0,0) calls non-existing (\"implicate-injections-MyLang-ID\",0,0) Executing strj failed: {} Failing builder was required by \"Generate sources\". BUILD FAILED To solve this, ensure you have declared ID (in this example) as a lexical sort in your syntax, and make sure that the syntax file with rules for MySort that reference ID import the syntax file that declares ID .","title":"Calls non-existing"},{"location":"howtos/statix/signature-generator/#transformation-failed-unexpectedly","text":"Clean or build fails with an error such as this: ERROR: Optional sorts are not supported by Statix: Opt(Sort(\"MySort\")) Transformation failed unexpectedly for eclipse:///mylang/syntax/mysyntax.sdf3 org.metaborg.core.transform.TransformException: Invoking Stratego strategy generate-statix failed at term: CfSignature(\"MySort\", Some(\"MyCons\"), [ Param(Opt(Sort(\"MySort\")), \"mySort\") ]) Stratego trace: generate_statix_0_0 generate_statix_abstract_0_0 geninj_generate_statix_0_0 geninj_module_to_sig_0_0 with_1_1 flatfilter_1_0 filter_1_0 with_1_1 <== map_1_0 geninj_symbol_to_stxsig_0_0 Internal error: 'with' clause failed unexpectedly in 'geninj-sig-to-stxsig' Note the first line with ERROR , it tells you that something is not supported. In this case, the use of optional sorts such as MySort? is not supported by Statix and the Statix signature generator. To solve this, rewrite a syntax rule with an optional sort such as: Stmt . VarDecl = << Type ? > < ID > = < Exp >> Into a rule with an explicit sort: Stmt . VarDecl = << Type-OPT > < ID > = < Exp >> Type-OPT . NoType = <> Type-OPT = Type Note that the -OPT suffix has no special meaning. You can name the sort differently, such as OptionalType .","title":"Transformation failed unexpectedly"},{"location":"howtos/statix/signature-generator/#constructor-mysort-plhdr0-not-declared","text":"Build fails with an error such as this: [ strj | error ] in rule explicate-injections-MyLang-MySort(0|0): constructor MySort-Plhdr/0 not declared - MySort-Plhdr() Executing strj failed: {} BUILD FAILED You have declared a sort for which you don't have any syntax rules. Remove the sort from the context-free sorts or sorts block.","title":"Constructor MySort-Plhdr/0 not declared"},{"location":"howtos/statix/signature-generator/#no-pp-entry-found-cannot-rewrite-to-box","text":"Clean fails with an error such as this: [ identity crisis | error ] No pp entry found for: (1,[\"declSortLex\"]) - [ identity crisis | error ] Cannot rewrite to box: - declSortLex(\"MySort\") You are using the old sdf2table : c . Change this in metaborg.yaml into sdf2table : java .","title":"No pp entry found, cannot rewrite to box"},{"location":"howtos/statix/signature-generator/#spt-analysis-tests-calling-stratego-strategies-fail","text":"An SPT test can run an arbitrary Stratego strategy on an analyzed AST and compare the results with the expected AST. If the origin of the is not tracked properly, the root constructor of the resulting analyzed AST will be missing and the comparison will fail. To fix this, ensure the pre-analyze and post-analyze strategies in analysis.str call origin-track-forced : imports libspoofax / term / origin rules pre-analyze = origin-track-forced ( explicate-injections-MyLang-Start ) post-analyze = origin-track-forced ( implicate-injections-MyLang-Start )","title":"SPT analysis tests calling Stratego strategies fail"},{"location":"howtos/stratego/concrete-syntax/","text":"Concrete Syntax \u00b6 When writing language-to-language transformations in Stratego, it is possible to use different approaches, for example, writing AST-to-AST transformations. However, to write such transformations, the language engineer needs to know the constructors of both languages. Moreover, AST nodes in rules that define such transformations may contain many nested children, making the work of writing such rules cumbersome and error-prone. Note that Stratego only statically checks the name and arity of constructors, thus, errors would only be detected when pretty-printing the generated AST to the target language. As an example of this approach, the rule below specifies a transformation of a Calc program to a Java program. program-to-java : Program ( stats ) - > CompilationUnit ( None () , [] , [ ClassDeclaration ( [ Public ()] , Id ( \"Program\" ) , None () , None () , None () , [ MethodDecl ( [ Public (), Static ()] , MethodHeader ( java-type , Id ( \"eval\" ) , NoParams () , [] , None () ) , Block ( [ java-stats * ] ) ) ] ) ] ) with java-type := ... java-stats * := ... An alternative approach consists of using string interpolation. Instead of generating abstract terms of the target language, transformations generate source code directly, interpolating strings with boilerplate code of the target language and variables defined in the transformation itself. The problem with this approach is that syntax errors in the string fragments of the target language are not detected statically. Consider the rule shown previously, rewritten below using string interpolation (the code between $[ and ]). Note that if the fragment would contain a typo, the syntax error would only be detected after the code had been generated. Note also that one can interpolate Stratego variables with the fragment of the target language by escaping them between [ and ]. program-to-java : Program ( stats ) - > $ [ public class Program { public static [ java-type ] eval () { [ java-stats ] } } ] with java-type := ... java-stats := ... The third option is to use concrete syntax. When using concrete syntax, the transformation is still AST-to-AST but the AST of the target language is abstracted over using the concrete syntax of the language instead. That is, the concrete syntax fragment is parsed internally producing an AST, and that AST is resulted from the transformation. The same rule defined using concrete syntax is shown below. Note that any syntax error in the fragment would in fact, be detected by the editor, as the fragment is being parsed internally. Moreover, the fragment also has the syntax highlighting of the target language when shown by the editor. program-to-java : Program ( stats ) - > compilation-unit |[ public class Program { public static ~ type : java-type eval () { ~ bstm * : java-stats * } } ]| with java-type := ... java-stats * := ... There are two aspects to consider when enabling concrete syntax inside Spoofax. The first one is being able to write Stratego transformations with fragments of a target (or source) language. In other words, the first aspect consists of generating a mixed parse table that embeds the desired target language inside Stratego. The second aspect consists of including the parse table inside an Spoofax project, adding an additional .meta file to then enable concrete syntax for a specific Stratego file. Below, we describe both aspects with more detail. Mixing Grammars \u00b6 To generate a mixed parse table that embeds a language inside Stratego, it is necessary to modify the original Stratego grammar, extending it with the desired language. One problem that may occur when combining the grammars of two different languages is name clashing, i.e., non-terminals that have the same name in Stratego and the embedded language. For that reason, the embedding occurs using a modified Stratego grammar, which renames all Stratego context-free non-terminals using by prefixing it with StrategoLang , avoiding name clashes. Inside Spoofax, you can add a source dependency on the org.metaborg:stratego.lang:${metaborgVersion} project in you metaborg.yaml file, then you can import the so-called namespaced grammar. Once you have access to the namespaced grammar, the next step consists of defining the embedding grammar, the grammar that actually mixes the two languages. A grammar that embeds one language into another may contain three types of productions: productions that define quotations for elements of the target language in the host language, productions that define anti-quotations back to the host language from the target language, and variables, which are shortcuts to anti-quotations, and may appear inside the target language fragments. When embedding a language into Stratego, it is common to allow fragments of the host language as Stratego terms. For that reason, quotation productions are injected into Stratego terms. For example, the productions below, written in SDF3, indicates that a Java compilation unit can occur in Stratego in a place where a Stratego term can occur. imports StrategoLang / sugar / terms-namespaced java / packages / CompilationUnits context-free syntax StrategoLang-PreTerm . ToTerm = < compilation-unit |[ < CompilationUnit > ]|> StrategoLang-PreTerm . ToTerm = <|[ < CompilationUnit > ]|> Anti-quotation productions define points to insert elements of the host language inside fragments of the target language. For example, with the production below, we allow Stratego terms to occur in a Java fragment whenever a non-terminal Type can occur. imports java / types / Main context-free syntax Type . FromTerm = [ ~ [ StrategoLang-Term ]] Type . FromTerm = [ ~ type :[ StrategoLang-Term ]] Note that the constructor FromTerm indicates that productions represent anti-quotations. Furthermore, note that anti-quotations may also be named after the non-terminal being referenced (e.g., ~type: ). Using anti-quotations might make the fragment of the target language quite verbose. Therefore, it is also possible to define variables as shortcuts to anti-quotations. For example, the productions below define variables to reference anti-quotations to Type fragments. That is, instead of reference to a Stratego variable X by using ~type:X , one may name this variable t_1 which corresponds to a variable for a non-terminal Type . variables Type = \"t_\" [ 0 -9 \\' ] * { prefer } The prefer annotation indicates that in case of an ambiguity, the variable production should be preferred. Using the three types of productions above, it is possible to specify which fragments one wants to write using concrete syntax and which symbols may appear inside these fragments as Stratego variables (using anti-quotation or variables with a specific name). Finally, it is necessary to define a module Stratego-<LanguageName> that should import the Stratego grammar and the embedding grammar. This module should be defined in a file named Stratego-<LanguageName>.sdf3 and put in the syntax folder so that Spoofax can locate it and build the mixed table. That is, if we define the module for our Stratego-Java mixed grammar: module Stratego-Java imports EmbeddedJava StrategoLang / import-namespaced context-free start-symbols StrategoLang-Module Note that it is necessary to define the start symbol of the mixed grammar as StrategoLang-Module . After defining the embedding grammar and the Stratego-<LanguageName> module, Spoofax generates the mixed table inside the trans folder when rebuilding the project. Using Mixed Parse Tables to Allow Concrete Syntax \u00b6 Assuming a mixed parse table has been successfully generated or already exists, the next step is to allow concrete syntax in transformations using that table. The table needs to be in a folder that can be discovered by the Stratego compiler. By default it will be generated in the trans folder of the project that defines the mixed grammar, to be used within the project. If you wish to use the mixed grammar in another project, make sure to export the generate table by adding an entry for it in the metaborg.yaml file. Then any other project that depends on your mixed grammar project with a source dependency will also provide the table file to the Stratego compiler. Next, together with the file in which we would like to enable concrete syntax, it is necessary to create a .meta file with the same name. That is, to enable concrete syntax in a file generate.str , it is necessary to create, in the same directory, an addition file generate.meta . This file should indicate which mixed table should be used to parse generate.str . For that reason it should contain: Meta ([ Syntax ( \"<ParseTableName>\" )]) where <ParseTableName> is the filename of the parse table without extension, Stratego-Java in our example. With the configuration above, Spoofax automatically detects that the file contains concrete syntax and use that table to parse it. In that file, one may write rules containing concrete syntax as defined by the productions in the mixed grammar.","title":"Concrete Syntax"},{"location":"howtos/stratego/concrete-syntax/#concrete-syntax","text":"When writing language-to-language transformations in Stratego, it is possible to use different approaches, for example, writing AST-to-AST transformations. However, to write such transformations, the language engineer needs to know the constructors of both languages. Moreover, AST nodes in rules that define such transformations may contain many nested children, making the work of writing such rules cumbersome and error-prone. Note that Stratego only statically checks the name and arity of constructors, thus, errors would only be detected when pretty-printing the generated AST to the target language. As an example of this approach, the rule below specifies a transformation of a Calc program to a Java program. program-to-java : Program ( stats ) - > CompilationUnit ( None () , [] , [ ClassDeclaration ( [ Public ()] , Id ( \"Program\" ) , None () , None () , None () , [ MethodDecl ( [ Public (), Static ()] , MethodHeader ( java-type , Id ( \"eval\" ) , NoParams () , [] , None () ) , Block ( [ java-stats * ] ) ) ] ) ] ) with java-type := ... java-stats * := ... An alternative approach consists of using string interpolation. Instead of generating abstract terms of the target language, transformations generate source code directly, interpolating strings with boilerplate code of the target language and variables defined in the transformation itself. The problem with this approach is that syntax errors in the string fragments of the target language are not detected statically. Consider the rule shown previously, rewritten below using string interpolation (the code between $[ and ]). Note that if the fragment would contain a typo, the syntax error would only be detected after the code had been generated. Note also that one can interpolate Stratego variables with the fragment of the target language by escaping them between [ and ]. program-to-java : Program ( stats ) - > $ [ public class Program { public static [ java-type ] eval () { [ java-stats ] } } ] with java-type := ... java-stats := ... The third option is to use concrete syntax. When using concrete syntax, the transformation is still AST-to-AST but the AST of the target language is abstracted over using the concrete syntax of the language instead. That is, the concrete syntax fragment is parsed internally producing an AST, and that AST is resulted from the transformation. The same rule defined using concrete syntax is shown below. Note that any syntax error in the fragment would in fact, be detected by the editor, as the fragment is being parsed internally. Moreover, the fragment also has the syntax highlighting of the target language when shown by the editor. program-to-java : Program ( stats ) - > compilation-unit |[ public class Program { public static ~ type : java-type eval () { ~ bstm * : java-stats * } } ]| with java-type := ... java-stats * := ... There are two aspects to consider when enabling concrete syntax inside Spoofax. The first one is being able to write Stratego transformations with fragments of a target (or source) language. In other words, the first aspect consists of generating a mixed parse table that embeds the desired target language inside Stratego. The second aspect consists of including the parse table inside an Spoofax project, adding an additional .meta file to then enable concrete syntax for a specific Stratego file. Below, we describe both aspects with more detail.","title":"Concrete Syntax"},{"location":"howtos/stratego/concrete-syntax/#mixing-grammars","text":"To generate a mixed parse table that embeds a language inside Stratego, it is necessary to modify the original Stratego grammar, extending it with the desired language. One problem that may occur when combining the grammars of two different languages is name clashing, i.e., non-terminals that have the same name in Stratego and the embedded language. For that reason, the embedding occurs using a modified Stratego grammar, which renames all Stratego context-free non-terminals using by prefixing it with StrategoLang , avoiding name clashes. Inside Spoofax, you can add a source dependency on the org.metaborg:stratego.lang:${metaborgVersion} project in you metaborg.yaml file, then you can import the so-called namespaced grammar. Once you have access to the namespaced grammar, the next step consists of defining the embedding grammar, the grammar that actually mixes the two languages. A grammar that embeds one language into another may contain three types of productions: productions that define quotations for elements of the target language in the host language, productions that define anti-quotations back to the host language from the target language, and variables, which are shortcuts to anti-quotations, and may appear inside the target language fragments. When embedding a language into Stratego, it is common to allow fragments of the host language as Stratego terms. For that reason, quotation productions are injected into Stratego terms. For example, the productions below, written in SDF3, indicates that a Java compilation unit can occur in Stratego in a place where a Stratego term can occur. imports StrategoLang / sugar / terms-namespaced java / packages / CompilationUnits context-free syntax StrategoLang-PreTerm . ToTerm = < compilation-unit |[ < CompilationUnit > ]|> StrategoLang-PreTerm . ToTerm = <|[ < CompilationUnit > ]|> Anti-quotation productions define points to insert elements of the host language inside fragments of the target language. For example, with the production below, we allow Stratego terms to occur in a Java fragment whenever a non-terminal Type can occur. imports java / types / Main context-free syntax Type . FromTerm = [ ~ [ StrategoLang-Term ]] Type . FromTerm = [ ~ type :[ StrategoLang-Term ]] Note that the constructor FromTerm indicates that productions represent anti-quotations. Furthermore, note that anti-quotations may also be named after the non-terminal being referenced (e.g., ~type: ). Using anti-quotations might make the fragment of the target language quite verbose. Therefore, it is also possible to define variables as shortcuts to anti-quotations. For example, the productions below define variables to reference anti-quotations to Type fragments. That is, instead of reference to a Stratego variable X by using ~type:X , one may name this variable t_1 which corresponds to a variable for a non-terminal Type . variables Type = \"t_\" [ 0 -9 \\' ] * { prefer } The prefer annotation indicates that in case of an ambiguity, the variable production should be preferred. Using the three types of productions above, it is possible to specify which fragments one wants to write using concrete syntax and which symbols may appear inside these fragments as Stratego variables (using anti-quotation or variables with a specific name). Finally, it is necessary to define a module Stratego-<LanguageName> that should import the Stratego grammar and the embedding grammar. This module should be defined in a file named Stratego-<LanguageName>.sdf3 and put in the syntax folder so that Spoofax can locate it and build the mixed table. That is, if we define the module for our Stratego-Java mixed grammar: module Stratego-Java imports EmbeddedJava StrategoLang / import-namespaced context-free start-symbols StrategoLang-Module Note that it is necessary to define the start symbol of the mixed grammar as StrategoLang-Module . After defining the embedding grammar and the Stratego-<LanguageName> module, Spoofax generates the mixed table inside the trans folder when rebuilding the project.","title":"Mixing Grammars"},{"location":"howtos/stratego/concrete-syntax/#using-mixed-parse-tables-to-allow-concrete-syntax","text":"Assuming a mixed parse table has been successfully generated or already exists, the next step is to allow concrete syntax in transformations using that table. The table needs to be in a folder that can be discovered by the Stratego compiler. By default it will be generated in the trans folder of the project that defines the mixed grammar, to be used within the project. If you wish to use the mixed grammar in another project, make sure to export the generate table by adding an entry for it in the metaborg.yaml file. Then any other project that depends on your mixed grammar project with a source dependency will also provide the table file to the Stratego compiler. Next, together with the file in which we would like to enable concrete syntax, it is necessary to create a .meta file with the same name. That is, to enable concrete syntax in a file generate.str , it is necessary to create, in the same directory, an addition file generate.meta . This file should indicate which mixed table should be used to parse generate.str . For that reason it should contain: Meta ([ Syntax ( \"<ParseTableName>\" )]) where <ParseTableName> is the filename of the parse table without extension, Stratego-Java in our example. With the configuration above, Spoofax automatically detects that the file contains concrete syntax and use that table to parse it. In that file, one may write rules containing concrete syntax as defined by the productions in the mixed grammar.","title":"Using Mixed Parse Tables to Allow Concrete Syntax"},{"location":"howtos/stratego/debug-stratego/","text":"Debug Stratego Programs \u00b6 Debugging Stratego programs can be frustrating. In strategic programming failure is a first-class citizen and the language supports dynamically typed programming. Thes are useful features for realizing generic and modular programming. However, this may mean that that errors may show up late in the game. A program fails much later then where the error occurs. Often during pretty-printing. Here we discuss some remedies for finding the problem. Signatures \u00b6 signature sorts Exp constructors Add : Exp * Exp - > Exp Define a good signature for your terms. Ideally define the syntax of your language in a syntax definition in SDF3, and declare all sorts explicitly, and avoid injections. This will ensure that a signature and matching pretty-printer are generated automatically, as well as a signature for Statix. Types \u00b6 translate :: Exp - > List ( Instr ) Define type signatures for transformations. Starting with Stratego2, the language supports the definition of type signatures for transformations . This will catch many obvious errors. Use With instead of Where \u00b6 translate : Add ( e1 , e2 ) - > < concat >[ instrs1 , instrs2 , [ Add ()]] with < translate > e1 => instrs1 with < translate > e2 => instrs2 The with clause expresses that you expect a premisse of a rewrite rule to succeed in all cases. When this expectation is violated, the rule will throw an exception and display a stack trace, instead of silently failing. Define SPT tests \u00b6 test translate [[ 1 + 2 ]] run translate Define unit tests in the SPT testing language. Debug \u00b6 dbg (| \"translate/Add: \" ) In case the measures above fail, use dbg to figure out where the error is in your program.","title":"Debug Stratego Programs"},{"location":"howtos/stratego/debug-stratego/#debug-stratego-programs","text":"Debugging Stratego programs can be frustrating. In strategic programming failure is a first-class citizen and the language supports dynamically typed programming. Thes are useful features for realizing generic and modular programming. However, this may mean that that errors may show up late in the game. A program fails much later then where the error occurs. Often during pretty-printing. Here we discuss some remedies for finding the problem.","title":"Debug Stratego Programs"},{"location":"howtos/stratego/debug-stratego/#signatures","text":"signature sorts Exp constructors Add : Exp * Exp - > Exp Define a good signature for your terms. Ideally define the syntax of your language in a syntax definition in SDF3, and declare all sorts explicitly, and avoid injections. This will ensure that a signature and matching pretty-printer are generated automatically, as well as a signature for Statix.","title":"Signatures"},{"location":"howtos/stratego/debug-stratego/#types","text":"translate :: Exp - > List ( Instr ) Define type signatures for transformations. Starting with Stratego2, the language supports the definition of type signatures for transformations . This will catch many obvious errors.","title":"Types"},{"location":"howtos/stratego/debug-stratego/#use-with-instead-of-where","text":"translate : Add ( e1 , e2 ) - > < concat >[ instrs1 , instrs2 , [ Add ()]] with < translate > e1 => instrs1 with < translate > e2 => instrs2 The with clause expresses that you expect a premisse of a rewrite rule to succeed in all cases. When this expectation is violated, the rule will throw an exception and display a stack trace, instead of silently failing.","title":"Use With instead of Where"},{"location":"howtos/stratego/debug-stratego/#define-spt-tests","text":"test translate [[ 1 + 2 ]] run translate Define unit tests in the SPT testing language.","title":"Define SPT tests"},{"location":"howtos/stratego/debug-stratego/#debug","text":"dbg (| \"translate/Add: \" ) In case the measures above fail, use dbg to figure out where the error is in your program.","title":"Debug"},{"location":"howtos/stratego/exchange-terms/","text":"Exchange Terms \u00b6 The term format described above is used in Stratego programs to denote terms, but is also used to exchange terms between programs. Thus, the internal format and the external format exactly coincide. Of course, internally a Stratego program uses a data-structure in memory with pointers rather than manipulating a textual representation of terms. But this is completely hidden from the Stratego programmer.","title":"Exchange Terms"},{"location":"howtos/stratego/exchange-terms/#exchange-terms","text":"The term format described above is used in Stratego programs to denote terms, but is also used to exchange terms between programs. Thus, the internal format and the external format exactly coincide. Of course, internally a Stratego program uses a data-structure in memory with pointers rather than manipulating a textual representation of terms. But this is completely hidden from the Stratego programmer.","title":"Exchange Terms"},{"location":"howtos/stratego/generate-pretty-printer/","text":"How to Generate a Pretty-Printer \u00b6 A pretty-printer is a mapping from abstract syntax trees (terms) to text. The resulting text should observe the syntactic rules Production Templates \u00b6 Generate Pretty-Printer \u00b6 automatically in build generates rules for translation from AST to Box Example. Generate pretty-print rules Use Pretty-Printer \u00b6 Define Builder \u00b6 for interactive use Define Menu Action \u00b6 to invoke the builder","title":"How to Generate a Pretty-Printer"},{"location":"howtos/stratego/generate-pretty-printer/#how-to-generate-a-pretty-printer","text":"A pretty-printer is a mapping from abstract syntax trees (terms) to text. The resulting text should observe the syntactic rules","title":"How to Generate a Pretty-Printer"},{"location":"howtos/stratego/generate-pretty-printer/#production-templates","text":"","title":"Production Templates"},{"location":"howtos/stratego/generate-pretty-printer/#generate-pretty-printer","text":"automatically in build generates rules for translation from AST to Box Example. Generate pretty-print rules","title":"Generate Pretty-Printer"},{"location":"howtos/stratego/generate-pretty-printer/#use-pretty-printer","text":"","title":"Use Pretty-Printer"},{"location":"howtos/stratego/generate-pretty-printer/#define-builder","text":"for interactive use","title":"Define Builder"},{"location":"howtos/stratego/generate-pretty-printer/#define-menu-action","text":"to invoke the builder","title":"Define Menu Action"},{"location":"howtos/stratego/generate-signature/","text":"How to Generate a Stratego Signature \u00b6 It is tedious to write a signature that is in sync with a syntax definition. Therefore, Spoofax automatically generates a signature from a syntax definition for the abstract syntax trees that the parser for that syntax definition produces. Write a Syntax Definition \u00b6 In the /syntax/ directory in your language project for language lang write a syntax definition in file lang.sdf3 . Possibly write additional syntax defininitions modules and import the in lang.sdf3 . Example. Consider the following SDF3 syntax definition: module lang imports Base lexical sorts IntConst lexical syntax IntConst = [ 0 -9 ] + sorts Exp context-free syntax Exp . Int = IntConst Exp . Plus = [[ Exp ] + [ Exp ]] { left } Exp . Minus = [[ Exp ] - [ Exp ]] { left } Exp = [([ Exp ])] { bracket } context-free priorities { left : Exp . Plus Exp . Minus } Generate Signature \u00b6 The build for your language project will invoke the SDF3 compiler to generate a signature file for each SDF3 file in your project in directory /src-gen/signatures/ with suffix -sig and extension .str . The build should be invoked as soon as you save a file. Thus lang.sdf3 generates /src-gen/signatures/lang-sig.str . Example. For the SDF3 file above, the following signature is automatically generated: module signatures / lang-sig imports signatures / Base-sig signature sorts IntConst sorts Exp constructors : string - > IntConst Int : IntConst - > Exp Plus : Exp * Exp - > Exp Minus : Exp * Exp - > Exp IntConst-Plhdr : IntConst Exp-Plhdr : Exp IntConst-Plhdr : COMPLETION-INSERTION - > IntConst Exp-Plhdr : COMPLETION-INSERTION - > Exp The injection from strings into the lexical IntConst sort reflects the fact that tokens are represented as strings in ASTs. The placeholder constructors generated for the sorts are used to represent incomplete programs and syntactic code completion 1 . Use Signature \u00b6 To use the signature, import it into the Stratego module that uses its constructors. Example. To use the signature for the example import it as follows: module desugar imports signatures / lang-sig rules // ... References \u00b6 Luis Eduardo de Souza Amorim, Sebastian Erdweg, Guido Wachsmuth, and Eelco Visser. Principled syntactic code completion using placeholders. In Tijs van der Storm, Emilie Balland, and D\u00e1niel Varr\u00f3, editors, Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering, Amsterdam, The Netherlands, October 31 - November 1, 2016 , 163\u2013175. ACM, 2016. URL: http://dx.doi.org/10.1145/2997364.2997374 , doi:10.1145/2997364.2997374 . \u21a9","title":"How to Generate a Stratego Signature"},{"location":"howtos/stratego/generate-signature/#how-to-generate-a-stratego-signature","text":"It is tedious to write a signature that is in sync with a syntax definition. Therefore, Spoofax automatically generates a signature from a syntax definition for the abstract syntax trees that the parser for that syntax definition produces.","title":"How to Generate a Stratego Signature"},{"location":"howtos/stratego/generate-signature/#write-a-syntax-definition","text":"In the /syntax/ directory in your language project for language lang write a syntax definition in file lang.sdf3 . Possibly write additional syntax defininitions modules and import the in lang.sdf3 . Example. Consider the following SDF3 syntax definition: module lang imports Base lexical sorts IntConst lexical syntax IntConst = [ 0 -9 ] + sorts Exp context-free syntax Exp . Int = IntConst Exp . Plus = [[ Exp ] + [ Exp ]] { left } Exp . Minus = [[ Exp ] - [ Exp ]] { left } Exp = [([ Exp ])] { bracket } context-free priorities { left : Exp . Plus Exp . Minus }","title":"Write a Syntax Definition"},{"location":"howtos/stratego/generate-signature/#generate-signature","text":"The build for your language project will invoke the SDF3 compiler to generate a signature file for each SDF3 file in your project in directory /src-gen/signatures/ with suffix -sig and extension .str . The build should be invoked as soon as you save a file. Thus lang.sdf3 generates /src-gen/signatures/lang-sig.str . Example. For the SDF3 file above, the following signature is automatically generated: module signatures / lang-sig imports signatures / Base-sig signature sorts IntConst sorts Exp constructors : string - > IntConst Int : IntConst - > Exp Plus : Exp * Exp - > Exp Minus : Exp * Exp - > Exp IntConst-Plhdr : IntConst Exp-Plhdr : Exp IntConst-Plhdr : COMPLETION-INSERTION - > IntConst Exp-Plhdr : COMPLETION-INSERTION - > Exp The injection from strings into the lexical IntConst sort reflects the fact that tokens are represented as strings in ASTs. The placeholder constructors generated for the sorts are used to represent incomplete programs and syntactic code completion 1 .","title":"Generate Signature"},{"location":"howtos/stratego/generate-signature/#use-signature","text":"To use the signature, import it into the Stratego module that uses its constructors. Example. To use the signature for the example import it as follows: module desugar imports signatures / lang-sig rules // ...","title":"Use Signature"},{"location":"howtos/stratego/generate-signature/#references","text":"Luis Eduardo de Souza Amorim, Sebastian Erdweg, Guido Wachsmuth, and Eelco Visser. Principled syntactic code completion using placeholders. In Tijs van der Storm, Emilie Balland, and D\u00e1niel Varr\u00f3, editors, Proceedings of the 2016 ACM SIGPLAN International Conference on Software Language Engineering, Amsterdam, The Netherlands, October 31 - November 1, 2016 , 163\u2013175. ACM, 2016. URL: http://dx.doi.org/10.1145/2997364.2997374 , doi:10.1145/2997364.2997374 . \u21a9","title":"References"},{"location":"howtos/stratego/inspect-terms/","text":"Inspect Terms \u00b6 As a Stratego programmer you will be looking a lot at raw ATerms. Stratego pioneers did this by opening an ATerm file in emacs and trying to get a sense of the structure by parenthesis highlighting and inserting newlines here and there. These days your life is much more pleasant through pretty-printing ATerms, which adds layout to a term to make it readable. For example, parsing the following program let function fact(n : int) : int = if n < 1 then 1 else (n * fact(n - 1)) in printint(fact(10)) end produces the following ATerm: Let([FunDecs([FunDec(\"fact\",[FArg(\"n\",Tp(Tid(\"int\")))],Tp(Tid(\"int\")), If(Lt(Var(\"n\"),Int(\"1\")),Int(\"1\"),Seq([Times(Var(\"n\"),Call(Var(\"fact\"), [Minus(Var(\"n\"),Int(\"1\"))]))])))])],[Call(Var(\"printint\"),[Call(Var( \"fact\"),[Int(\"10\")])])]) By pretty-printing the term we get a much more readable term: Let( [ FunDecs( [ FunDec( \"fact\" , [FArg(\"n\", Tp(Tid(\"int\")))] , Tp(Tid(\"int\")) , If( Lt(Var(\"n\"), Int(\"1\")) , Int(\"1\") , Seq([ Times(Var(\"n\"), Call(Var(\"fact\"), [Minus(Var(\"n\"), Int(\"1\"))])) ]) ) ) ] ) ] , [ Call(Var(\"printint\"), [Call(Var(\"fact\"), [Int(\"10\")])]) ] ) In Spoofax/Eclipse, you will find that in some contexts ATerms are automatically pretty-printed, whereas in others they are simply printed linearly. However, you can obtain assistance with perceiving the structure of any ATerm by writing it into a file with the \".aterm\" extension and opening it in the Spoofax Editor in Eclipse. On the right there will be a convenient Outline Navigator which allows you to select any node in the ATerm and see the entire subtree below it highlighted in the editor.","title":"Inspect Terms"},{"location":"howtos/stratego/inspect-terms/#inspect-terms","text":"As a Stratego programmer you will be looking a lot at raw ATerms. Stratego pioneers did this by opening an ATerm file in emacs and trying to get a sense of the structure by parenthesis highlighting and inserting newlines here and there. These days your life is much more pleasant through pretty-printing ATerms, which adds layout to a term to make it readable. For example, parsing the following program let function fact(n : int) : int = if n < 1 then 1 else (n * fact(n - 1)) in printint(fact(10)) end produces the following ATerm: Let([FunDecs([FunDec(\"fact\",[FArg(\"n\",Tp(Tid(\"int\")))],Tp(Tid(\"int\")), If(Lt(Var(\"n\"),Int(\"1\")),Int(\"1\"),Seq([Times(Var(\"n\"),Call(Var(\"fact\"), [Minus(Var(\"n\"),Int(\"1\"))]))])))])],[Call(Var(\"printint\"),[Call(Var( \"fact\"),[Int(\"10\")])])]) By pretty-printing the term we get a much more readable term: Let( [ FunDecs( [ FunDec( \"fact\" , [FArg(\"n\", Tp(Tid(\"int\")))] , Tp(Tid(\"int\")) , If( Lt(Var(\"n\"), Int(\"1\")) , Int(\"1\") , Seq([ Times(Var(\"n\"), Call(Var(\"fact\"), [Minus(Var(\"n\"), Int(\"1\"))])) ]) ) ) ] ) ] , [ Call(Var(\"printint\"), [Call(Var(\"fact\"), [Int(\"10\")])]) ] ) In Spoofax/Eclipse, you will find that in some contexts ATerms are automatically pretty-printed, whereas in others they are simply printed linearly. However, you can obtain assistance with perceiving the structure of any ATerm by writing it into a file with the \".aterm\" extension and opening it in the Spoofax Editor in Eclipse. On the right there will be a convenient Outline Navigator which allows you to select any node in the ATerm and see the entire subtree below it highlighted in the editor.","title":"Inspect Terms"},{"location":"howtos/stratego/run-stratego-programs/","text":"How to Run Stratego Programs \u00b6 Stratego programs that are part of a language project in Spoofax/Eclipse are run by creating a menu entry that invokes a strategy in your program. Extend Spoofax Menu \u00b6 module $ModuleName menus menu : \"$Menu\" ( openeditor ) action : \"$MenuEntry\" = $Id Add a new ESV file in the editor/ directory of your project or use an existing one. In the menus section, create a menu with an appropriate name. Adding the attribute openeditor ensures that the result of running the program will be opened in an editor. Add an action entry to the menu with the name of the menu entry and the name of the builder strategy to invoke. This should add a menu entry Spoofax > Menu > Action to the editor for your language. Define Builder \u00b6 $ Id : ( node , _ , _ , path , project-path ) - > ( filename , result ) with filename := < guarantee-extension (| $ Extension )> path with result := < $ Strategy > node In your Stratego program, probably in some top-level file in the trans/ directory of your project, add a 'builder' rewrite rule. Such a rewrite rule defines the interface between the user-interface (action menu entry) and your program. A builder has the interface shown above. When invoking the builder, Spoofax takes care of parsing the program and converting it to abstract syntax term. It takes a quintuple of the selected AST node , the entire ast , the file path , and the project-path and returns a pair of the filename and result . The results are computed in the conditions of the builder rule. The new filename is typically derived from the old file name in path The result is computed by invoking a strategy on the selected node or on the entire ast . Example: Parser \u00b6 module Syntax //... menus menu : \"Syntax\" ( openeditor ) action : \"Show parsed AST\" = debug-show-aterm ( source ) debug-show-aterm : ( node , _ , _ , path , project-path ) - > ( filename , result ) with filename := < guarantee-extension (| \"aterm\" )> path ; result := node The debug-show-aterm provided with new Spoofax projects returns the selected node as result. Since Spoofax ensures that the content of the editor is parsed, this returns the AST of the editor content as a (pretty-printed) term. Example: Term Builder \u00b6 module Compilation menus menu : \"Compilation\" ( openeditor ) action : \"Desugar (AST)\" = desugar-aterm rules desugar-aterm : ( node , _ , _ , path , project-path ) - > ( filename , result ) with filename := < guarantee-extension (| \"d.aterm\" )> path with result := < desugar > node The Desugar (AST) menu action calls the desugar-aterm builder, which in turn uses the desugar strategy to transform the selected node . The result is returned in a file with extension d.aterm . Example: Pretty Printing Builder \u00b6 module Compilation menus menu : \"Compilation\" ( openeditor ) action : \"Desugar\" = desugar-pp rules desugar-pp : ( node , _ , _ , path , project-path ) - > ( filename , result ) with filename := < guarantee-extension (| \"d.tig\" )> path with result := < desugar ; pp-tiger-string > node The Desugar menu action calls the desugar-pp builder. That strategy transforms the selected node with desugar and pretty-prints the resulting term with pp-tiger-string . The result is returned in a file with extension d.tig . Define SPT Tests \u00b6 An alternative way to run transformations is to test them using SPT tests. This allows you to systematically run a transformation on a number of typical cases. Run On Save \u00b6 When a Stratego program is applied in production, a transformation can be applied automatically whenever a program in your language is saved.","title":"How to Run Stratego Programs"},{"location":"howtos/stratego/run-stratego-programs/#how-to-run-stratego-programs","text":"Stratego programs that are part of a language project in Spoofax/Eclipse are run by creating a menu entry that invokes a strategy in your program.","title":"How to Run Stratego Programs"},{"location":"howtos/stratego/run-stratego-programs/#extend-spoofax-menu","text":"module $ModuleName menus menu : \"$Menu\" ( openeditor ) action : \"$MenuEntry\" = $Id Add a new ESV file in the editor/ directory of your project or use an existing one. In the menus section, create a menu with an appropriate name. Adding the attribute openeditor ensures that the result of running the program will be opened in an editor. Add an action entry to the menu with the name of the menu entry and the name of the builder strategy to invoke. This should add a menu entry Spoofax > Menu > Action to the editor for your language.","title":"Extend Spoofax Menu"},{"location":"howtos/stratego/run-stratego-programs/#define-builder","text":"$ Id : ( node , _ , _ , path , project-path ) - > ( filename , result ) with filename := < guarantee-extension (| $ Extension )> path with result := < $ Strategy > node In your Stratego program, probably in some top-level file in the trans/ directory of your project, add a 'builder' rewrite rule. Such a rewrite rule defines the interface between the user-interface (action menu entry) and your program. A builder has the interface shown above. When invoking the builder, Spoofax takes care of parsing the program and converting it to abstract syntax term. It takes a quintuple of the selected AST node , the entire ast , the file path , and the project-path and returns a pair of the filename and result . The results are computed in the conditions of the builder rule. The new filename is typically derived from the old file name in path The result is computed by invoking a strategy on the selected node or on the entire ast .","title":"Define Builder"},{"location":"howtos/stratego/run-stratego-programs/#example-parser","text":"module Syntax //... menus menu : \"Syntax\" ( openeditor ) action : \"Show parsed AST\" = debug-show-aterm ( source ) debug-show-aterm : ( node , _ , _ , path , project-path ) - > ( filename , result ) with filename := < guarantee-extension (| \"aterm\" )> path ; result := node The debug-show-aterm provided with new Spoofax projects returns the selected node as result. Since Spoofax ensures that the content of the editor is parsed, this returns the AST of the editor content as a (pretty-printed) term.","title":"Example: Parser"},{"location":"howtos/stratego/run-stratego-programs/#example-term-builder","text":"module Compilation menus menu : \"Compilation\" ( openeditor ) action : \"Desugar (AST)\" = desugar-aterm rules desugar-aterm : ( node , _ , _ , path , project-path ) - > ( filename , result ) with filename := < guarantee-extension (| \"d.aterm\" )> path with result := < desugar > node The Desugar (AST) menu action calls the desugar-aterm builder, which in turn uses the desugar strategy to transform the selected node . The result is returned in a file with extension d.aterm .","title":"Example: Term Builder"},{"location":"howtos/stratego/run-stratego-programs/#example-pretty-printing-builder","text":"module Compilation menus menu : \"Compilation\" ( openeditor ) action : \"Desugar\" = desugar-pp rules desugar-pp : ( node , _ , _ , path , project-path ) - > ( filename , result ) with filename := < guarantee-extension (| \"d.tig\" )> path with result := < desugar ; pp-tiger-string > node The Desugar menu action calls the desugar-pp builder. That strategy transforms the selected node with desugar and pretty-prints the resulting term with pp-tiger-string . The result is returned in a file with extension d.tig .","title":"Example: Pretty Printing Builder"},{"location":"howtos/stratego/run-stratego-programs/#define-spt-tests","text":"An alternative way to run transformations is to test them using SPT tests. This allows you to systematically run a transformation on a number of typical cases.","title":"Define SPT Tests"},{"location":"howtos/stratego/run-stratego-programs/#run-on-save","text":"When a Stratego program is applied in production, a transformation can be applied automatically whenever a program in your language is saved.","title":"Run On Save"},{"location":"howtos/stratego/stratego-1-to-2/","text":"Migrating from Stratego 1 to Stratego 2 \u00b6 Stratego 2 is somewhat unstable Stratego 2 is quite new and has a lot of exciting new things going for it. But it is therefore also more unstable, with errors popping up somewhat regularly. Your project build may break or your code may behave in unexpected ways. Of course you can file bug reports . Stratego 2 is the new version of Stratego that provides access to the incremental compiler and gradual type system that were developed for Stratego. Stratego 2 is accessible separately because it is organised quite differently from Stratego 1 and it provides a clear distinction and documentable upgrade path. This is that documented upgrade path. This How-To will guide you through the changes you need to make in your Spoofax project in order to use Stratego 2. Make sure you run the development version of Spoofax. This step will become obsolete in the future but currently Stratego 2 development is moving quickly and fixing bugs every week. In your metaborg.yaml file: Add a compile dependency ( dependencies.compile ) on org.metaborg:stratego.lang:${metaborgVersion} (the Stratego 2 language) Add a source dependency on org.metaborg:strategolib:${metaborgVersion} (the Stratego 2 version of strategolib, the standard library) Remove from language.stratego.args the -la and stratego-lib lines (2 lines, leave the other -la ) Remove language.stratego.build if there, it is now ignored, all compilation will be incremental Remove language.stratego.format if there, it is now ignored, the compilation is always to jar. If the format option you remove is ctree , also search your .esv files for a line provider : target/metaborg/stratego.ctree , likely in editor/Main.esv , and remove it. Rename all .str files in your project that are not in src-gen to .str2 . Generated Stratego files in src-gen should already have a .str2 version next to the .str version of the file. Remove any imports to libstratego-lib or libstrategolib in those renamed files. Add the strategolib import to all your .str2 files outside of src-gen . Stratego versions The version numbers of Stratego are a little strange, Stratego and Stratego/XT used to number up to 0.17, then did not receive any more numbered releases even though small bugfixes and changes were released through Spoofax 1 and 2. In the current documentation we now consider this post-0.17 Stratego in Spoofax to be Stratego 1. This is not necessarily a statement of stability and matureness of the language but more to distinguish it from the new Stratego 2 project. Imports in Stratego 2 \u00b6 At this point your project may be buildable again, but perhaps you are still getting errors about unresolved strategies or constructors. Stratego 2 has a stricter import policy than Stratego 1. If you use a strategy, rule or constructor, you must either define that strategy/rule/constructor in the module or import a module that defines it. Imports are no longer transitive for name resolution.","title":"Migrating from Stratego 1 to Stratego 2"},{"location":"howtos/stratego/stratego-1-to-2/#migrating-from-stratego-1-to-stratego-2","text":"Stratego 2 is somewhat unstable Stratego 2 is quite new and has a lot of exciting new things going for it. But it is therefore also more unstable, with errors popping up somewhat regularly. Your project build may break or your code may behave in unexpected ways. Of course you can file bug reports . Stratego 2 is the new version of Stratego that provides access to the incremental compiler and gradual type system that were developed for Stratego. Stratego 2 is accessible separately because it is organised quite differently from Stratego 1 and it provides a clear distinction and documentable upgrade path. This is that documented upgrade path. This How-To will guide you through the changes you need to make in your Spoofax project in order to use Stratego 2. Make sure you run the development version of Spoofax. This step will become obsolete in the future but currently Stratego 2 development is moving quickly and fixing bugs every week. In your metaborg.yaml file: Add a compile dependency ( dependencies.compile ) on org.metaborg:stratego.lang:${metaborgVersion} (the Stratego 2 language) Add a source dependency on org.metaborg:strategolib:${metaborgVersion} (the Stratego 2 version of strategolib, the standard library) Remove from language.stratego.args the -la and stratego-lib lines (2 lines, leave the other -la ) Remove language.stratego.build if there, it is now ignored, all compilation will be incremental Remove language.stratego.format if there, it is now ignored, the compilation is always to jar. If the format option you remove is ctree , also search your .esv files for a line provider : target/metaborg/stratego.ctree , likely in editor/Main.esv , and remove it. Rename all .str files in your project that are not in src-gen to .str2 . Generated Stratego files in src-gen should already have a .str2 version next to the .str version of the file. Remove any imports to libstratego-lib or libstrategolib in those renamed files. Add the strategolib import to all your .str2 files outside of src-gen . Stratego versions The version numbers of Stratego are a little strange, Stratego and Stratego/XT used to number up to 0.17, then did not receive any more numbered releases even though small bugfixes and changes were released through Spoofax 1 and 2. In the current documentation we now consider this post-0.17 Stratego in Spoofax to be Stratego 1. This is not necessarily a statement of stability and matureness of the language but more to distinguish it from the new Stratego 2 project.","title":"Migrating from Stratego 1 to Stratego 2"},{"location":"howtos/stratego/stratego-1-to-2/#imports-in-stratego-2","text":"At this point your project may be buildable again, but perhaps you are still getting errors about unresolved strategies or constructors. Stratego 2 has a stricter import policy than Stratego 1. If you use a strategy, rule or constructor, you must either define that strategy/rule/constructor in the module or import a module that defines it. Imports are no longer transitive for name resolution.","title":"Imports in Stratego 2"},{"location":"references/","text":"References \u00b6 This are the Spoofax and meta-language references. For more background information on the ideas, architecture, and design decisions behind Spoofax and its meta-languages, see the Background section. The reference section should explain language constructs (syntax, statics, dynamics) and be structured following a taxonomy of the language Table of Contents \u00b6 SDF3 (Jasper) Statix (Aron) FlowSpec (Matthijs, Jeff) Stratego (Eelco, Jeff) PIE (Ivo, Gabri\u00ebl) MkDocs (Dani\u00ebl) bibtex syntax highlighting ESV / editor services Reviewing Peter Toine","title":"References"},{"location":"references/#references","text":"This are the Spoofax and meta-language references. For more background information on the ideas, architecture, and design decisions behind Spoofax and its meta-languages, see the Background section. The reference section should explain language constructs (syntax, statics, dynamics) and be structured following a taxonomy of the language","title":"References"},{"location":"references/#table-of-contents","text":"SDF3 (Jasper) Statix (Aron) FlowSpec (Matthijs, Jeff) Stratego (Eelco, Jeff) PIE (Ivo, Gabri\u00ebl) MkDocs (Dani\u00ebl) bibtex syntax highlighting ESV / editor services Reviewing Peter Toine","title":"Table of Contents"},{"location":"references/config/","text":"Language Configuration \u00b6 metaborg.yaml","title":"Language Configuration"},{"location":"references/config/#language-configuration","text":"metaborg.yaml","title":"Language Configuration"},{"location":"references/editor-services/","text":"Editor Services \u00b6 Most editor services are configured in an ESV file . This way the following editor services can be defined: Action Menus Analysis File Extensions Hover Tooltips On-Save Handlers Outline View Parsing Reference Resolution Stratego Strategies Syntax Highlighting Additionally, the following editor services are configured in a different way: Rename Refactoring","title":"Editor Services"},{"location":"references/editor-services/#editor-services","text":"Most editor services are configured in an ESV file . This way the following editor services can be defined: Action Menus Analysis File Extensions Hover Tooltips On-Save Handlers Outline View Parsing Reference Resolution Stratego Strategies Syntax Highlighting Additionally, the following editor services are configured in a different way: Rename Refactoring","title":"Editor Services"},{"location":"references/editor-services/analysis/","text":"Analysis \u00b6 The analyzer strategy is used to perform static analyses such as name and type analysis, on the AST that a parser produces. An analysis context provides a project-wide store to facilitate multi-file analysis and incrementality. There are four ways to configure the analysis, which set the analyzer strategy with the observer and context keys in an ESV file . language context : $Context observer : $Strategy No Analysis \u00b6 To completely disable analysis, do not set an observer and set the context to none: language context : none Stratego \u00b6 Stratego-based analysis allows you to implement your analysis in Stratego: language context : legacy observer : editor-analyze The identifier after the colon refers to the Stratego strategy that performs the analysis. It must take as input a 3-tuple (ast, path, projectPath) . As output it must produce a 4-tuple (ast, error*, warning*, note*) . The following Stratego code is an example of a strategy that implements this signature: editor-analyze : ( ast , path , projectPath ) - > ( ast ' , errors , warnings , notes ) with ast ' := < analyze > ast ; errors := < collect-all ( check-error )> ast ' ; warnings := < collect-all ( check-warning )> ast ' ; notes := < collect-all ( check-note )> ast ' Statix \u00b6 To use Statix as the meta-language for name and type analysis, use the editor-analyze strategy defined in trans/analysis.str , annotate it with the (constraint) modifier, and set no context: language observer : editor-analyze ( constraint ) By default, the Statix analyzer works in single-file mode and does not consider multi-file name resolution. To enable that, add the (multifile) modifier: language observer : editor-analyze ( constraint ) ( multifile )","title":"Analysis"},{"location":"references/editor-services/analysis/#analysis","text":"The analyzer strategy is used to perform static analyses such as name and type analysis, on the AST that a parser produces. An analysis context provides a project-wide store to facilitate multi-file analysis and incrementality. There are four ways to configure the analysis, which set the analyzer strategy with the observer and context keys in an ESV file . language context : $Context observer : $Strategy","title":"Analysis"},{"location":"references/editor-services/analysis/#no-analysis","text":"To completely disable analysis, do not set an observer and set the context to none: language context : none","title":"No Analysis"},{"location":"references/editor-services/analysis/#stratego","text":"Stratego-based analysis allows you to implement your analysis in Stratego: language context : legacy observer : editor-analyze The identifier after the colon refers to the Stratego strategy that performs the analysis. It must take as input a 3-tuple (ast, path, projectPath) . As output it must produce a 4-tuple (ast, error*, warning*, note*) . The following Stratego code is an example of a strategy that implements this signature: editor-analyze : ( ast , path , projectPath ) - > ( ast ' , errors , warnings , notes ) with ast ' := < analyze > ast ; errors := < collect-all ( check-error )> ast ' ; warnings := < collect-all ( check-warning )> ast ' ; notes := < collect-all ( check-note )> ast '","title":"Stratego"},{"location":"references/editor-services/analysis/#statix","text":"To use Statix as the meta-language for name and type analysis, use the editor-analyze strategy defined in trans/analysis.str , annotate it with the (constraint) modifier, and set no context: language observer : editor-analyze ( constraint ) By default, the Statix analyzer works in single-file mode and does not consider multi-file name resolution. To enable that, add the (multifile) modifier: language observer : editor-analyze ( constraint ) ( multifile )","title":"Statix"},{"location":"references/editor-services/esv/","text":"ESV \u00b6 The Editor Service (ESV) language is a declarative meta-language for configuring the editor services of a language. For example, the following ESV code fragment configures the syntax highlighting for a language, based on the types of tokens: module color colorer keyword : 153 51 153 identifier : black string : 177 47 2 number : 17 131 22 operator : black layout : 63 127 95 italic Structure \u00b6 ESV files end with the .esv extension, and are by convention placed in the editor/ folder of a language project. Each ESV file defines a module for the file, followed by import statements and then the main configuration sections. Each section consists of a number of keys and values. Main File By convention, the main ESV file of a language project must live at editor/Main.esv (default) or editor/main.esv . Other ESV files can be (transitively) imported from the main ESV file. Module Definition \u00b6 An ESV file starts with a module definition at the top of the file: module $ModuleName The module name is the filename of the ESV file without the exttension, and relative to the editor/ directory. For example, the module editor/mylang/Syntax.esv would have the following module name: module mylang/Syntax Module names can only contains the alphanumeric characters and dash, underscore, and period, and use the forward slash ( / ) as the path separator. Module names cannot be in parent directories, so ../Syntax is not allowed. Imports \u00b6 The imports section is an optional section immediately following the module definition. When specified it is given as: imports $Imports For example, to import editor/Syntax.esv and editor/Analysis.esv : imports Syntax Analysis Imports are transitive. At most one imports section is permitted. When specified, the imports section cannot be empty. Configuration Sections \u00b6 The main body of an ESV file consists of any number of configuration sections. An example of a configuration section is: language line comment : \"//\" block comment : \"/*\" \"*/\" The configuration sections are hard-coded in the ESV language, but mostly use a consistent syntax for the keys and values. The following configuration sections are currently defined: colorer Syntax Highlighting language Language File Extensions Parsing Analysis On-Save Handlers Stratego Strategies menus Action menus references Hover Tooltips Reference Resolutions views Outline View The following sections have been deprecated: analysis builders completions folding outliner refactorings","title":"ESV"},{"location":"references/editor-services/esv/#esv","text":"The Editor Service (ESV) language is a declarative meta-language for configuring the editor services of a language. For example, the following ESV code fragment configures the syntax highlighting for a language, based on the types of tokens: module color colorer keyword : 153 51 153 identifier : black string : 177 47 2 number : 17 131 22 operator : black layout : 63 127 95 italic","title":"ESV"},{"location":"references/editor-services/esv/#structure","text":"ESV files end with the .esv extension, and are by convention placed in the editor/ folder of a language project. Each ESV file defines a module for the file, followed by import statements and then the main configuration sections. Each section consists of a number of keys and values. Main File By convention, the main ESV file of a language project must live at editor/Main.esv (default) or editor/main.esv . Other ESV files can be (transitively) imported from the main ESV file.","title":"Structure"},{"location":"references/editor-services/esv/#module-definition","text":"An ESV file starts with a module definition at the top of the file: module $ModuleName The module name is the filename of the ESV file without the exttension, and relative to the editor/ directory. For example, the module editor/mylang/Syntax.esv would have the following module name: module mylang/Syntax Module names can only contains the alphanumeric characters and dash, underscore, and period, and use the forward slash ( / ) as the path separator. Module names cannot be in parent directories, so ../Syntax is not allowed.","title":"Module Definition"},{"location":"references/editor-services/esv/#imports","text":"The imports section is an optional section immediately following the module definition. When specified it is given as: imports $Imports For example, to import editor/Syntax.esv and editor/Analysis.esv : imports Syntax Analysis Imports are transitive. At most one imports section is permitted. When specified, the imports section cannot be empty.","title":"Imports"},{"location":"references/editor-services/esv/#configuration-sections","text":"The main body of an ESV file consists of any number of configuration sections. An example of a configuration section is: language line comment : \"//\" block comment : \"/*\" \"*/\" The configuration sections are hard-coded in the ESV language, but mostly use a consistent syntax for the keys and values. The following configuration sections are currently defined: colorer Syntax Highlighting language Language File Extensions Parsing Analysis On-Save Handlers Stratego Strategies menus Action menus references Hover Tooltips Reference Resolutions views Outline View The following sections have been deprecated: analysis builders completions folding outliner refactorings","title":"Configuration Sections"},{"location":"references/editor-services/file-extensions/","text":"Language File Extensions \u00b6 The file extensions that the editor should recognize as files belonging to the language definition, are configured in the language section extensions key of an ESV file . They are specified without a leading dot: language extensions : ent Multiple extensions can be set with a comma-separated list: language extensions : ent , entity , entities This will assign for example foo.ent , foo.entity , and foo.entities to the language.","title":"Language File Extensions"},{"location":"references/editor-services/file-extensions/#language-file-extensions","text":"The file extensions that the editor should recognize as files belonging to the language definition, are configured in the language section extensions key of an ESV file . They are specified without a leading dot: language extensions : ent Multiple extensions can be set with a comma-separated list: language extensions : ent , entity , entities This will assign for example foo.ent , foo.entity , and foo.entities to the language.","title":"Language File Extensions"},{"location":"references/editor-services/hover/","text":"Hover Tooltips \u00b6 Hover tooltips show a textual tooltip with extra information, when hovering part of the text. Hover tooltips are created by a Stratego strategy, but are configured in an ESV file under the references section: references hover _ : $Strategy For example: references hover _ : editor-hover The identifier after the colon refers to the Stratego strategy that creates the hover tooltip. The Stratego strategy takes an AST node, and either fails if no tooltip should be produced, or returns a tooltip string. The string may contain a few simple HTML tag to style the output. The following tags are supported: <br/> \u2014 line break <b>text</b> \u2014 bold <i>text</i> \u2014 italic <pre>code</pre> \u2014 preformatted (code) text Unrecognized HTML tags are stripped from the hover tooltip. Escape angled brackets and ampersands to show them verbatim in the tooltip.","title":"Hover Tooltips"},{"location":"references/editor-services/hover/#hover-tooltips","text":"Hover tooltips show a textual tooltip with extra information, when hovering part of the text. Hover tooltips are created by a Stratego strategy, but are configured in an ESV file under the references section: references hover _ : $Strategy For example: references hover _ : editor-hover The identifier after the colon refers to the Stratego strategy that creates the hover tooltip. The Stratego strategy takes an AST node, and either fails if no tooltip should be produced, or returns a tooltip string. The string may contain a few simple HTML tag to style the output. The following tags are supported: <br/> \u2014 line break <b>text</b> \u2014 bold <i>text</i> \u2014 italic <pre>code</pre> \u2014 preformatted (code) text Unrecognized HTML tags are stripped from the hover tooltip. Escape angled brackets and ampersands to show them verbatim in the tooltip.","title":"Hover Tooltips"},{"location":"references/editor-services/menus/","text":"Action Menus \u00b6 Menus are used to bind actions of your language, such as transformations, to a menu in the IDE. Menus are defined using the menu keyword under a menus section in an ESV file , and can themselves contain submenus, actions, and separators. menu : $String $MenuOptions $MenuContribs Menu Contributions \u00b6 A menu has zero or more $MenuContrib , which are: action , submenu , or separator . Actions \u00b6 Actions (sometimes called builders ) are defined under a menu or submenu with syntax: action : $String = $StrategoCall $MenuOptions Submenus \u00b6 Submenus allow grouping of actions in nested menus. Their syntax is: sub menu : $String $MenuOptions $MenuContribs end Separators \u00b6 Separators allow inserting a separator in a menu list using the syntax: separator Menu Options \u00b6 The menu options specify the behavior of the menu item. The following modifiers are supported: Modifier Description (source) Action is performed on the parsed AST instead of the default analyzed AST. (openeditor) The result should be opened in a new editor. (realtime) (meta) Example \u00b6 An example menu: menus menu : \"Generate\" action : \"To normal form\" = to-normal-form ( source ) sub menu : \"To Java\" action : \"Abstract\" = to-java-abstract ( openeditor ) action : \"Concrete\" = to-java-concrete end","title":"Action Menus"},{"location":"references/editor-services/menus/#action-menus","text":"Menus are used to bind actions of your language, such as transformations, to a menu in the IDE. Menus are defined using the menu keyword under a menus section in an ESV file , and can themselves contain submenus, actions, and separators. menu : $String $MenuOptions $MenuContribs","title":"Action Menus"},{"location":"references/editor-services/menus/#menu-contributions","text":"A menu has zero or more $MenuContrib , which are: action , submenu , or separator .","title":"Menu Contributions"},{"location":"references/editor-services/menus/#actions","text":"Actions (sometimes called builders ) are defined under a menu or submenu with syntax: action : $String = $StrategoCall $MenuOptions","title":"Actions"},{"location":"references/editor-services/menus/#submenus","text":"Submenus allow grouping of actions in nested menus. Their syntax is: sub menu : $String $MenuOptions $MenuContribs end","title":"Submenus"},{"location":"references/editor-services/menus/#separators","text":"Separators allow inserting a separator in a menu list using the syntax: separator","title":"Separators"},{"location":"references/editor-services/menus/#menu-options","text":"The menu options specify the behavior of the menu item. The following modifiers are supported: Modifier Description (source) Action is performed on the parsed AST instead of the default analyzed AST. (openeditor) The result should be opened in a new editor. (realtime) (meta)","title":"Menu Options"},{"location":"references/editor-services/menus/#example","text":"An example menu: menus menu : \"Generate\" action : \"To normal form\" = to-normal-form ( source ) sub menu : \"To Java\" action : \"Abstract\" = to-java-abstract ( openeditor ) action : \"Concrete\" = to-java-concrete end","title":"Example"},{"location":"references/editor-services/on-save/","text":"On-Save Handlers \u00b6 The on-save handler (also known as the compiler strategy) is used to transform files when they are saved in an editor. In an IDE, when a new project is opened, the compiler strategy is also executed on each file in the project, as well as when files change in the background. In a command-line batch compiler setting, it is used to transform all files. The compiler strategy is configured in an ESV file with the on save key: language on save : $Strategy The identifier after the colon refers to the Stratego strategy that performs the transformation. This strategy must have the exact same signature as the one for actions . For example: language on save : compile-file","title":"On-Save Handlers"},{"location":"references/editor-services/on-save/#on-save-handlers","text":"The on-save handler (also known as the compiler strategy) is used to transform files when they are saved in an editor. In an IDE, when a new project is opened, the compiler strategy is also executed on each file in the project, as well as when files change in the background. In a command-line batch compiler setting, it is used to transform all files. The compiler strategy is configured in an ESV file with the on save key: language on save : $Strategy The identifier after the colon refers to the Stratego strategy that performs the transformation. This strategy must have the exact same signature as the one for actions . For example: language on save : compile-file","title":"On-Save Handlers"},{"location":"references/editor-services/outline/","text":"Outline View \u00b6 An outline is a summary of the structure of a file, shown in a separate view next to a textual editor. An outline is created by a Stratego strategy, but is configured in an ESV file under the views section: views outline view : $Strategy expand to level : $Int The Stratego strategy specified as $Strategy must have the following signature: signature constructors Node : Label * Children - > Node rules editor-outline : ( node , position , ast , path , project-path ) - > outline Where the input is the default tuple used for builders , and the result is a list of Node terms, each carrying a label and a (possibly empty) list of child nodes. Preserve origins on the node's label to allow navigating to the corresponding code from the outline. For example: views outline view : editor-outline expand to level : 3 This configures the editor-outline Stratego strategy to be used to create outlines, and that outline nodes should be expanded 3 levels deep by default.","title":"Outline View"},{"location":"references/editor-services/outline/#outline-view","text":"An outline is a summary of the structure of a file, shown in a separate view next to a textual editor. An outline is created by a Stratego strategy, but is configured in an ESV file under the views section: views outline view : $Strategy expand to level : $Int The Stratego strategy specified as $Strategy must have the following signature: signature constructors Node : Label * Children - > Node rules editor-outline : ( node , position , ast , path , project-path ) - > outline Where the input is the default tuple used for builders , and the result is a list of Node terms, each carrying a label and a (possibly empty) list of child nodes. Preserve origins on the node's label to allow navigating to the corresponding code from the outline. For example: views outline view : editor-outline expand to level : 3 This configures the editor-outline Stratego strategy to be used to create outlines, and that outline nodes should be expanded 3 levels deep by default.","title":"Outline View"},{"location":"references/editor-services/parsing/","text":"Parsing \u00b6 Parsing language files in an editor is configured in the language section of an ESV file . The syntax is as follows: language table : $Path start symbols : $Sorts line comment : $String block comment : $String * $String fences : $Fences For example: language table : target/metaborg/sdf . tbl start symbols : File line comment : \"//\" block comment : \"/*\" * \"*/\" fences : [ ] ( ) { } Parse Table \u00b6 The parse table of your language is set with the table key. By default, the parse table of an SDF specification is always produced at target/metaborg/sdf.tbl . It is only necessary to change this configuration when a custom parse table is used. Start Symbols \u00b6 The start symbols key determine which start symbols to use when an editor is opened. This must be a subset of the start symbols defined in the SDF3 specification of your language. Multiple start symbols can be set with a comma-separated list: language start symbols : Start , Program Comments \u00b6 The syntax for comments is: language line comment : $String block comment : $String * $String For example, Java comments are specified as: language line comment : \"//\" block comment : \"/*\" * \"*/\" The line comment key determines how single-line comments are created. It is used by editors to toggle the comment for a single line. For example, in Eclipse, pressing Ctrl + / ( Cmd + / on macOS), respectively comments or uncomments the line. The block comment key determines how multi-line comments are created. It is used when a whole block needs to be commented or uncommented. A block comment is described by the two strings denoting the start and end symbols of the block comment respectively. Fences \u00b6 Fences for bracket matching are set as follows: language fences : $Fences The fences key determines which symbols to use and match for bracket matching. A single fence is defined by a starting and closing symbol. Multiple fences can be set with a space-separated list. Fences are used to do bracket matching in text editors. For example, the default fences in a new Spoofax language project are: language fences : [ ] ( ) { } Multi-Character Fences Fences can contain multiple characters, but some implementations may not handle bracket matching with multiple fence characters. For example, Eclipse does not handle this case and ignores multi-character fences.","title":"Parsing"},{"location":"references/editor-services/parsing/#parsing","text":"Parsing language files in an editor is configured in the language section of an ESV file . The syntax is as follows: language table : $Path start symbols : $Sorts line comment : $String block comment : $String * $String fences : $Fences For example: language table : target/metaborg/sdf . tbl start symbols : File line comment : \"//\" block comment : \"/*\" * \"*/\" fences : [ ] ( ) { }","title":"Parsing"},{"location":"references/editor-services/parsing/#parse-table","text":"The parse table of your language is set with the table key. By default, the parse table of an SDF specification is always produced at target/metaborg/sdf.tbl . It is only necessary to change this configuration when a custom parse table is used.","title":"Parse Table"},{"location":"references/editor-services/parsing/#start-symbols","text":"The start symbols key determine which start symbols to use when an editor is opened. This must be a subset of the start symbols defined in the SDF3 specification of your language. Multiple start symbols can be set with a comma-separated list: language start symbols : Start , Program","title":"Start Symbols"},{"location":"references/editor-services/parsing/#comments","text":"The syntax for comments is: language line comment : $String block comment : $String * $String For example, Java comments are specified as: language line comment : \"//\" block comment : \"/*\" * \"*/\" The line comment key determines how single-line comments are created. It is used by editors to toggle the comment for a single line. For example, in Eclipse, pressing Ctrl + / ( Cmd + / on macOS), respectively comments or uncomments the line. The block comment key determines how multi-line comments are created. It is used when a whole block needs to be commented or uncommented. A block comment is described by the two strings denoting the start and end symbols of the block comment respectively.","title":"Comments"},{"location":"references/editor-services/parsing/#fences","text":"Fences for bracket matching are set as follows: language fences : $Fences The fences key determines which symbols to use and match for bracket matching. A single fence is defined by a starting and closing symbol. Multiple fences can be set with a space-separated list. Fences are used to do bracket matching in text editors. For example, the default fences in a new Spoofax language project are: language fences : [ ] ( ) { } Multi-Character Fences Fences can contain multiple characters, but some implementations may not handle bracket matching with multiple fence characters. For example, Eclipse does not handle this case and ignores multi-character fences.","title":"Fences"},{"location":"references/editor-services/reference-resolution/","text":"Reference Resolution \u00b6 Reference resolution takes an AST node containing a reference, and tries to resolve it to its definition. The resolution is performed by a Stratego strategy, but is configured in an ESV file under the references section: references reference _ : $Strategy The identifier after the colon refers to the Stratego strategy that performs the resolution. The Stratego strategy takes an AST node, and either fails if it could not be resolved, or returns an AST node that has an origin location pointing to the definition site. For example: references reference _ : editor-resolve","title":"Reference Resolution"},{"location":"references/editor-services/reference-resolution/#reference-resolution","text":"Reference resolution takes an AST node containing a reference, and tries to resolve it to its definition. The resolution is performed by a Stratego strategy, but is configured in an ESV file under the references section: references reference _ : $Strategy The identifier after the colon refers to the Stratego strategy that performs the resolution. The Stratego strategy takes an AST node, and either fails if it could not be resolved, or returns an AST node that has an origin location pointing to the definition site. For example: references reference _ : editor-resolve","title":"Reference Resolution"},{"location":"references/editor-services/renaming/","text":"Rename Refactoring \u00b6 Spoofax provides an automated rename refactoring as an editor service for every language developed with it that has the static semantics defined with Statix or NaBL2. Strategy \u00b6 Rename refactoring is enabled by default for new Spoofax language projects. This works by registering the rename-action strategy from the statixruntime library as an action in a menu. This strategy takes three parameters: a layout-preserving pretty-printing strategy ( construct-textual-change by default), the editor analyze strategy ( editor-analyze by default), and a strategy that should succeed when renaming in multi-file mode. The default rename refactoring strategy looks like this: rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , fail ) To enable multi-file mode, change the last argument to id : rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , id ) Statix \u00b6 For the renaming to work correctly in all cases when using Statix, terms that represent a declaration of a program entity, such as a function or a variable, need to set the @decl property on the name of the entity. For example, when declaring a type: declareType ( scope , name , T ) : - ! type [ name , T ] in scope , @ name . decl := name , query type filter P * I * and { name' : - name' == name } in scope |-> [ _ ] . Known Issues \u00b6 When a parse tree of a name is preceded by a term which is parsed from an empty string. The renaming algorithm will incorrectly select the preceding term to be renamed, and mostly fail accordingly. Sometimes, this issue can be circumvented by selecting the complete surrounding term. This issue is known to occur for: Statix predicate names. See Also \u00b6 How-To: Add Rename Refactoring to an Existing Project","title":"Rename Refactoring"},{"location":"references/editor-services/renaming/#rename-refactoring","text":"Spoofax provides an automated rename refactoring as an editor service for every language developed with it that has the static semantics defined with Statix or NaBL2.","title":"Rename Refactoring"},{"location":"references/editor-services/renaming/#strategy","text":"Rename refactoring is enabled by default for new Spoofax language projects. This works by registering the rename-action strategy from the statixruntime library as an action in a menu. This strategy takes three parameters: a layout-preserving pretty-printing strategy ( construct-textual-change by default), the editor analyze strategy ( editor-analyze by default), and a strategy that should succeed when renaming in multi-file mode. The default rename refactoring strategy looks like this: rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , fail ) To enable multi-file mode, change the last argument to id : rules rename-menu-action = rename-action ( construct-textual-change , editor-analyze , id )","title":"Strategy"},{"location":"references/editor-services/renaming/#statix","text":"For the renaming to work correctly in all cases when using Statix, terms that represent a declaration of a program entity, such as a function or a variable, need to set the @decl property on the name of the entity. For example, when declaring a type: declareType ( scope , name , T ) : - ! type [ name , T ] in scope , @ name . decl := name , query type filter P * I * and { name' : - name' == name } in scope |-> [ _ ] .","title":"Statix"},{"location":"references/editor-services/renaming/#known-issues","text":"When a parse tree of a name is preceded by a term which is parsed from an empty string. The renaming algorithm will incorrectly select the preceding term to be renamed, and mostly fail accordingly. Sometimes, this issue can be circumvented by selecting the complete surrounding term. This issue is known to occur for: Statix predicate names.","title":"Known Issues"},{"location":"references/editor-services/renaming/#see-also","text":"How-To: Add Rename Refactoring to an Existing Project","title":"See Also"},{"location":"references/editor-services/stratego/","text":"Stratego \u00b6 The Java JAR and CTree files that will be loaded into the Stratego runtime for your language can be configured with the provider key in an ESV file : language provider : $Path The path is a path to a .jar or .ctree file, relative to the root of the project. For example: language provider : target/metaborg/stratego . ctree The extension of the provider should match the format in the metaborg.yaml file of your language. Multiple files can be set by setting the key multiple times: language provider : target/metaborg/stratego . ctree provider : target/custom1 . jar provider : target/custom2 . ctree","title":"Stratego"},{"location":"references/editor-services/stratego/#stratego","text":"The Java JAR and CTree files that will be loaded into the Stratego runtime for your language can be configured with the provider key in an ESV file : language provider : $Path The path is a path to a .jar or .ctree file, relative to the root of the project. For example: language provider : target/metaborg/stratego . ctree The extension of the provider should match the format in the metaborg.yaml file of your language. Multiple files can be set by setting the key multiple times: language provider : target/metaborg/stratego . ctree provider : target/custom1 . jar provider : target/custom2 . ctree","title":"Stratego"},{"location":"references/editor-services/syntax-highlighting/","text":"Syntax Highlighting \u00b6 Token-based syntax highlighting is configured in a colorer section of an ESV file . Such a section can contain style definitions and styling rules. Style Definitions \u00b6 Style definitions bind an identifier to a style for later reuse, using the syntax: $ID = $Style Styles \u00b6 A style specifies a combination of a foreground color, optional background color, and optional font style. Colors are specified as Red-Green-Blue values ranging from 0 (none) to 255 (full). The possible font attributes are: Font attribute Description (none) Normal font. bold Bold font. italic Italic font. bold italic Bond and italic font. italic bold Same as bold italic . For example, the following style definitions bind the red , green , and blue colors: colorer red = 255 0 0 green = 0 255 0 blue = 0 0 255 An optional background color can be set by adding another RGB value: colorer redWithGreenBackground = 255 0 0 0 255 0 The font attributes can be used to make the font bold or italic: colorer redWithBold = 255 0 0 bold redWithItalic = 255 0 0 italic redWithGreenBackgroundWithBoldItalic = 255 0 0 0 255 0 bold italic Style Rules \u00b6 Style rules assign a style to matched tokens with syntax: $Matcher : $Style Or assigns a previously defined style definition: $Matcher : $Ref The left hand side of style rules matches a token, whereas the right hand side assigns a style by referring to a previously defined style definition, or by directly assigning a style. For example, the following matches a token type and references a style definition: colorer operator : black whereas the following matches a token with a sort and constructor, and directly assigns a style: colorer ClassBodyDec . MethodDec : 0 255 0 Matchers \u00b6 There are several ways in which the matcher on the left-hand side of a style rule can be specified: by type, by sort, by constructor, or by sort and constructor. Match by Sort and Constructor \u00b6 The combination of a token sort and constructor can be matched by specifying the $Sort.$Constructor . For example: colorer ClassBodyDec.MethodDec : yellow ClassBodyDec.FieldDec : red Match by Constructor \u00b6 It is also possible to match constructors, regardless of their token sorts, using _ in place of the sort name. For example: colorer _ . Str : blue _ . StrCong : blue _ . QStr : blue _ . QDollar : blue _ . QBr : gray Match by Sort \u00b6 Additionally, it is possible to match any constructor for a specific sort. For this, just specify the name of the sort, $Sort . For example: colorer ID : darkblue TYPEID : blue JQTYPEID : blue PQTYPEID : blue FUNCID : 153 51 0 JFUNCID : 153 51 0 STRING : 177 47 2 Match by Type \u00b6 Finally, the following built-in token types can be matched on: identifier \u2014 matches identifiers, found by lexical non-terminals without numbers; keyword \u2014 matches keywords, found by terminals in the syntax definition; layout \u2014 matches layout, such as whitespace and comments, found by layout definition; number \u2014 matches numbers, found by lexical non-terminals with numbers; operator \u2014 matches operations, found by terminals that contain just symbols (no characters); string \u2014 matches strings, found by lexical non-terminals that include quotation marks; unknown \u2014 matches tokens which the parser was unable to infer a type for. var error For example, the following code defines a simple highlighting with token types: colorer keyword : 153 51 153 identifier : black string : 177 47 2 number : 17 131 22 operator : black layout : 63 127 95 italic","title":"Syntax Highlighting"},{"location":"references/editor-services/syntax-highlighting/#syntax-highlighting","text":"Token-based syntax highlighting is configured in a colorer section of an ESV file . Such a section can contain style definitions and styling rules.","title":"Syntax Highlighting"},{"location":"references/editor-services/syntax-highlighting/#style-definitions","text":"Style definitions bind an identifier to a style for later reuse, using the syntax: $ID = $Style","title":"Style Definitions"},{"location":"references/editor-services/syntax-highlighting/#styles","text":"A style specifies a combination of a foreground color, optional background color, and optional font style. Colors are specified as Red-Green-Blue values ranging from 0 (none) to 255 (full). The possible font attributes are: Font attribute Description (none) Normal font. bold Bold font. italic Italic font. bold italic Bond and italic font. italic bold Same as bold italic . For example, the following style definitions bind the red , green , and blue colors: colorer red = 255 0 0 green = 0 255 0 blue = 0 0 255 An optional background color can be set by adding another RGB value: colorer redWithGreenBackground = 255 0 0 0 255 0 The font attributes can be used to make the font bold or italic: colorer redWithBold = 255 0 0 bold redWithItalic = 255 0 0 italic redWithGreenBackgroundWithBoldItalic = 255 0 0 0 255 0 bold italic","title":"Styles"},{"location":"references/editor-services/syntax-highlighting/#style-rules","text":"Style rules assign a style to matched tokens with syntax: $Matcher : $Style Or assigns a previously defined style definition: $Matcher : $Ref The left hand side of style rules matches a token, whereas the right hand side assigns a style by referring to a previously defined style definition, or by directly assigning a style. For example, the following matches a token type and references a style definition: colorer operator : black whereas the following matches a token with a sort and constructor, and directly assigns a style: colorer ClassBodyDec . MethodDec : 0 255 0","title":"Style Rules"},{"location":"references/editor-services/syntax-highlighting/#matchers","text":"There are several ways in which the matcher on the left-hand side of a style rule can be specified: by type, by sort, by constructor, or by sort and constructor.","title":"Matchers"},{"location":"references/editor-services/syntax-highlighting/#match-by-sort-and-constructor","text":"The combination of a token sort and constructor can be matched by specifying the $Sort.$Constructor . For example: colorer ClassBodyDec.MethodDec : yellow ClassBodyDec.FieldDec : red","title":"Match by Sort and Constructor"},{"location":"references/editor-services/syntax-highlighting/#match-by-constructor","text":"It is also possible to match constructors, regardless of their token sorts, using _ in place of the sort name. For example: colorer _ . Str : blue _ . StrCong : blue _ . QStr : blue _ . QDollar : blue _ . QBr : gray","title":"Match by Constructor"},{"location":"references/editor-services/syntax-highlighting/#match-by-sort","text":"Additionally, it is possible to match any constructor for a specific sort. For this, just specify the name of the sort, $Sort . For example: colorer ID : darkblue TYPEID : blue JQTYPEID : blue PQTYPEID : blue FUNCID : 153 51 0 JFUNCID : 153 51 0 STRING : 177 47 2","title":"Match by Sort"},{"location":"references/editor-services/syntax-highlighting/#match-by-type","text":"Finally, the following built-in token types can be matched on: identifier \u2014 matches identifiers, found by lexical non-terminals without numbers; keyword \u2014 matches keywords, found by terminals in the syntax definition; layout \u2014 matches layout, such as whitespace and comments, found by layout definition; number \u2014 matches numbers, found by lexical non-terminals with numbers; operator \u2014 matches operations, found by terminals that contain just symbols (no characters); string \u2014 matches strings, found by lexical non-terminals that include quotation marks; unknown \u2014 matches tokens which the parser was unable to infer a type for. var error For example, the following code defines a simple highlighting with token types: colorer keyword : 153 51 153 identifier : black string : 177 47 2 number : 17 131 22 operator : black layout : 63 127 95 italic","title":"Match by Type"},{"location":"references/flowspec/","text":"FlowSpec \u00b6 Programs that are syntactically well-formed are not necessarily valid programs. Programming languages typically impose additional context-sensitive requirements on programs that cannot be captured in a syntax definition. Languages use data and control flow to check certain extra properties that fall outside of names and type systems. The FlowSpec \u2018Flow Analysis Specification Language\u2019 supports the specification of rules to define the static control flow of a language, and data flow analysis over that control flow. FlowSpec supports flow-sensitive intra-procedural data flow analysis. Control Flow Graphs \u00b6 Control-flow represents the execution order of a program. Depending on the input given to the program, or other things the program may observe of its execution environment (e.g. network communication, or a source of noise used to generate pseudo-random numbers), a program may execute a different trace of instructions. Since in general programs may not terminate at all, and humans are not very adapt at reasoning about possible infinities, we use a finite representation of possibly infinite program traces using control-flow graphs. Control-flow graphs are similarly finite as program text and are usually very similar, giving rise to a visual representation of the program. Loops in the program are represented as cycles in the control-flow graph, conditional code is represented by a split in control-flow which is merged again automatically after the conditional code. Data Flow Analysis over Control Flow Graphs \u00b6 Data-flow analysis propagates information either forward or backward along the control-flow graph. This can be information that approximates the data that is handled by the program, or the way in which the program interacts with memory, or something else altogether. Examples of data-flow analysis include constant analysis which checks when variables that are used in the program are guaranteed to have the same value regardless of the execution circumstances of the program, or live variables analysis which identifies if values in variables are actually observable by the program.","title":"Index"},{"location":"references/flowspec/#flowspec","text":"Programs that are syntactically well-formed are not necessarily valid programs. Programming languages typically impose additional context-sensitive requirements on programs that cannot be captured in a syntax definition. Languages use data and control flow to check certain extra properties that fall outside of names and type systems. The FlowSpec \u2018Flow Analysis Specification Language\u2019 supports the specification of rules to define the static control flow of a language, and data flow analysis over that control flow. FlowSpec supports flow-sensitive intra-procedural data flow analysis.","title":"FlowSpec"},{"location":"references/flowspec/#control-flow-graphs","text":"Control-flow represents the execution order of a program. Depending on the input given to the program, or other things the program may observe of its execution environment (e.g. network communication, or a source of noise used to generate pseudo-random numbers), a program may execute a different trace of instructions. Since in general programs may not terminate at all, and humans are not very adapt at reasoning about possible infinities, we use a finite representation of possibly infinite program traces using control-flow graphs. Control-flow graphs are similarly finite as program text and are usually very similar, giving rise to a visual representation of the program. Loops in the program are represented as cycles in the control-flow graph, conditional code is represented by a split in control-flow which is merged again automatically after the conditional code.","title":"Control Flow Graphs"},{"location":"references/flowspec/#data-flow-analysis-over-control-flow-graphs","text":"Data-flow analysis propagates information either forward or backward along the control-flow graph. This can be information that approximates the data that is handled by the program, or the way in which the program interacts with memory, or something else altogether. Examples of data-flow analysis include constant analysis which checks when variables that are used in the program are guaranteed to have the same value regardless of the execution circumstances of the program, or live variables analysis which identifies if values in variables are actually observable by the program.","title":"Data Flow Analysis over Control Flow Graphs"},{"location":"references/flowspec/Stratego_API/","text":"Setup \u00b6 Using the Stratego API requires a dependency on the FlowSpec Stratego code (source dependency), and an import of flowspec/api . Example. A Stratego module importing the FlowSpec API. module example imports flowspec / api Running the analysis \u00b6 There are strategies to integrate FlowSpec analysis in the NaBL2 analysis, and strategies for doing both NaBL2 analysis and FlowSpec analysis on an AST. Integrated into NaBL2 analysis \u00b6 These can be used in the final phase of the NaBL2 analysis process using the Stratego hooks . /** * Analyze the given AST with FlowSpec. * The FlowSpec analysis is added to given NaBL2 analysis result and returned. * * @param analysis:Analysis * @param propnames:String or List(String) * @type ast:Term -> Analysis */ flowspec-analyze (| analysis ) /** * Analyze the given AST with FlowSpec, but only the given FlowSpec properties. * The FlowSpec analysis is added to given NaBL2 analysis result and returned. * * @param analysis:Analysis * @param propnames:String or List(String) * @type ast:Term -> Analysis */ flowspec-analyze (| analysis , propnames ) The analysis results are also usable at that point for generating editor messages. Integration with NaBL2 is done by giving the FlowSpec analysis result as the \u201ccustom final analysis result\u201d: nabl2-custom-analysis-unit-hook : ( resource , ast , custom-initial-result ) - > ( resource , ast ) nabl2-custom-analysis-final-hook (| a ): ( resource , custom-initial-result , custom-unit-results ) - > ( errors , warnings , notes , custom-final-result ) with asts := < map ( \\ ( ast-resource , ast ) - > < nabl2--index-ast (| ast-resource )> ast \\ )> custom-unit-results ; // workaround for https://yellowgrass.org/issue/NaBL2/54 custom-final-result := < flowspec-analyze (| a )> asts ; errors := ... ; warnings := ... ; notes := ... This propagates the AST of each unit from the unit phase, and analyzes all of them together in the final phase. The custom-final-result is returned so that NaBL2 preserves it for later usage. FlowSpec provides convenience functions that request the custom final result again later: Running the analysis manually \u00b6 Sometimes you need data-flow analysis between transformations which change the program. That means you need to run the analysis just before a transformation to have analysis results corresponding to the current program. The following strategies execute the analysis and help with consuming the resulting tuple. /** * Analyze the given AST with NaBL2 and FlowSpec * * @param resource:String * @type ast:Term -> (ast:Term, Analysis, errors:List(EditorMessage), warnings:List(EditorMessage), notes:List(EditorMessage)) */ flowspec-analyze-ast (| resource ) /** * Analyze the given AST with NaBL2 and FlowSpec. * Transform the AST with pre before the FlowSpec analysis, and with post after the FlowSpec analysis. * * @param pre:Term -> Term * @param post:Term -> Term * @param resource:String * @type ast:Term -> (ast:Term, Analysis, errors:List(EditorMessage), warnings:List(EditorMessage), notes:List(EditorMessage)) */ flowspec-analyze-ast ( pre , post | resource ) /** * Analyze the given AST with NaBL2 and FlowSpec, but only the given FlowSpec properties. * * @param resource:String * @param propnames:String or List(String) * @type ast:Term -> (ast:Term, Analysis, errors:List(EditorMessage), warnings:List(EditorMessage), notes:List(EditorMessage)) */ flowspec-analyze-ast (| resource , propname ) /** * Analyze the given AST with NaBL2 and FlowSpec, but only the given FlowSpec properties. * Transform the AST with pre before the FlowSpec analysis, and with post after the FlowSpec analysis. * * @param pre:Term -> Term * @param post:Term -> Term * @param resource:String * @param propnames:String or List(String) * @type ast:Term -> (ast:Term, Analysis, errors:List(EditorMessage), warnings:List(EditorMessage), notes:List(EditorMessage)) */ flowspec-analyze-ast ( pre , post | resource , propnames ) /** * Take the analyze-ast 5-tuple output and return the result of applying the given strategy to the AST. * Note that the strategy takes the analysis object as a term argument. * * @param s(|Analysis): Term -> Term * @type ast: (ast:Term, Analysis, errors:List(EditorMessage), warnings:List(EditorMessage), notes:List(EditorMessage)) -> Term */ flowspec-then ( s ) /** * Analyze the given AST with NaBL2 and FlowSpec, but only the given FlowSpec properties. * Then return the result of applying the given strategy to the AST. * Note that the strategy takes the analysis object as a term argument. * * @param s(|Analysis): Term -> Term * @param resource:String * @param propnames:String or List(String) * @type ast:Term -> Term */ flowspec-analyze-ast-then ( s | resource , propnames ) /** * Analyze the given AST with NaBL2 and FlowSpec, but only the given FlowSpec properties. * Transform the AST with pre before the FlowSpec analysis, and with post after the FlowSpec analysis. * Then return the result of applying the given strategy to the AST. * Note that the strategy takes the analysis object as a term argument. * * @param pre:Term -> Term * @param post:Term -> Term * @param s(|Analysis): Term -> Term * @param resource:String * @param propnames:String or List(String) * @type ast:Term -> Term */ flowspec-analyze-ast-then ( pre , post , s | resource , propnames ) Querying analysis \u00b6 The NaBL2 API defines several strategies to get an analysis term by resource name or from an AST node . This analysis term can then be passed to the querying strategies that give access to the data flow properties, if you hooked FlowSpec into the NaBL2 analysis process. The other way to get the analysis term is to execute the analysis with the flowspec-analyze-ast* variants. Control-flow graph \u00b6 There are a number of strategies to get the control-flow graph nodes associated with an AST fragment, as well as control-flow graph navigation strategies and AST search strategies to get back to the AST from a control-flow graph node. Note that querying the control-flow graph is cheap but finding the way back from the control-flow graph to the AST is more expensive. /** * Get the control flow graph node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-node (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-start-node (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-end-node (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-entry-node (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-exit-node (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-prev-nodes (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-next-nodes (| a ) /** * Find AST node corresponding to the CFGNode back again * * @param ast : Term * @type node:CFGNode -> Term */ flowspec-cfg-node-ast (| ast ) /** * Find AST node corresponding to the CFGNode back again * * @param ast : Term * @type pos:Position -> Term */ flowspec-pos-ast (| ast ) /** * Find parent of AST node corresponding to the CFGNode back again by matching the parent with * the parent argument and giving back the child that is likely to be a match to the CFG node. * * @param parent : Term -> Term * @param ast : Term * @type node:CFGNode -> Term */ flowspec-cfg-node-ast ( parent | ast ) /** * Find parent of AST node corresponding to the CFGNode back again by matching the parent with * the parent argument and giving back the child that is likely to be a match to the CFG node. * * @param parent : Term -> Term * @param ast : Term * @type pos:Position -> Term */ flowspec-pos-ast ( parent | ast ) /** * Get the position of an AST node. * * @type Term -> Position */ flowspec-get-position Data flow properties \u00b6 FlowSpec properties can be read in two versions, pre and post. These indicate whether the effect of the cfg node has been applied yet. Whether or not it is applied depends on the direction of the analysis. pre for a forward analysis is without the effect of the node, but pre for a backward analysis includes the effect of the node. Note that each strategy can simply take the term that\u2019s associated with the control-flow graph node. But the control-flow graph node itself is also an accepted input. /** * Get the property of the control flow graph node associated with * the given term. The value returned is the value of the property * _before_ the effect of the control flow graph node. * * @param a : Analysis * @param prop : String * @type term:Term -> Term */ flowspec-get-property-pre (| a , propname ) /** * Get the property of the control flow graph node associated with * the given term. The value returned is the value of the property * _after_ the effect of the control flow graph node. * * @param a : Analysis * @param prop : String * @type term:Term -> Term */ flowspec-get-property-post (| a , propname ) /** * Get the property of the control flow graph node associated with * the given term. The value returned is the value of the property * _after_ the effect of the control flow graph node. If no node * is found the exit control flow graph node of the AST node is * queried for its post-effect property value. * * @param a : Analysis * @param prop : String * @type term:Term -> Term */ flowspec-get-property-post-or-exit-post (| analysis-result , analysis-name ) FlowSpec data helpers \u00b6 FlowSpec sets and maps are passed back to Stratego as lists wrapped in Set and Map constructors. As a convenience, the most common operations are lifted and added to the flowspec API: /** * Check if a FlowSpec Set contains an element. Succeeds if the given strategy succeeds for at * least one element. * * @param s: Term -?> * @type FlowSpecSet -?> FlowSpecSet */ flowspec-set-contains ( s ) /** * Look up elements in a FlowSpec Set of pairs. Returns the right elements of all pairs where * the given strategy succeeds on the left element. * * @param s: Term -?> * @type FlowSpecSet -?> List(Term) */ flowspec-set-lookup ( s ) /** * Look up a key in a FlowSpec Map. Returns the element if the given key exists in the map. * * @param k: Term * @type FlowSpecMap -?> Term */ flowspec-map-lookup (| k ) Hover text \u00b6 For a hover implementation that displays name, type and FlowSpec properties use: /** * Provides a strategy for a hover message with as much information as possible about name, type * (from NaBl2) and FlowSpec properties. */ flowspec-editor-hover ( language-pp ) Profiling information \u00b6 /** * If flowspec-debug-profile is extended to succeed, some timing information will be printed in * stderr when using flowspec-analyze*. */ flowspec-debug-profile","title":"Stratego API"},{"location":"references/flowspec/Stratego_API/#setup","text":"Using the Stratego API requires a dependency on the FlowSpec Stratego code (source dependency), and an import of flowspec/api . Example. A Stratego module importing the FlowSpec API. module example imports flowspec / api","title":"Setup"},{"location":"references/flowspec/Stratego_API/#running-the-analysis","text":"There are strategies to integrate FlowSpec analysis in the NaBL2 analysis, and strategies for doing both NaBL2 analysis and FlowSpec analysis on an AST.","title":"Running the analysis"},{"location":"references/flowspec/Stratego_API/#integrated-into-nabl2-analysis","text":"These can be used in the final phase of the NaBL2 analysis process using the Stratego hooks . /** * Analyze the given AST with FlowSpec. * The FlowSpec analysis is added to given NaBL2 analysis result and returned. * * @param analysis:Analysis * @param propnames:String or List(String) * @type ast:Term -> Analysis */ flowspec-analyze (| analysis ) /** * Analyze the given AST with FlowSpec, but only the given FlowSpec properties. * The FlowSpec analysis is added to given NaBL2 analysis result and returned. * * @param analysis:Analysis * @param propnames:String or List(String) * @type ast:Term -> Analysis */ flowspec-analyze (| analysis , propnames ) The analysis results are also usable at that point for generating editor messages. Integration with NaBL2 is done by giving the FlowSpec analysis result as the \u201ccustom final analysis result\u201d: nabl2-custom-analysis-unit-hook : ( resource , ast , custom-initial-result ) - > ( resource , ast ) nabl2-custom-analysis-final-hook (| a ): ( resource , custom-initial-result , custom-unit-results ) - > ( errors , warnings , notes , custom-final-result ) with asts := < map ( \\ ( ast-resource , ast ) - > < nabl2--index-ast (| ast-resource )> ast \\ )> custom-unit-results ; // workaround for https://yellowgrass.org/issue/NaBL2/54 custom-final-result := < flowspec-analyze (| a )> asts ; errors := ... ; warnings := ... ; notes := ... This propagates the AST of each unit from the unit phase, and analyzes all of them together in the final phase. The custom-final-result is returned so that NaBL2 preserves it for later usage. FlowSpec provides convenience functions that request the custom final result again later:","title":"Integrated into NaBL2 analysis"},{"location":"references/flowspec/Stratego_API/#running-the-analysis-manually","text":"Sometimes you need data-flow analysis between transformations which change the program. That means you need to run the analysis just before a transformation to have analysis results corresponding to the current program. The following strategies execute the analysis and help with consuming the resulting tuple. /** * Analyze the given AST with NaBL2 and FlowSpec * * @param resource:String * @type ast:Term -> (ast:Term, Analysis, errors:List(EditorMessage), warnings:List(EditorMessage), notes:List(EditorMessage)) */ flowspec-analyze-ast (| resource ) /** * Analyze the given AST with NaBL2 and FlowSpec. * Transform the AST with pre before the FlowSpec analysis, and with post after the FlowSpec analysis. * * @param pre:Term -> Term * @param post:Term -> Term * @param resource:String * @type ast:Term -> (ast:Term, Analysis, errors:List(EditorMessage), warnings:List(EditorMessage), notes:List(EditorMessage)) */ flowspec-analyze-ast ( pre , post | resource ) /** * Analyze the given AST with NaBL2 and FlowSpec, but only the given FlowSpec properties. * * @param resource:String * @param propnames:String or List(String) * @type ast:Term -> (ast:Term, Analysis, errors:List(EditorMessage), warnings:List(EditorMessage), notes:List(EditorMessage)) */ flowspec-analyze-ast (| resource , propname ) /** * Analyze the given AST with NaBL2 and FlowSpec, but only the given FlowSpec properties. * Transform the AST with pre before the FlowSpec analysis, and with post after the FlowSpec analysis. * * @param pre:Term -> Term * @param post:Term -> Term * @param resource:String * @param propnames:String or List(String) * @type ast:Term -> (ast:Term, Analysis, errors:List(EditorMessage), warnings:List(EditorMessage), notes:List(EditorMessage)) */ flowspec-analyze-ast ( pre , post | resource , propnames ) /** * Take the analyze-ast 5-tuple output and return the result of applying the given strategy to the AST. * Note that the strategy takes the analysis object as a term argument. * * @param s(|Analysis): Term -> Term * @type ast: (ast:Term, Analysis, errors:List(EditorMessage), warnings:List(EditorMessage), notes:List(EditorMessage)) -> Term */ flowspec-then ( s ) /** * Analyze the given AST with NaBL2 and FlowSpec, but only the given FlowSpec properties. * Then return the result of applying the given strategy to the AST. * Note that the strategy takes the analysis object as a term argument. * * @param s(|Analysis): Term -> Term * @param resource:String * @param propnames:String or List(String) * @type ast:Term -> Term */ flowspec-analyze-ast-then ( s | resource , propnames ) /** * Analyze the given AST with NaBL2 and FlowSpec, but only the given FlowSpec properties. * Transform the AST with pre before the FlowSpec analysis, and with post after the FlowSpec analysis. * Then return the result of applying the given strategy to the AST. * Note that the strategy takes the analysis object as a term argument. * * @param pre:Term -> Term * @param post:Term -> Term * @param s(|Analysis): Term -> Term * @param resource:String * @param propnames:String or List(String) * @type ast:Term -> Term */ flowspec-analyze-ast-then ( pre , post , s | resource , propnames )","title":"Running the analysis manually"},{"location":"references/flowspec/Stratego_API/#querying-analysis","text":"The NaBL2 API defines several strategies to get an analysis term by resource name or from an AST node . This analysis term can then be passed to the querying strategies that give access to the data flow properties, if you hooked FlowSpec into the NaBL2 analysis process. The other way to get the analysis term is to execute the analysis with the flowspec-analyze-ast* variants.","title":"Querying analysis"},{"location":"references/flowspec/Stratego_API/#control-flow-graph","text":"There are a number of strategies to get the control-flow graph nodes associated with an AST fragment, as well as control-flow graph navigation strategies and AST search strategies to get back to the AST from a control-flow graph node. Note that querying the control-flow graph is cheap but finding the way back from the control-flow graph to the AST is more expensive. /** * Get the control flow graph node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-node (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-start-node (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-end-node (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-entry-node (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-exit-node (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-prev-nodes (| a ) /** * Get the control flow graph start node associated with the given term. * * @param a : Analysis * @type term:Term -> CFGNode */ flowspec-get-cfg-next-nodes (| a ) /** * Find AST node corresponding to the CFGNode back again * * @param ast : Term * @type node:CFGNode -> Term */ flowspec-cfg-node-ast (| ast ) /** * Find AST node corresponding to the CFGNode back again * * @param ast : Term * @type pos:Position -> Term */ flowspec-pos-ast (| ast ) /** * Find parent of AST node corresponding to the CFGNode back again by matching the parent with * the parent argument and giving back the child that is likely to be a match to the CFG node. * * @param parent : Term -> Term * @param ast : Term * @type node:CFGNode -> Term */ flowspec-cfg-node-ast ( parent | ast ) /** * Find parent of AST node corresponding to the CFGNode back again by matching the parent with * the parent argument and giving back the child that is likely to be a match to the CFG node. * * @param parent : Term -> Term * @param ast : Term * @type pos:Position -> Term */ flowspec-pos-ast ( parent | ast ) /** * Get the position of an AST node. * * @type Term -> Position */ flowspec-get-position","title":"Control-flow graph"},{"location":"references/flowspec/Stratego_API/#data-flow-properties","text":"FlowSpec properties can be read in two versions, pre and post. These indicate whether the effect of the cfg node has been applied yet. Whether or not it is applied depends on the direction of the analysis. pre for a forward analysis is without the effect of the node, but pre for a backward analysis includes the effect of the node. Note that each strategy can simply take the term that\u2019s associated with the control-flow graph node. But the control-flow graph node itself is also an accepted input. /** * Get the property of the control flow graph node associated with * the given term. The value returned is the value of the property * _before_ the effect of the control flow graph node. * * @param a : Analysis * @param prop : String * @type term:Term -> Term */ flowspec-get-property-pre (| a , propname ) /** * Get the property of the control flow graph node associated with * the given term. The value returned is the value of the property * _after_ the effect of the control flow graph node. * * @param a : Analysis * @param prop : String * @type term:Term -> Term */ flowspec-get-property-post (| a , propname ) /** * Get the property of the control flow graph node associated with * the given term. The value returned is the value of the property * _after_ the effect of the control flow graph node. If no node * is found the exit control flow graph node of the AST node is * queried for its post-effect property value. * * @param a : Analysis * @param prop : String * @type term:Term -> Term */ flowspec-get-property-post-or-exit-post (| analysis-result , analysis-name )","title":"Data flow properties"},{"location":"references/flowspec/Stratego_API/#flowspec-data-helpers","text":"FlowSpec sets and maps are passed back to Stratego as lists wrapped in Set and Map constructors. As a convenience, the most common operations are lifted and added to the flowspec API: /** * Check if a FlowSpec Set contains an element. Succeeds if the given strategy succeeds for at * least one element. * * @param s: Term -?> * @type FlowSpecSet -?> FlowSpecSet */ flowspec-set-contains ( s ) /** * Look up elements in a FlowSpec Set of pairs. Returns the right elements of all pairs where * the given strategy succeeds on the left element. * * @param s: Term -?> * @type FlowSpecSet -?> List(Term) */ flowspec-set-lookup ( s ) /** * Look up a key in a FlowSpec Map. Returns the element if the given key exists in the map. * * @param k: Term * @type FlowSpecMap -?> Term */ flowspec-map-lookup (| k )","title":"FlowSpec data helpers"},{"location":"references/flowspec/Stratego_API/#hover-text","text":"For a hover implementation that displays name, type and FlowSpec properties use: /** * Provides a strategy for a hover message with as much information as possible about name, type * (from NaBl2) and FlowSpec properties. */ flowspec-editor-hover ( language-pp )","title":"Hover text"},{"location":"references/flowspec/Stratego_API/#profiling-information","text":"/** * If flowspec-debug-profile is extended to succeed, some timing information will be printed in * stderr when using flowspec-analyze*. */ flowspec-debug-profile","title":"Profiling information"},{"location":"references/flowspec/configuration/","text":"We will show you how to prepare your project for use with FlowSpec, and write your first small specification. Prepare your project \u00b6 You can start using FlowSpec by creating a new project, or by modifying an existing project. See below for the steps for your case. Start a new project \u00b6 If you have not done this already, install Spoofax Eclipse, by following the installation instructions . Create a new project by selecting New > Project... from the menu. Selecting Spoofax > Spoofax language project from the list, and click Next . After filling in a project name, an identifier, name etc will be automatically suggested. Select NaBL2 as the analysis type, FlowSpec builds on top of NaBL2\u2019s analysis infrastructure. Click Finish to create the project. Add the following dependencies in the metaborg.yaml file: --- # ... dependencies: compile: - org.metaborg:flowspec.lang:${metaborgVersion} source: - org.metaborg:flowspec.lang:${metaborgVersion} Add menus to access the result of analysis, by adding the following import to editor/Main.esv. module Main imports flowspec/Menus Convert an existing project \u00b6 If you have an existing project, and you want to start using FlowSpec, there are a few changes you need to make. First of all, make sure the metaborg.yaml file contains at least the following dependencies. --- # ... dependencies: compile: - org.metaborg:org.metaborg.meta.nabl2.lang:${metaborgVersion} - org.metaborg:flowspec.lang:${metaborgVersion} source: - org.metaborg:org.metaborg.meta.nabl2.shared:${metaborgVersion} - org.metaborg:org.metaborg.meta.nabl2.runtime:${metaborgVersion} - org.metaborg:flowspec.lang:${metaborgVersion} We will set things up, such that analysis rules will be grouped together in the directory trans/analysis . Create a file trans/analysis/main.str that contains the following. module analysis/main imports nabl2shared nabl2runtime analysis/- Add the following lines to your main trans/LANGUAGE.str . module LANGUAGE imports analysis/main rules editor-analyze = nabl2-analyze(desugar-pre) If your language does not have a desugaring step, use nabl2-analyze(id) instead. Add an NaBL2 specification. The most minimal one is the following. module analysis/minimal rules init. [[ _ ]]. Running and integrating the FlowSpec analysis is explained on the Stratego API page . Finally, we will add reference resolution and menus to access the result of analysis, by adding the following lines to editor/Main.esv . module Main imports nabl2/References nabl2/Menus flowspec/Menus You can now continue to the example specification here , or directly to the language reference . Inspecting analysis results \u00b6 You can debug your specification by inspecting the result of analysis. The result of analysis can be inspected, by selecting elements from the Spoofax > FlowSpec Analysis the menu. For multi-file projects, use the Project results, or the File results for single-file projects. The result is given as a control-flow graph annotated with data-flow properties in the DOT format used by GraphViz. If you have GraphViz installed, you can set the dot executable in the settings of the graphviz editor to allow you to jump straight from Eclipse to the rendered graph.","title":"Configuration"},{"location":"references/flowspec/configuration/#prepare-your-project","text":"You can start using FlowSpec by creating a new project, or by modifying an existing project. See below for the steps for your case.","title":"Prepare your project"},{"location":"references/flowspec/configuration/#start-a-new-project","text":"If you have not done this already, install Spoofax Eclipse, by following the installation instructions . Create a new project by selecting New > Project... from the menu. Selecting Spoofax > Spoofax language project from the list, and click Next . After filling in a project name, an identifier, name etc will be automatically suggested. Select NaBL2 as the analysis type, FlowSpec builds on top of NaBL2\u2019s analysis infrastructure. Click Finish to create the project. Add the following dependencies in the metaborg.yaml file: --- # ... dependencies: compile: - org.metaborg:flowspec.lang:${metaborgVersion} source: - org.metaborg:flowspec.lang:${metaborgVersion} Add menus to access the result of analysis, by adding the following import to editor/Main.esv. module Main imports flowspec/Menus","title":"Start a new project"},{"location":"references/flowspec/configuration/#convert-an-existing-project","text":"If you have an existing project, and you want to start using FlowSpec, there are a few changes you need to make. First of all, make sure the metaborg.yaml file contains at least the following dependencies. --- # ... dependencies: compile: - org.metaborg:org.metaborg.meta.nabl2.lang:${metaborgVersion} - org.metaborg:flowspec.lang:${metaborgVersion} source: - org.metaborg:org.metaborg.meta.nabl2.shared:${metaborgVersion} - org.metaborg:org.metaborg.meta.nabl2.runtime:${metaborgVersion} - org.metaborg:flowspec.lang:${metaborgVersion} We will set things up, such that analysis rules will be grouped together in the directory trans/analysis . Create a file trans/analysis/main.str that contains the following. module analysis/main imports nabl2shared nabl2runtime analysis/- Add the following lines to your main trans/LANGUAGE.str . module LANGUAGE imports analysis/main rules editor-analyze = nabl2-analyze(desugar-pre) If your language does not have a desugaring step, use nabl2-analyze(id) instead. Add an NaBL2 specification. The most minimal one is the following. module analysis/minimal rules init. [[ _ ]]. Running and integrating the FlowSpec analysis is explained on the Stratego API page . Finally, we will add reference resolution and menus to access the result of analysis, by adding the following lines to editor/Main.esv . module Main imports nabl2/References nabl2/Menus flowspec/Menus You can now continue to the example specification here , or directly to the language reference .","title":"Convert an existing project"},{"location":"references/flowspec/configuration/#inspecting-analysis-results","text":"You can debug your specification by inspecting the result of analysis. The result of analysis can be inspected, by selecting elements from the Spoofax > FlowSpec Analysis the menu. For multi-file projects, use the Project results, or the File results for single-file projects. The result is given as a control-flow graph annotated with data-flow properties in the DOT format used by GraphViz. If you have GraphViz installed, you can set the dot executable in the settings of the graphviz editor to allow you to jump straight from Eclipse to the rendered graph.","title":"Inspecting analysis results"},{"location":"references/flowspec/references/","text":"References \u00b6","title":"References"},{"location":"references/flowspec/references/#references","text":"","title":"References"},{"location":"references/flowspec/structure/","text":"Modules \u00b6 A module is defined by a single flowspec file. A module can contain several sections, for defining control-flow, data flow, types, and functions. Modules can import other modules. module $module-id imports $module-ref* $section* Terms and Patterns \u00b6 FlowSpec defines various data types, including terms, tuples, sets, and maps. These can be constructed by the user, or introduced by matching on the AST. term = ctor-id \"(\" {term \",\"}* \")\" | \"(\" {term \",\"}* \")\" | \"{\" {term \",\"}* \"}\" | \"{\" term \"|\" {term \",\"}* \"}\" | \"{\" {(term \"|->\" term) \",\"}* \"}\" | \"{\" term \"|->\" term \"|\" {(term \"|->\" term) \",\"}* \"}\" Examples of these terms can be found in the Expressions subsection. Control flow and data flow rules can use patterns to define which rules apply to which AST nodes. pattern = ctor-id \"(\" {pattern \",\"}* \")\" | \"(\" {pattern \",\"}* \")\" | var-id \"@\" pattern | \"_\" | var-id Example. The following shows an example of a pattern, matching a VarDec constructor with an Int child, binding some of the subterms. VarDec(n, _, num@Int(i)) Control Flow \u00b6 The control-flow section contains the rules that define the control-flow for the subject language. control-flow rules $control-flow-rule* Control Flow Rules \u00b6 A control-flow rule consists of a pattern and a corresponding list of control-flow chains. control-flow-rule = \"root\"? pattern \"=\" {cfg-chain \",\"}+ | \"node\" pattern cfg-chain = {cfg-chain-elem \"->\"}+ cfg-chain-elem = \"entry\" | \"exit\" | variable | \"node\" variable | \"this\" Example. Module that specifies how the control-flow for the Add AST node goes from the lhs, the rhs, and then to the Add itself. It also specifies that Int must have a node in the control-flow graph. module control control-flow rules node Int(_) Add(l, r) = entry -> l -> r -> this -> exit Root Rules \u00b6 A root of the control-flow defines the start and end nodes of a control-flow graph. You can have multiple control-flow graphs in the same AST, but not nested ones. Each control-flow graph has a unique start and end node. A root control-flow rule introduces the start and end node. In other control-flow rules these nodes can be referred to for abrupt termination. cfg-chain-elem = ... | \"start\" | \"end\" Example. Module that defines control-flow for a procedure, and the return statement that goes straight to the end of the procedure. module control control-flow rules root Procedure(args, _, body) = start -> args -> body -> end Return(_) = entry -> this -> end Data Flow \u00b6 Properties \u00b6 The data flow section contains definitions of the properties to compute, and the rules that define how these properties are computed. properties property-definition* A property has a name, and a corresponding lattice type. The result after analysis will be a lattice of this type for each node in the control-flow graph. property-definition = name \":\" lattice Example. Lattice definition for a constant-value analysis. properties values: Map[name, Value] Rules \u00b6 The data flow rules specify how data should flow across the control-flow graph. property rules property-rule* property-rule = name \"(\" prop-pattern \")\" \"=\" expr prop-pattern = name \"->\" pattern | pattern \"->\" name | pattern \".\" \"start\" | pattern \".\" \"end\" Example. A simple specification for a constant-value analysis. property rules values(_.end) = Map[string, Value].bottom values(prev -> VarDec(n, _, Int(i))) = { k |-> v | (k |-> v) <- values(prev), k != n } \\/ {n |-> Const(i)} values(prev -> VarDec(n, _, _)) = { k |-> v | (k |-> v) <- values(prev), k != n } \\/ {n |-> Top()} values(prev -> _) = values(prev) Types \u00b6 Algebraic datatypes can be defined for use within lattices definitions. Users can directly match these datatypes, or construct new values. types type-definition* An algebraic datatype consists of a constructor and zero or more arguments. name = (\"|\" ctor-id \"(\" {type \",\"}* \")\")+ Example. The definition for an algebraic type ConstProp used in constant value analysis. types ConstProp = | Top() | Const(int) | Bottom() Lattices \u00b6 Lattices are the main data type used in data-flow analysis, because of their desirable properties. Properties (the analysis results) must always be of type lattice. FlowSpec contains some builtin lattice types, but users can also specify their own. lattices lattice-definition* Lattice definitions must include the following: the underlying datatype, a join operator (either least-upper bound or greatest-lower bound), a top, and a bottom. name where type = type lub([name], name) = expr top = expr bottom = expr Example. A lattice definition using the ConstProp above to define a Value type. lattices Value where type = ConstProp bottom = Bottom() top = Top() lub(l, r) = match (l, r) with | (Top(), _) => Top() | (_, Top()) => Top() | (Const(i), Const(j)) => if i == j then Const(i) else Top() | (_, Bottom()) => l | (Bottom(), _) => r Functions \u00b6 Functions make it possible to reuse functionality and avoid duplication of logic. functions function-definition* name([{(name \":\" type) \",\"}+]) = expr Expressions \u00b6 Integers \u00b6 Integer literals are written with an optional minus sign followed by one or more decimals. Supported integer operations are: Addition [ + ] Subtraction [ - ] Multiplication [ * ] Division [ / ] Modulo [ % ] Negate [ - ] Comparison [ < , <= , > , >= , == , != ] Booleans \u00b6 Boolean literals true and false are available as well as the usual boolean operations: And [ && ] Or [ || ] Not [ ! ] Sets and Maps \u00b6 Set and map literals are both denoted with curly braces. A set literal contains a comma-separated list of elements: {elem1, elem2, elem3} . A map literal contains a comma-separated list of bindings of the form key |-> value: { key1 |-> value1, key2 |-> value2 } . Operations on sets and maps include Union [ \\/ ] Intersection [ /\\ ] Set/map minus [ \\ ] Containment/lookup [ in ] There are also comprehensions of the form { new | old <- set, conditions } or { newkey |-> newvalue | oldkey |-> oldvalue <- map, condition } , where new elements or bindings are gathered based on old ones from a set or map, as long as the boolean condition expressions hold. Such a condition expression may also be a match expression without a body for the arms. This is commonly used to filter maps or sets. Example. The following are some examples of sets and maps. // A map comprehension filtering the key n { k |-> v | (k |-> v) <- values(prev), k != n } // A map literal {n |-> Top()} // A set comprehension filtering the value n { k | k <- live(prev), k != n } // A set literal { n, \"b\", \"foo\" } Match \u00b6 Pattern matching can be done with a match expression: match expr with | pattern1 => expr2 | pattern2 => expr2 , where expr are expressions and pattern are patterns. Terms and patterns are defined at the start of the reference. Variables and References \u00b6 Pattern matching can introduce variables. Other references include values in the lattice, such as MaySet.bottom or MustSet.top . Functions and Lattice Operations \u00b6 User defined functions are invoked with functionname(arg1, arg2) . Lattice operations can be similarly invoked, requiring the type name: MaySet.lub(s1, s2) . Property Lookup \u00b6 Property lookup is similar to a function call, although property lookup only ever has a single argument. Example. The following property rule performs a set comprehension over the results of a property lookup, live(prev) , where the property live has been declared in the properties section, and next is bound in the pattern. live(VarDec(n, _, _) -> next) = { k | k <- live(next), k != n } Term Positions \u00b6 FlowSpec provides a builtin function that returns the position of a term: position(term) . This can be used to differentiate two terms from an AST that are otherwise equal. Lexical Grammar \u00b6 Identifiers \u00b6 Most identifiers in FlowSpec fall into one of two categories, which we will refer to as: Lowercase identifiers, that start with a lowercase character, and must match the regular expression [a-z][a-zA-Z0-9]*. Uppercase identifiers, that start with an uppercase character, and must match the regular expression [A-Z][a-zA-Z0-9]*. Comments \u00b6 Comments in FlowSpec follow C-style comments: // ... single line ... for single-line comments /* ... multiple lines ... */ for multi-line comments Multi-line comments can be nested, and run until the end of the file when the closing */ is omitted.","title":"Structure"},{"location":"references/flowspec/structure/#modules","text":"A module is defined by a single flowspec file. A module can contain several sections, for defining control-flow, data flow, types, and functions. Modules can import other modules. module $module-id imports $module-ref* $section*","title":"Modules"},{"location":"references/flowspec/structure/#terms-and-patterns","text":"FlowSpec defines various data types, including terms, tuples, sets, and maps. These can be constructed by the user, or introduced by matching on the AST. term = ctor-id \"(\" {term \",\"}* \")\" | \"(\" {term \",\"}* \")\" | \"{\" {term \",\"}* \"}\" | \"{\" term \"|\" {term \",\"}* \"}\" | \"{\" {(term \"|->\" term) \",\"}* \"}\" | \"{\" term \"|->\" term \"|\" {(term \"|->\" term) \",\"}* \"}\" Examples of these terms can be found in the Expressions subsection. Control flow and data flow rules can use patterns to define which rules apply to which AST nodes. pattern = ctor-id \"(\" {pattern \",\"}* \")\" | \"(\" {pattern \",\"}* \")\" | var-id \"@\" pattern | \"_\" | var-id Example. The following shows an example of a pattern, matching a VarDec constructor with an Int child, binding some of the subterms. VarDec(n, _, num@Int(i))","title":"Terms and Patterns"},{"location":"references/flowspec/structure/#control-flow","text":"The control-flow section contains the rules that define the control-flow for the subject language. control-flow rules $control-flow-rule*","title":"Control Flow"},{"location":"references/flowspec/structure/#control-flow-rules","text":"A control-flow rule consists of a pattern and a corresponding list of control-flow chains. control-flow-rule = \"root\"? pattern \"=\" {cfg-chain \",\"}+ | \"node\" pattern cfg-chain = {cfg-chain-elem \"->\"}+ cfg-chain-elem = \"entry\" | \"exit\" | variable | \"node\" variable | \"this\" Example. Module that specifies how the control-flow for the Add AST node goes from the lhs, the rhs, and then to the Add itself. It also specifies that Int must have a node in the control-flow graph. module control control-flow rules node Int(_) Add(l, r) = entry -> l -> r -> this -> exit","title":"Control Flow Rules"},{"location":"references/flowspec/structure/#root-rules","text":"A root of the control-flow defines the start and end nodes of a control-flow graph. You can have multiple control-flow graphs in the same AST, but not nested ones. Each control-flow graph has a unique start and end node. A root control-flow rule introduces the start and end node. In other control-flow rules these nodes can be referred to for abrupt termination. cfg-chain-elem = ... | \"start\" | \"end\" Example. Module that defines control-flow for a procedure, and the return statement that goes straight to the end of the procedure. module control control-flow rules root Procedure(args, _, body) = start -> args -> body -> end Return(_) = entry -> this -> end","title":"Root Rules"},{"location":"references/flowspec/structure/#data-flow","text":"","title":"Data Flow"},{"location":"references/flowspec/structure/#properties","text":"The data flow section contains definitions of the properties to compute, and the rules that define how these properties are computed. properties property-definition* A property has a name, and a corresponding lattice type. The result after analysis will be a lattice of this type for each node in the control-flow graph. property-definition = name \":\" lattice Example. Lattice definition for a constant-value analysis. properties values: Map[name, Value]","title":"Properties"},{"location":"references/flowspec/structure/#rules","text":"The data flow rules specify how data should flow across the control-flow graph. property rules property-rule* property-rule = name \"(\" prop-pattern \")\" \"=\" expr prop-pattern = name \"->\" pattern | pattern \"->\" name | pattern \".\" \"start\" | pattern \".\" \"end\" Example. A simple specification for a constant-value analysis. property rules values(_.end) = Map[string, Value].bottom values(prev -> VarDec(n, _, Int(i))) = { k |-> v | (k |-> v) <- values(prev), k != n } \\/ {n |-> Const(i)} values(prev -> VarDec(n, _, _)) = { k |-> v | (k |-> v) <- values(prev), k != n } \\/ {n |-> Top()} values(prev -> _) = values(prev)","title":"Rules"},{"location":"references/flowspec/structure/#types","text":"Algebraic datatypes can be defined for use within lattices definitions. Users can directly match these datatypes, or construct new values. types type-definition* An algebraic datatype consists of a constructor and zero or more arguments. name = (\"|\" ctor-id \"(\" {type \",\"}* \")\")+ Example. The definition for an algebraic type ConstProp used in constant value analysis. types ConstProp = | Top() | Const(int) | Bottom()","title":"Types"},{"location":"references/flowspec/structure/#lattices","text":"Lattices are the main data type used in data-flow analysis, because of their desirable properties. Properties (the analysis results) must always be of type lattice. FlowSpec contains some builtin lattice types, but users can also specify their own. lattices lattice-definition* Lattice definitions must include the following: the underlying datatype, a join operator (either least-upper bound or greatest-lower bound), a top, and a bottom. name where type = type lub([name], name) = expr top = expr bottom = expr Example. A lattice definition using the ConstProp above to define a Value type. lattices Value where type = ConstProp bottom = Bottom() top = Top() lub(l, r) = match (l, r) with | (Top(), _) => Top() | (_, Top()) => Top() | (Const(i), Const(j)) => if i == j then Const(i) else Top() | (_, Bottom()) => l | (Bottom(), _) => r","title":"Lattices"},{"location":"references/flowspec/structure/#functions","text":"Functions make it possible to reuse functionality and avoid duplication of logic. functions function-definition* name([{(name \":\" type) \",\"}+]) = expr","title":"Functions"},{"location":"references/flowspec/structure/#expressions","text":"","title":"Expressions"},{"location":"references/flowspec/structure/#integers","text":"Integer literals are written with an optional minus sign followed by one or more decimals. Supported integer operations are: Addition [ + ] Subtraction [ - ] Multiplication [ * ] Division [ / ] Modulo [ % ] Negate [ - ] Comparison [ < , <= , > , >= , == , != ]","title":"Integers"},{"location":"references/flowspec/structure/#booleans","text":"Boolean literals true and false are available as well as the usual boolean operations: And [ && ] Or [ || ] Not [ ! ]","title":"Booleans"},{"location":"references/flowspec/structure/#sets-and-maps","text":"Set and map literals are both denoted with curly braces. A set literal contains a comma-separated list of elements: {elem1, elem2, elem3} . A map literal contains a comma-separated list of bindings of the form key |-> value: { key1 |-> value1, key2 |-> value2 } . Operations on sets and maps include Union [ \\/ ] Intersection [ /\\ ] Set/map minus [ \\ ] Containment/lookup [ in ] There are also comprehensions of the form { new | old <- set, conditions } or { newkey |-> newvalue | oldkey |-> oldvalue <- map, condition } , where new elements or bindings are gathered based on old ones from a set or map, as long as the boolean condition expressions hold. Such a condition expression may also be a match expression without a body for the arms. This is commonly used to filter maps or sets. Example. The following are some examples of sets and maps. // A map comprehension filtering the key n { k |-> v | (k |-> v) <- values(prev), k != n } // A map literal {n |-> Top()} // A set comprehension filtering the value n { k | k <- live(prev), k != n } // A set literal { n, \"b\", \"foo\" }","title":"Sets and Maps"},{"location":"references/flowspec/structure/#match","text":"Pattern matching can be done with a match expression: match expr with | pattern1 => expr2 | pattern2 => expr2 , where expr are expressions and pattern are patterns. Terms and patterns are defined at the start of the reference.","title":"Match"},{"location":"references/flowspec/structure/#variables-and-references","text":"Pattern matching can introduce variables. Other references include values in the lattice, such as MaySet.bottom or MustSet.top .","title":"Variables and References"},{"location":"references/flowspec/structure/#functions-and-lattice-operations","text":"User defined functions are invoked with functionname(arg1, arg2) . Lattice operations can be similarly invoked, requiring the type name: MaySet.lub(s1, s2) .","title":"Functions and Lattice Operations"},{"location":"references/flowspec/structure/#property-lookup","text":"Property lookup is similar to a function call, although property lookup only ever has a single argument. Example. The following property rule performs a set comprehension over the results of a property lookup, live(prev) , where the property live has been declared in the properties section, and next is bound in the pattern. live(VarDec(n, _, _) -> next) = { k | k <- live(next), k != n }","title":"Property Lookup"},{"location":"references/flowspec/structure/#term-positions","text":"FlowSpec provides a builtin function that returns the position of a term: position(term) . This can be used to differentiate two terms from an AST that are otherwise equal.","title":"Term Positions"},{"location":"references/flowspec/structure/#lexical-grammar","text":"","title":"Lexical Grammar"},{"location":"references/flowspec/structure/#identifiers","text":"Most identifiers in FlowSpec fall into one of two categories, which we will refer to as: Lowercase identifiers, that start with a lowercase character, and must match the regular expression [a-z][a-zA-Z0-9]*. Uppercase identifiers, that start with an uppercase character, and must match the regular expression [A-Z][a-zA-Z0-9]*.","title":"Identifiers"},{"location":"references/flowspec/structure/#comments","text":"Comments in FlowSpec follow C-style comments: // ... single line ... for single-line comments /* ... multiple lines ... */ for multi-line comments Multi-line comments can be nested, and run until the end of the file when the closing */ is omitted.","title":"Comments"},{"location":"references/flowspec/testing/","text":"Testing \u00b6","title":"Testing"},{"location":"references/flowspec/testing/#testing","text":"","title":"Testing"},{"location":"references/pipelines/","text":"Pipelines for Interactive Environments \u00b6 Pipelines for interactive Environments (PIE) is the build system for Spoofax 3. PIE consists of two parts: a Java framework, a Java runtime and the PIE Domain Specific Language (DSL). This reference documentation is for the PIE DSL and will only provide some high level information about the framework and runtime to provide context. PIE uses tasks to compose pipelines. Each task has 0 or more inputs and one output. Each task can depend on files or on other tasks. Tasks can be marked as explicitly observed to indicate that we want the output of these tasks to stay up to date. The PIE runtime executes tasks incrementally, which means that it only executes tasks that are no longer up to date and that are required for a task which is explicitly observed. Tasks can be written in Java, but this involves a lot of boilerplate. Tasks can also be written in the PIE DSL. The PIE DSL is specifically made for PIE, so it has little boilerplate. Tasks written in the PIE DSL are compiled to Java. The PIE DSL \u00b6 PIE models a pipeline as tasks that call each other. The PIE DSL calls these tasks \"functions\", because each task has inputs and an output. A PIE DSL program consists of one or more files. File structure \u00b6 module fully:qualified:moduleName import fully:qualified:name:of:another:module import org:example:multipleDefs:{func1, func2 as other, aDataTypeAsWell} import org:example:languages:{java, cpp, sql}:spoofax:{parse, analyze, compile} data coolDataType = foreign java org.example.MyFirstJavaClass { func aMethod(int) -> bool } func greetWorld() -> string = \"Hello world!\" PIE DSL files contain a module statement, imports, and data and function definitions. The module statement declares the fully qualified name of the module. Imports are optional and import datatypes and function from other modules. They can import multiple functions or datatypes at the same time, and they can rename elements. Data and function definitions define functions and datatypes. Directory structure and module system \u00b6 PIE files have the extension .pie . Each PIE file forms a module. Modules can define functions and datatypes, and can import functions and datatypes from other modules. It is recommended to use the same name for the module as the path and filename, but this is not required. As such, the PIE DSL does not place any restrictions on paths and file names besides the standard restrictions for Spoofax languages. The module system is described in Modules . Types and data definitions \u00b6 The PIE DSL is a statically typed language. There are a few built-in types, such as int and path . Built-in types use lowercase characters. Custom datatypes can currently only be imported from Java as foreign definitions. The types in the PIE DSL are described in Types . The PIE DSL also supports generic datatypes. These follow Java semantics. The semantics of generics can be found in Generics . Function definitions \u00b6 Functions express task definitions. Functions consist of a head and an implementation. func $FuncHead = $FuncImpl func greet(name: string) -> string = \"Hello ${name}!\" func doSomethingDifficult() -> path = foreign org.example.DoSomethingDifficult func callJavaStaticFunction() -> bool = foreign java fully.qualified.java.ClassName#staticMethodName func createCustomType() -> CustomType = foreign java constructor org.example.CustomType The function head describes the signature of the function: the name, the input parameter types and the output type. All functions can be called the same way regardless of their implementation. The function implementation describes the way a function is implemented. A function can be implemented in PIE by providing an expression, as can be seen with greet Expressions are described in Expressions . A function can also be implemented in Java. The three ways this can be done are shown in the example as well. A complete overview of functions is given in Functions . Misc information. \u00b6 Java and C use the function called main with a certain signature as the entry point to the program. A PIE program does not have a set entry point. The entry point is whatever function is called from the PIE runtime.","title":"Pipelines for Interactive Environments"},{"location":"references/pipelines/#pipelines-for-interactive-environments","text":"Pipelines for interactive Environments (PIE) is the build system for Spoofax 3. PIE consists of two parts: a Java framework, a Java runtime and the PIE Domain Specific Language (DSL). This reference documentation is for the PIE DSL and will only provide some high level information about the framework and runtime to provide context. PIE uses tasks to compose pipelines. Each task has 0 or more inputs and one output. Each task can depend on files or on other tasks. Tasks can be marked as explicitly observed to indicate that we want the output of these tasks to stay up to date. The PIE runtime executes tasks incrementally, which means that it only executes tasks that are no longer up to date and that are required for a task which is explicitly observed. Tasks can be written in Java, but this involves a lot of boilerplate. Tasks can also be written in the PIE DSL. The PIE DSL is specifically made for PIE, so it has little boilerplate. Tasks written in the PIE DSL are compiled to Java.","title":"Pipelines for Interactive Environments"},{"location":"references/pipelines/#the-pie-dsl","text":"PIE models a pipeline as tasks that call each other. The PIE DSL calls these tasks \"functions\", because each task has inputs and an output. A PIE DSL program consists of one or more files.","title":"The PIE DSL"},{"location":"references/pipelines/#file-structure","text":"module fully:qualified:moduleName import fully:qualified:name:of:another:module import org:example:multipleDefs:{func1, func2 as other, aDataTypeAsWell} import org:example:languages:{java, cpp, sql}:spoofax:{parse, analyze, compile} data coolDataType = foreign java org.example.MyFirstJavaClass { func aMethod(int) -> bool } func greetWorld() -> string = \"Hello world!\" PIE DSL files contain a module statement, imports, and data and function definitions. The module statement declares the fully qualified name of the module. Imports are optional and import datatypes and function from other modules. They can import multiple functions or datatypes at the same time, and they can rename elements. Data and function definitions define functions and datatypes.","title":"File structure"},{"location":"references/pipelines/#directory-structure-and-module-system","text":"PIE files have the extension .pie . Each PIE file forms a module. Modules can define functions and datatypes, and can import functions and datatypes from other modules. It is recommended to use the same name for the module as the path and filename, but this is not required. As such, the PIE DSL does not place any restrictions on paths and file names besides the standard restrictions for Spoofax languages. The module system is described in Modules .","title":"Directory structure and module system"},{"location":"references/pipelines/#types-and-data-definitions","text":"The PIE DSL is a statically typed language. There are a few built-in types, such as int and path . Built-in types use lowercase characters. Custom datatypes can currently only be imported from Java as foreign definitions. The types in the PIE DSL are described in Types . The PIE DSL also supports generic datatypes. These follow Java semantics. The semantics of generics can be found in Generics .","title":"Types and data definitions"},{"location":"references/pipelines/#function-definitions","text":"Functions express task definitions. Functions consist of a head and an implementation. func $FuncHead = $FuncImpl func greet(name: string) -> string = \"Hello ${name}!\" func doSomethingDifficult() -> path = foreign org.example.DoSomethingDifficult func callJavaStaticFunction() -> bool = foreign java fully.qualified.java.ClassName#staticMethodName func createCustomType() -> CustomType = foreign java constructor org.example.CustomType The function head describes the signature of the function: the name, the input parameter types and the output type. All functions can be called the same way regardless of their implementation. The function implementation describes the way a function is implemented. A function can be implemented in PIE by providing an expression, as can be seen with greet Expressions are described in Expressions . A function can also be implemented in Java. The three ways this can be done are shown in the example as well. A complete overview of functions is given in Functions .","title":"Function definitions"},{"location":"references/pipelines/#misc-information","text":"Java and C use the function called main with a certain signature as the entry point to the program. A PIE program does not have a set entry point. The entry point is whatever function is called from the PIE runtime.","title":"Misc information."},{"location":"references/pipelines/expressions/","text":"Expressions \u00b6 Function bodies in PIE DSL consist entirely of expressions. Every expression has a type and returns a value of that type, with the exception of fail and return . Expressions can use brackets to override the default priority rules, for example a && (b || c) . Brackets have the following form: ($Exp) This section describes expressions in the PIE DSL. Expressions can use declared values. These are described in this section of the functions documentation . Syntactic priorities (disambiguation) \u00b6 Nested expressions can be ambiguous, for example ! true && false could either ! (true && false) // = true or (!true) && false // = false . To solve these ambiguities, each expression has a priority. Expressions with higher priories will be nested in expressions with lower priority. The example above is parsed as (!true) && false because not has a higher priority than logical and . All expressions are left-associative, which means that if two expressions with the same priority are ambiguous, the leftmost expression will be nested in the rightmost expression. For example, 3 - 2 + 1 is equivalent to (3 - 2) + 1 . The following list lists expressions in order of descending priority. Expressions on the same number have the same priority. If an expression is not listed, it cannot be ambiguous (e.g. Blocks and list literals) Not Make nullable , Make non-nullable Addition Compare for (in)equality Logical and Logical or list , walk requires , generates read , exists Function calls , Method calls , Create supplier , Task supplier Filters Tuple literal , List literal List comprehension Value declaration return , fail if-else if Quick overview \u00b6 The following table gives a quick overview of all expressions in the PIE DSL. name syntax example description type Block {$Exps} {val x = 4; x+7} A sequence of expressions The type of the last expression Make nullable $Exp? \"nullable string\"? Makes the type T of an expression nullable ( T? ) When applied to an expression of type T , T? Make non-nullable $Exp! input! Makes the type T? of an expression non- nullable ( T ). Throws an exception if the value is null When applied to an expression of type T? , T Not !$Exp !flag Turns false to true and vice versa bool Compare for (in)equality $Exp == $Exp and Exp != $Exp result == null , errors != [] Compares two values for (in)equality bool Logical or $Exp || $Exp dryRun || input == null Is true unless both the values are false bool Logical and $Exp && $Exp !dryRun && input != null Is true iff both values are true bool Addition $Exp + $Exp x + y Adds or concatenates two values Depends on the types of the expressions Value declaration val $VALID TypeHint? = $Exp val num: int = 47 Declare a value by name The type of the declared value Value reference $VALID x Reference a value or parameter The type of the referenced value if if ($Exp) $Exp if (input == null) fail \"Input is null\" Evaluates the body if the condition evaluates to true unit if-else if ($Exp) $Exp else $Exp if (name != null) name else default Evaluates the first branch if the condition is true , and the second branch otherwise The least upper bound of the types of the branches List comprehension [$Exp | $Binder <- $Exp] [\"Key: $key; value: $value\" | (key, value) <- pairs] Evaluate the expression for each element in a list A list of the type of the expression Function calls $ModuleList$FUNCID$TypeArgs($Exps) stdLib:okResult<string>(\"Hello world!\") Call a function The return type of the function Method calls $Exp.$FUNCID$TypeArgs($Exps) file.replaceExtension(\"pp.pie\") Call a method The return type of the method Create supplier supplier$TypeArgs($Exps) supplier(47) Create a supplier A supplier of the provided type Task supplier $ModuleList$FUNCID.supplier$TypeArgs($Exps) lang:java:parse.supplier(file) Create a supplier from a function A supplier of the return type of the function requires requires $Exp $FilterPart? $StamperPart? requires ./metaborg.yaml by hash Declare that the current task depends on the provided path unit generates generates $Exp $StamperPart? generates file by hash Declare that the current task generates on the provided path unit list list $Exp $FilterPart? list ./examples with extension \"pie\" Lists the direct children of the given directory . Note: for list literals, see further down this table. A list of paths , i.e. path* walk walk $Exp $FilterPart? walk ./examples with extension \"pie\" Recursively get all descendants of the given directory A list of paths , i.e. path* exists exists $Exp exists ./config.json Check if a file exists bool read read $Exp read ./config.json Returns the file contents as a string, or null if the file does not exist A nullable string , i.e. string? return return $Exp return false Returns the provided value from the current function unit . Note: may get changed to bottom type fail fail $Exp fail \"input cannot be null\" Throws an ExecException with the provided string as message unit . Note: may get changed to bottom type Unit literal unit unit Literal expression of the only value of the unit type unit true true true Literal expression for the boolean value true . bool false false false Literal expression for the boolean value false . bool int literal \"-\"? [0-9]+ 0 , 23 , -4 A literal value of the int type int null null null Literal expression for the null value of nullable types . null type Tuple literal ($Exps) (1, \"one\", \"une\") A literal value of a tuple type A tuple type of the types of the elements List literal [$Exps] (1, 2, 3) A literal value of a list type . For the keyword list , see earlier in this table. A list of the least upper bound of the types of the elements String literal \"$StrParts\" \"Hello $name!\" A literal value of the string type string Path literal $PathStart$PathParts ./src/test/resources A literal value of the path type path There is also a section on common lexical elements . This covers filters and stampers . Block \u00b6 Blocks are expressed as {$Exps} , where $Exps is a list of semi-colon separated expressions. Its type and value are those of the final expressions in the block. The final expression does not end with a semi-colon. For example: { val name = read ./example/name.txt; val greeting = read ./example/greeting.txt; \"$greeting $name\" } Empty blocks (blocks without any expression) are allowed, their type and value is unit . Blocks introduce their own scope. Expressions in the block are evaluated in that scope. Values declared in the block are not allowed to shadow values or parameters outside the block. This means that it is not allowed to declare a value with a name that already exists. Values declared before the block are still visible inside the block. Values declared inside the block are not visible after the block. Make nullable \u00b6 Make nullable is expressed as a question mark after an expression. As its name suggests, it makes a non-nullable expression nullable. The value remains unchanged, but the type of the expression is now nullable . Its syntax is $Exp? , for example read ./name.txt == \"Bob\"? . It is an error to use this expression on an expression that was nullable already. Make non-nullable \u00b6 Make non-nullable is expressed as an exclamation mark after an expression. As its name suggests, it makes a nullable expression non-nullable. The value remains unchanged, but the type of the expression is now non-nullable . Its syntax is $Exp! , for example read file! . It is an error to use this expression on an expression that was non-nullable already. Not \u00b6 Logical negation. It takes a boolean expression and returns the opposite boolean value. Its syntax is !$Exp , for example if (!exists file) fail \"$file should exist . Compare for (in)equality \u00b6 Compare two expressions for equality or inequality. Two values are considered equal if they are both null or if the equals method in the backing Java class for the first value returns true when applied to the second value. Otherwise, they are considered in-equal. The syntax for equals is $Exp == $Exp , for example x == null . The syntax for in-equals is $Exp != $Exp , for example x != null . Expressions can only be compared if one is a non-strict subtype of the other. This provides protection against accidentally comparing two expressions that can never be equal. Comparing null and empty lists Expressions with nullable types can have equal values despite not being subtypes of each other, if they are both null . The same goes for list types with the empty list [] . In these cases, check for these specific values: x == null && y == null . Logical or \u00b6 Logical or. Takes two boolean expressions and returns true if either of them is true and false if both are false . This operator short-circuits if the first expression is true , in which case the second expression will not be evaluated. Its syntax is $Exp || $Exp , for example exists file || default != null . logical and \u00b6 Logical and. Takes two boolean expressions and returns true if both of them are true and false if either of them is false . This operator short-circuits if the first expression is false , in which case the second expression will not be evaluated. Its syntax is $Exp && $Exp , for example configFile != null && exists configFile . Addition \u00b6 The add operator is an overloaded in the PIE DSL. Its syntax is $Exp + $Exp . The type of adding two values depends on their static types. Adding two int s uses mathematical plus: 1 + 2 // result: 3 , and the result is an int as well. Adding any value to a string converts the value to a string and then concatenates the strings, resulting in a string : \"The value is:\" + x . String interpolation It might be clearer to use string interpolation. Adding a string or a path to a relative path concatenates the values and results in a new path : projectDir + ./src/test/resources/ Adding a string or a path to an absolute path results in a runtime error. Path interpolation It might be clearer to use path interpolation. Finally, adding a type T2 to a list with type T1* has two cases If T2 is a list as well both lists will be concatenated. The element type of T2 must be a subtype of T1 . Adding a list as an element To add a list list: T* as an element to a list of lists lists: T** , wrap the list in another list: lists + [list] Empty lists The PIE DSL keeps track of empty lists statically. This allows it to give a warning when concatenating an empty list: [1, 2, 3] + [] will give a warning. All other cases will append the second item to the first list. T2 must be a (non-strict) subtype of T1 . The element type of the resulting list is T1 , unless T2 is the null type . In that case, the element type of the resulting list is nullable as well. Value declaration \u00b6 Value declarations declare one or more named values. Expressions that are evaluated afterwards in the same scope or an inner scope can use the declared values by referencing them . For more information on values, see parameters and values . A basic value declaration declares a name with a value: val x = 9 . It is possible to give a type hint with the name: val y: int? = 8 . When a type hint is given, the type of the expression must be assignable to the type of the type hint. The type of the declared value is the provided type from the type hint, or the type of the expression if there is no type hint. A value declaration can also do tuple destructuring and assign its values to multiple variables at once: val (name: string, times: int*) = getPerformance(id) . Each name in a tuple destructuring can have a type hint. Tuple destructuring cannot be nested, so the following will not parse: val (a, (b, c)) = (1, (2, 3)) . Some examples of value declarations val firstName = \"Bob\"; // simple value declaration val age: int = 27; // with type hint val size: (int, int) = (800, 400); // assign tuple to single value. val (width, height) = size; // tuple destructuring // tuple destructuring with type hints val (name: string, in: path, out: path) = (\"expressions\", ./in/expressions.pie, ./out/expressions.java); // tuple destructuring with mixed type hints val (year, values: (string, bool)*) = (2020, []); Value declarations have the following syntax: val $Binder = $Exp , where the binder can be either a single binder $Bind or tuple binder ($Binds) , and binds can be only a name $VALID or a name with a type hint $VALID: Type . The type of a value declaration is the type of the variable(s) that it declares. It uses the type hint if available and the expression type otherwise. Single declarations have that single type, tuple declarations return a tuple of all the types that they declare. Value reference \u00b6 Value references reference a declared value or parameter by name. The name must be declared beforehand in the current scope or an outer scope. The type and value of a value reference is the type and value of the referenced value. The syntax is $VALID , for example: func incrementInefficiently(x: int) -> int = { val y = 1; val res = x + y; // reference parameter x and value y res // reference value res } if \u00b6 An if expression conditionally evaluates an expression. It takes two expressions. The first one is the condition and is of type boolean . The second one is the body and can have any type. Its syntax is if ($Exp) $Exp , for example if (text == null) fail \"Could not read $file\" . If the condition evaluates to true , the body is evaluated, otherwise not. The type of an if expression is the unit type . The condition and body are evaluated in their own scope, so value declarations in an if expression are not visible after the if . Because an if expression is evaluated in its own scope and always returns the same value, the only use for an if expression is if the body has side-effects. if-else \u00b6 Conditionally evaluates one of two expressions. It takes three expressions. The first one is the condition and is of type boolean . The second one is the true-branch and can have any type. The third one is the false-branch and can also have any type. Its syntax is if ($Exp) $Exp else $Exp , for example if (name != null) name else default . If the condition evaluates to true , the true-branch is evaluated, otherwise the false-branch is evaluated. The type of an if-else expression is the least upper bound of both branches. It is an error if the least upper bound of the two branches is the top type . The condition and branches are evaluated in their own scope, so value declarations in an if-else expression are not visible after the expression. Some examples of the least upper bound of different types val cat1: Cat = getCat(1); val cat2: Cat = getCat(2); val mammal: Mammal = getMammal(); val animal: Animal = getAnimal(); val dog: Dog = getDog(); val fish: Fish = getFish(); val animal1: Animal = getAnimal(1); val animal2: Animal = getAnimal(2); val catBox1: Box<Cat> = box(cat1); val catBox2: Box<Cat> = box(cat2); val anyBox1: Box<_> = box(animal1); val anyBox2: Box<_> = box(catBox2); // same type if (flag) \"hello\" else \"world\" // type: string if (flag) 0 else 10 // type: int if (flag) cat1 else cat2 // type: Cat if (flag) animal1 else animal2 // type: Animal if (flag) catBox1 else catBox2 // type: Box<Cat> if (flag) anyBox1 else anyBox2 // type: Box<_> // subtypes if (flag) cat else mammal // type: Mammal if (flag) catBox else anyBox // type: Box<_> // different types if (flag) cat1 else dog // type: Mammal if (flag) dog else cat2 // type: Mammal if (flag) cat2 else fish // type: Animal if (flag) \"hello\" else 2 // type: top type, error List comprehension \u00b6 List comprehensions apply an expression to every element of a list and return a new list with the new elements. They are special syntax for mapping a list, which would not ordinarily be possible in the PIE DSL because there are no higher-order functions. They have the syntax [$Exp | $Binder <- $Exp] , for example [\"Key: $key; value: $value\" | (key, value) <- pairs] The last expression is the input list and must have type T* for some T . The binder defines names for the elements. It can either be a single binder to bind the complete element, or a tuple binder if the element is a tuple, see value declarations for more explanation The first expression can use the names defined by the binder. The type of that expression is some type Q . The type of the full list comprehension is a list of the type that was mapped to, i.e. Q* . Empty lists A list comprehension over an empty list simply returns a new empty list. A list comprehension will give a warning if the input list is statically known to be empty. List comprehensions over empty lists are compiled to an immediate empty list of the declared type because it is not known what the element type of the empty list is. Function calls \u00b6 Function calls invoke a declared function . They have the syntax $ModuleList$FUNCID$TypeArgs($Exps) , for example stdLib:okResult<string>(\"Hello world!\") . The second element is the function name . This function name can either be qualified or left unqualified by the module list. If it is unqualified, the function name must be defined in the current module or be imported with a function import . If it is qualified, the function is looked up in that module . The number of type arguments must match the number of type parameters on the function declaration , and the type arguments must be within bounds for the type parameters. The expressions are the arguments to the function. They must match the number of parameters that the function declared and they must be subtypes of the parameters. The type of a call is the type of the declared function, where type parameters are replaced with their corresponding type arguments. Return type is a generic parameter func id<T>(param: T) -> T = param func test() -> unit = { id<string>(\"Hello world!\"); // type is string id<int>(42); // type is int unit } Arguments can declare values Value declarations in the arguments are legal and are visible after the call. Doing this is bad practice in almost all cases. Declare the value before the call instead. For example: // declares the value `x` with value 9 // also passes 9 as argument to `test` test(val x = 9); x // x is visible after the call. Better way: val x = 9; // declare before call test(x); // refer to declared value x Method calls \u00b6 Method calls invoke a declared method . They have the syntax $Exp.$FUNCID$TypeArgs($Exps) , for example file.replaceExtension(\"pp.pie\") . The first element is an arbitrary expression. The second element is the method name. This method is looked up on the type of the first expression. The number of type arguments must match the number of type parameters on the method declaration , and the type arguments must be within bounds for the type parameters. The expressions are the arguments to the method. They must match the number of parameters that the method declared and they must be subtypes of the parameters. The type of a method call is the type of the declared method, where type parameters are replaced with their corresponding type arguments. No methods on nullable types There are no methods defined on nullable types . To access the methods of the inner type, cast the expression to non-nullable first: val maybe: Result<string, _ : Exception> = null; maybe.unwrap(); // error: Cannot call method on nullable type maybe!.unwrap(); // type checks, but will throw a run time exception // Better: handle null value before casting if (maybe == null) { // handle null value here \"Cannot get value, result is null\" } else { maybe!.unwrap() } Return type is a generic parameter data Box<T> = foreign java org.example.methodCall.Box { func get() -> T } func test(box1: Box<int>, box2: Box<bool>) -> unit = { box1.get(); // type is int box2.get(); // type is bool unit } Expression and arguments can declare values Value declarations in the expression or the arguments are legal and are visible after the call. Declarations in the expression are also visible to the expressions. Doing this is bad practice in almost all cases. Declare the value before the call instead. For example: // declares the value `name` and `msg` // also passes 9 as argument to `test` // Note: getName returns an stdLib:Result<string, _ : Exception> (val name = getName()).unwrapOrDefault( // `name` is visible inside the arguments val msg = \"Could not get name, exception: ${if (val ok = name.isOk()) \"No error\" else name.unwrapErr() }\" ); (name, ok, msg); // `name`, `ok` and `message` are visible after the call. Better way: val name = getName(); val ok = name.isOk(); val msg = if (ok) \"Could not get name, exception: No error\" else \"Could not get name, exception: ${name.unwrapErr()}\" create supplier \u00b6 A supplier for a value can be created with the supplier keyword. It has the syntax supplier$TypeArgs($Exps) , for example supplier(47) or supplier<string>(\"Hello world!\") . The type arguments can either be omitted or must be a single type argument. The expressions are the arguments for the supplier. There should be only one argument, the value that the supplier will supply. The type T for the supplier is the type argument if it was provided, or the type of the argument otherwise. In case a type argument is provided, the argument should be a subtype of that type argument. The type of a supplier creation expression is supplier<T> . Supplier can be treated as a normal function Creating a supplier is like a normal function call , but built into the language grammar for implementation reasons. This is the only function call where the type argument is derived at the moment. task supplier \u00b6 A task supplier expression creates a supplier from a function. A task supplier expression does not execute the task yet, but instead defers it until the supplier's get method is called. It has the syntax $ModuleList$FUNCID.supplier$TypeArgs($Exps) , for example lang:java:parse.supplier(file) . The second element is the function name . This function name can either be qualified or left unqualified by the module list. If it is unqualified, the function name must be defined in the current module or be imported with a function import . If it is qualified, the function is looked up in that module . The number of type arguments must match the number of type parameters on the function declaration , and the type arguments must be within bounds for the type parameters. The expressions are the arguments to the function. They must match the number of parameters that the function declared and they must be subtypes of the parameters. The type of a task supplier expression is supplier<T> , where T is the type of the declared function with the type parameters replaced with their corresponding type arguments. requires \u00b6 A requires expression expresses that the current task depends on the given path . It has the syntax requires $Exp $FilterPart? $StamperPart? , for example requires ./metaborg.yaml by hash or requires sampleDir with extension \"sdf3\" . The expression is the path to depend on. The filter part is optional and adds a filter to filter out any paths that do not match the filter. It is described in the section on common lexical elements . The stamper part is also optional and provides a way to determine if a file or path is up-to-date. It is also described in the section on common lexical elements . The type and value of the expression is unit . Exceptions? What happens if there is another task that provides the path? Does it quietly schedule that task before this one, does it throw an error? What if that other task runs after this task? generates \u00b6 Marks the given path as provided by the current task. It has the syntax generates $Exp $StamperPart? , for example generates file by hash . The expression is the path to depend on. The stamper part is optional and provides a way to determine if a file or path is up-to-date. It is described in the section on common lexical elements . The type and value of this expression is unit . Make file modifications before using this expression The contents or metadata of the file at the time that this expression is called may be cached and used for incrementality. Make all modifications to the file before using this expression. Exceptions? Can this mark a directory as provided or only a file? What happens when two tasks generate a file? What happens when this task declares it generates a file after another task has already used it? Can a task both require and provide a file? What happens if this task calls a task that provides a file and then this task also declares it generated that file? list \u00b6 Defining list literals This section is about listing children of a directory with the list keyword. To define a literal list value, see list literal . Lists direct children of the given directory. List expressions have the syntax list $Exp $FilterPart? , for example list getProjectRootDir() + ./examples with extension \"pie\" . The expression must have type path and refer to an existing directory. The filter part is optional and adds a filter to filter out any paths that do not match the filter. It is described in the section on common lexical elements . A list expression returns a list of the direct children of the given directory, and its type is path* . Declaring a dependency on the directory You will likely need to declare a dependency on the directory using requires . You may also need to declare dependencies on the individual files if you do not call a task which already does that. Recursive listing List only gets the direct children of the given directory. To recursively get all files and directories in a given directory, use walk . Todo What happens if the starting directory does not exist? What happens if it is not a directory? walk \u00b6 Recursively gets descendants of the given directory. Walk expressions have the syntax walk $Exp $FilterPart? , for example walk getProjectRootDir() + ./src/test/java with extension \"java\" . The expression must have type path and refer to an existing directory. The filter part is optional and adds a filter to filter out any files that do not match the filter. It is described in the section on common lexical elements . A walk expression returns a list of all the files in the given directory and its descendants, and its type is path* . Declaring a dependency on the directory You will likely need to declare a dependency on the directory and all subdirectories using requires . You may also need to declare dependencies on the individual files if you do not call a task which already does that. Getting only the direct descendants Walk recursively gets all files in the given directory. To only get direct and directories in a given directory, use list . Todo What happens if the starting directory does not exist? What happens if it is not a directory? Does the filter also filter directories or only files? Should recursive directories automatically be declared required ? exists \u00b6 Checks if a file or folder exists. The syntax is exists $Exp , for example exists ./config.json . The expression is the path for which it should be checked if it exists. It returns a boolean indicating whether the file or path exists. read \u00b6 Reads the contents of the given file into a string . The syntax is read $Exp , for example read pie.sdf3 . The expression is the file to be read, with type path . The file is read with the system default file encoding. It returns a string with the contents of the file. Return \u00b6 Returns from the current function with the provided value. Its syntax is return $Exp , for example return true or return errResult<FileNotFoundException>(createFileNotFoundException(\"could not find $file\")) . The expression is evaluated and its value returned. The type of the expression should be a subtype of the declared return type of the current function . The type of a return expression is unit . Type may get changed to bottom type The type of a return expression may be changed to the bottom type in the future. This would allow using a return expression as a branch in an if-else expression. fail \u00b6 Throws an ExecException with the provided string as message. This exits the function, any code after this expression is not evaluated. Its syntax is fail $Exp , for example fail \"Could not open $file, it does not exist\" The expression is the message for the exception and must be of type string . The type of a fail expression is unit . Type may get changed to bottom type The type of a fail expression may be changed to the bottom type in the future. This would allow using a fail expression as a branch in an if-else expression. consider using Result<T, E> Fail throws an exception, which cannot be handled in the PIE DSL. We recommend using Result<T, E> from the PIE standard library instead. Unit literal \u00b6 unit is a literal expression of the only value of the unit type . True \u00b6 true is the literal expression for one of the two values of the boolean type . False \u00b6 false is the literal expression for one of the two values of the boolean type . int literal \u00b6 Int literals are a literal value of the int type . Their syntax is \"-\"? [0-9]+ , for example 0 , 1 , 2 , -1 , 47 and -30774 . That is an optional dash (unary minus, - ), followed by some digits. This syntax is lexical, meaning that there cannot be any layout between the sign or digits. valid int values Int literals represent literal values of the int type. As such, they must be valid values of the int type, i.e. in the range \\(-2^{31}\\) to \\(2^{31}-1\\) , inclusive. This is currently not enforced, and breaking this constraint will lead to Java compile errors after compiling the PIE code. -0 is allowed and equal to 0 . Examples // Valid integer literals: 0 1 234 2349273 -4 -237894 -0 // same as 0 0010 // same as 10 // invalid integer literals - 12 // not allowed to have a space between the minus and the digits. Unary minus is not supported. 1 024 // spaces between digits are not allowed 2,048 // commas between digits are not allowed 324.346 // periods between digits are not allowed. Floats do not exist in PIE, separators are not supported. DEADBEEF // letters are not allowed 0b0101011110010 // binary notation is not supported 0x234e9617ab7 // hexadecimal notation is not supported abd547 // this is not a hexadecimal value but a value reference ten // this is not an int literal but a value reference null \u00b6 null is the value that is added by making a type nullable . It is also a value of the top type . Tuple literal \u00b6 Tuple literals express literal values of the tuple type . Their syntax is ($Exps) , for example (\"scala\", \"VF_SCALA\", walk (subjectScalaSrcDir + ./lib/scala.jar)) . The expressions are the elements of the tuple. There must be at least two elements. The type of a tuple literal is the tuple type of the types of the elements, so the literal (1?, \"a string\", true) has type (int?, string, bool) . Tuples with less than two elements It is not possible to express tuples with zero or a single element. Zero element tuples cannot be expressed by design. Their common use case as a unit type is covered by the unit literal instead. Single element tuples will be parsed as a bracketed expression instead. Tuple literals from subtype elements Tuple types cannot be assigned to each other in most cases, so the following is not possible: data Fruit = $DataImpl data Apple : Fruit = $DataImpl data Pear : Fruit = $DataImpl func getApple() -> Apple = $FuncImpl func getPear() -> Pear = $FuncImpl // type (Apple, Pear) cannot be assigned to (Fruit, Fruit) func example() -> (Fruit, Fruit) = (getApple(), getPear()) To create such a tuple, assign the elements explicitly to the correct types first: func example() -> (Fruit, Fruit) = { val apple: Fruit = getApple(); val pear: Fruit = getPear(); (apple, pear) } List literal \u00b6 List keyword This section is about defining list literals. For information on the list keyword, see list expressions , which list the children of a directory. Define a literal list value. The syntax is [$Exps] , for example [1, 2, 3] , or [apple, banana, pear] . The expressions are the elements of the list. The least upper bound of the types of the expressions is the list element type T . The type of the list literal is a list of T , i.e. T* . The list element type must not be the top type. Empty list literals may lead to Java errors The empty list literal [] has a special type for implementation reasons. It compiles to a list with the bottom type . As such, the generated Java code may have compile errors. String literal \u00b6 Define a literal value of type string . The syntax is \"$StrParts\" , where $StrParts are parts of the string. String parts are lexical, which means that there cannot be any layout between them (layout between string parts will be part of the string). The possible string parts are: A sequence of characters excluding $ , \" , \\ and newlines, for example A sequence of characters = 123!? . This expresses that exact sequence of characters. $ followed by the name of a value or parameter, for example $dir . This converts the value to a string. It is an error to use an undefined name. ${$Exp} , for example ${1 + 2} . This evaluates the expression and converts the resulting value into a string. \\$ . This represents the literal character $ . \\ followed by another character. This represents a character according to Java semantics . For example, \\n is a newline character, \\\\ is a single backslash, and \\r is a carriage return character. All of the string parts are concatenated into a single string value without separating characters between them. path literal \u00b6 Define a literal value of type path . The syntax is $PathStart$PathParts , for example /home/alice/very-important-documents or ./src/test/resources/ . $PathStart is either / for absolute paths or ./ for relative paths. $PathParts are parts of the path. Path start and path parts are lexical, which means that there cannot be any layout between them (layout between path parts will result in parse errors). The possible path parts are: A sequence of characters excluding $ , \" , \\ and layout, for example path/to/the/goods . This expresses that exact sequence of characters. $ followed by the name of a value or parameter, for example $rootDir . The value must be of type path. It is an error to use an undefined name. ${$Exp} , for example ${getProjectDir()} . This evaluates the expression. The resulting value must be of type path. \\$ . This represents the literal character $ . \\ followed by another character. This represents a character according to Java semantics . For example, \\\\ is a single backslash. The path start and all of the path parts are concatenated into a single path value without separating characters between them. Path validity and existence (is not checked) The validity or existence of paths literals is not checked. This means that a path literal like .////. is allowed, even though it would be invalid for most file systems. To check if a path exists, use exists . Common lexical elements \u00b6 This section describes common lexical elements that are used by multiple expressions. Filter and FilterPart \u00b6 Filters are used expressions that read directories from the filesystem, so requires , list and walk . They are used to keep certain paths and ignore all other paths based on the name and the extension. They have the syntax with $Filter , for example with extension \"str\" or with patterns [\"test-res\", \"result\", \"generated\"] The possible filters are listed in the table below. name expression description Regex regex $Exp Keeps files if they match the provided regular expression. The expression must be a string representing a regular expression. Todo: Figure out what exactly it matches on (full path, name, includes extension?), regex flavor (Java, some other kind?) Pattern pattern $Exp Keeps files if the name contains the provided string. The expression must be a string . TODO: I assume that this only needs to match part of the name and does not include the extension Patterns patterns $Exp Keeps files if the name contains any of the provided strings. The expression must be a list of strings . TODO: I assume that this only needs to match part of the name and does not include the extension Extension extension $Exp Keeps files if the file extension matches the provided string. The extension must match the string exactly, so pie is different from PIE and PIE-simple . The string should not include the period ( . ) separating the filename and the file extension. The expression must be a string . TODO: I assume that it needs to be an exact match. Can it match pp.pie ? Extensions extensions $Exp Keeps files if the file extension matches any of the provided strings. The extension must match one of the strings exactly, so pie is different from PIE and PIE-simple . The strings should not include the period ( . ) separating the filename and the file extension. The expression must be a list of strings . TODO: I assume that it needs to be an exact match. Can it match pp.pie ? Todo Find exact semantics for the filters. Can they handle directories or do they only work on files? See also todos in the table. Multiple filters It is not allowed to use multiple filters. If you need multiple filters, encode your requirements in a regex filter instead. Stamper, StamperPart and StamperKind \u00b6 A Stamper specifies how it is determined whether a path is up-to-date when executing incrementally. They are used by requires and generates . They use the syntax by $StamperKind , where the stamper kind can be hash or modified . Stamping by hash will calculate the md5 hash of a file and assume that the file is up to date if the hash matches the cached hash. Stamping by modified will check the modification time, and assumes it is up-to-date when that time is at or before the cached time. Checking the full file contents There is currently no way in the PIE DSL to specify that the full file contents should match for a file to be considered up-to-date. If you need this, write the task in Java or use read file . Todo Does it work for directories or only files? Does the hash calculate md5 hash or another hash? What happens when a file is generated by modified but required by hash ? What is the default modifier?","title":"Expressions"},{"location":"references/pipelines/expressions/#expressions","text":"Function bodies in PIE DSL consist entirely of expressions. Every expression has a type and returns a value of that type, with the exception of fail and return . Expressions can use brackets to override the default priority rules, for example a && (b || c) . Brackets have the following form: ($Exp) This section describes expressions in the PIE DSL. Expressions can use declared values. These are described in this section of the functions documentation .","title":"Expressions"},{"location":"references/pipelines/expressions/#syntactic-priorities-disambiguation","text":"Nested expressions can be ambiguous, for example ! true && false could either ! (true && false) // = true or (!true) && false // = false . To solve these ambiguities, each expression has a priority. Expressions with higher priories will be nested in expressions with lower priority. The example above is parsed as (!true) && false because not has a higher priority than logical and . All expressions are left-associative, which means that if two expressions with the same priority are ambiguous, the leftmost expression will be nested in the rightmost expression. For example, 3 - 2 + 1 is equivalent to (3 - 2) + 1 . The following list lists expressions in order of descending priority. Expressions on the same number have the same priority. If an expression is not listed, it cannot be ambiguous (e.g. Blocks and list literals) Not Make nullable , Make non-nullable Addition Compare for (in)equality Logical and Logical or list , walk requires , generates read , exists Function calls , Method calls , Create supplier , Task supplier Filters Tuple literal , List literal List comprehension Value declaration return , fail if-else if","title":"Syntactic priorities (disambiguation)"},{"location":"references/pipelines/expressions/#quick-overview","text":"The following table gives a quick overview of all expressions in the PIE DSL. name syntax example description type Block {$Exps} {val x = 4; x+7} A sequence of expressions The type of the last expression Make nullable $Exp? \"nullable string\"? Makes the type T of an expression nullable ( T? ) When applied to an expression of type T , T? Make non-nullable $Exp! input! Makes the type T? of an expression non- nullable ( T ). Throws an exception if the value is null When applied to an expression of type T? , T Not !$Exp !flag Turns false to true and vice versa bool Compare for (in)equality $Exp == $Exp and Exp != $Exp result == null , errors != [] Compares two values for (in)equality bool Logical or $Exp || $Exp dryRun || input == null Is true unless both the values are false bool Logical and $Exp && $Exp !dryRun && input != null Is true iff both values are true bool Addition $Exp + $Exp x + y Adds or concatenates two values Depends on the types of the expressions Value declaration val $VALID TypeHint? = $Exp val num: int = 47 Declare a value by name The type of the declared value Value reference $VALID x Reference a value or parameter The type of the referenced value if if ($Exp) $Exp if (input == null) fail \"Input is null\" Evaluates the body if the condition evaluates to true unit if-else if ($Exp) $Exp else $Exp if (name != null) name else default Evaluates the first branch if the condition is true , and the second branch otherwise The least upper bound of the types of the branches List comprehension [$Exp | $Binder <- $Exp] [\"Key: $key; value: $value\" | (key, value) <- pairs] Evaluate the expression for each element in a list A list of the type of the expression Function calls $ModuleList$FUNCID$TypeArgs($Exps) stdLib:okResult<string>(\"Hello world!\") Call a function The return type of the function Method calls $Exp.$FUNCID$TypeArgs($Exps) file.replaceExtension(\"pp.pie\") Call a method The return type of the method Create supplier supplier$TypeArgs($Exps) supplier(47) Create a supplier A supplier of the provided type Task supplier $ModuleList$FUNCID.supplier$TypeArgs($Exps) lang:java:parse.supplier(file) Create a supplier from a function A supplier of the return type of the function requires requires $Exp $FilterPart? $StamperPart? requires ./metaborg.yaml by hash Declare that the current task depends on the provided path unit generates generates $Exp $StamperPart? generates file by hash Declare that the current task generates on the provided path unit list list $Exp $FilterPart? list ./examples with extension \"pie\" Lists the direct children of the given directory . Note: for list literals, see further down this table. A list of paths , i.e. path* walk walk $Exp $FilterPart? walk ./examples with extension \"pie\" Recursively get all descendants of the given directory A list of paths , i.e. path* exists exists $Exp exists ./config.json Check if a file exists bool read read $Exp read ./config.json Returns the file contents as a string, or null if the file does not exist A nullable string , i.e. string? return return $Exp return false Returns the provided value from the current function unit . Note: may get changed to bottom type fail fail $Exp fail \"input cannot be null\" Throws an ExecException with the provided string as message unit . Note: may get changed to bottom type Unit literal unit unit Literal expression of the only value of the unit type unit true true true Literal expression for the boolean value true . bool false false false Literal expression for the boolean value false . bool int literal \"-\"? [0-9]+ 0 , 23 , -4 A literal value of the int type int null null null Literal expression for the null value of nullable types . null type Tuple literal ($Exps) (1, \"one\", \"une\") A literal value of a tuple type A tuple type of the types of the elements List literal [$Exps] (1, 2, 3) A literal value of a list type . For the keyword list , see earlier in this table. A list of the least upper bound of the types of the elements String literal \"$StrParts\" \"Hello $name!\" A literal value of the string type string Path literal $PathStart$PathParts ./src/test/resources A literal value of the path type path There is also a section on common lexical elements . This covers filters and stampers .","title":"Quick overview"},{"location":"references/pipelines/expressions/#block","text":"Blocks are expressed as {$Exps} , where $Exps is a list of semi-colon separated expressions. Its type and value are those of the final expressions in the block. The final expression does not end with a semi-colon. For example: { val name = read ./example/name.txt; val greeting = read ./example/greeting.txt; \"$greeting $name\" } Empty blocks (blocks without any expression) are allowed, their type and value is unit . Blocks introduce their own scope. Expressions in the block are evaluated in that scope. Values declared in the block are not allowed to shadow values or parameters outside the block. This means that it is not allowed to declare a value with a name that already exists. Values declared before the block are still visible inside the block. Values declared inside the block are not visible after the block.","title":"Block"},{"location":"references/pipelines/expressions/#make-nullable","text":"Make nullable is expressed as a question mark after an expression. As its name suggests, it makes a non-nullable expression nullable. The value remains unchanged, but the type of the expression is now nullable . Its syntax is $Exp? , for example read ./name.txt == \"Bob\"? . It is an error to use this expression on an expression that was nullable already.","title":"Make nullable"},{"location":"references/pipelines/expressions/#make-non-nullable","text":"Make non-nullable is expressed as an exclamation mark after an expression. As its name suggests, it makes a nullable expression non-nullable. The value remains unchanged, but the type of the expression is now non-nullable . Its syntax is $Exp! , for example read file! . It is an error to use this expression on an expression that was non-nullable already.","title":"Make non-nullable"},{"location":"references/pipelines/expressions/#not","text":"Logical negation. It takes a boolean expression and returns the opposite boolean value. Its syntax is !$Exp , for example if (!exists file) fail \"$file should exist .","title":"Not"},{"location":"references/pipelines/expressions/#compare-for-inequality","text":"Compare two expressions for equality or inequality. Two values are considered equal if they are both null or if the equals method in the backing Java class for the first value returns true when applied to the second value. Otherwise, they are considered in-equal. The syntax for equals is $Exp == $Exp , for example x == null . The syntax for in-equals is $Exp != $Exp , for example x != null . Expressions can only be compared if one is a non-strict subtype of the other. This provides protection against accidentally comparing two expressions that can never be equal. Comparing null and empty lists Expressions with nullable types can have equal values despite not being subtypes of each other, if they are both null . The same goes for list types with the empty list [] . In these cases, check for these specific values: x == null && y == null .","title":"Compare for (in)equality"},{"location":"references/pipelines/expressions/#logical-or","text":"Logical or. Takes two boolean expressions and returns true if either of them is true and false if both are false . This operator short-circuits if the first expression is true , in which case the second expression will not be evaluated. Its syntax is $Exp || $Exp , for example exists file || default != null .","title":"Logical or"},{"location":"references/pipelines/expressions/#logical-and","text":"Logical and. Takes two boolean expressions and returns true if both of them are true and false if either of them is false . This operator short-circuits if the first expression is false , in which case the second expression will not be evaluated. Its syntax is $Exp && $Exp , for example configFile != null && exists configFile .","title":"logical and"},{"location":"references/pipelines/expressions/#addition","text":"The add operator is an overloaded in the PIE DSL. Its syntax is $Exp + $Exp . The type of adding two values depends on their static types. Adding two int s uses mathematical plus: 1 + 2 // result: 3 , and the result is an int as well. Adding any value to a string converts the value to a string and then concatenates the strings, resulting in a string : \"The value is:\" + x . String interpolation It might be clearer to use string interpolation. Adding a string or a path to a relative path concatenates the values and results in a new path : projectDir + ./src/test/resources/ Adding a string or a path to an absolute path results in a runtime error. Path interpolation It might be clearer to use path interpolation. Finally, adding a type T2 to a list with type T1* has two cases If T2 is a list as well both lists will be concatenated. The element type of T2 must be a subtype of T1 . Adding a list as an element To add a list list: T* as an element to a list of lists lists: T** , wrap the list in another list: lists + [list] Empty lists The PIE DSL keeps track of empty lists statically. This allows it to give a warning when concatenating an empty list: [1, 2, 3] + [] will give a warning. All other cases will append the second item to the first list. T2 must be a (non-strict) subtype of T1 . The element type of the resulting list is T1 , unless T2 is the null type . In that case, the element type of the resulting list is nullable as well.","title":"Addition"},{"location":"references/pipelines/expressions/#value-declaration","text":"Value declarations declare one or more named values. Expressions that are evaluated afterwards in the same scope or an inner scope can use the declared values by referencing them . For more information on values, see parameters and values . A basic value declaration declares a name with a value: val x = 9 . It is possible to give a type hint with the name: val y: int? = 8 . When a type hint is given, the type of the expression must be assignable to the type of the type hint. The type of the declared value is the provided type from the type hint, or the type of the expression if there is no type hint. A value declaration can also do tuple destructuring and assign its values to multiple variables at once: val (name: string, times: int*) = getPerformance(id) . Each name in a tuple destructuring can have a type hint. Tuple destructuring cannot be nested, so the following will not parse: val (a, (b, c)) = (1, (2, 3)) . Some examples of value declarations val firstName = \"Bob\"; // simple value declaration val age: int = 27; // with type hint val size: (int, int) = (800, 400); // assign tuple to single value. val (width, height) = size; // tuple destructuring // tuple destructuring with type hints val (name: string, in: path, out: path) = (\"expressions\", ./in/expressions.pie, ./out/expressions.java); // tuple destructuring with mixed type hints val (year, values: (string, bool)*) = (2020, []); Value declarations have the following syntax: val $Binder = $Exp , where the binder can be either a single binder $Bind or tuple binder ($Binds) , and binds can be only a name $VALID or a name with a type hint $VALID: Type . The type of a value declaration is the type of the variable(s) that it declares. It uses the type hint if available and the expression type otherwise. Single declarations have that single type, tuple declarations return a tuple of all the types that they declare.","title":"Value declaration"},{"location":"references/pipelines/expressions/#value-reference","text":"Value references reference a declared value or parameter by name. The name must be declared beforehand in the current scope or an outer scope. The type and value of a value reference is the type and value of the referenced value. The syntax is $VALID , for example: func incrementInefficiently(x: int) -> int = { val y = 1; val res = x + y; // reference parameter x and value y res // reference value res }","title":"Value reference"},{"location":"references/pipelines/expressions/#if","text":"An if expression conditionally evaluates an expression. It takes two expressions. The first one is the condition and is of type boolean . The second one is the body and can have any type. Its syntax is if ($Exp) $Exp , for example if (text == null) fail \"Could not read $file\" . If the condition evaluates to true , the body is evaluated, otherwise not. The type of an if expression is the unit type . The condition and body are evaluated in their own scope, so value declarations in an if expression are not visible after the if . Because an if expression is evaluated in its own scope and always returns the same value, the only use for an if expression is if the body has side-effects.","title":"if"},{"location":"references/pipelines/expressions/#if-else","text":"Conditionally evaluates one of two expressions. It takes three expressions. The first one is the condition and is of type boolean . The second one is the true-branch and can have any type. The third one is the false-branch and can also have any type. Its syntax is if ($Exp) $Exp else $Exp , for example if (name != null) name else default . If the condition evaluates to true , the true-branch is evaluated, otherwise the false-branch is evaluated. The type of an if-else expression is the least upper bound of both branches. It is an error if the least upper bound of the two branches is the top type . The condition and branches are evaluated in their own scope, so value declarations in an if-else expression are not visible after the expression. Some examples of the least upper bound of different types val cat1: Cat = getCat(1); val cat2: Cat = getCat(2); val mammal: Mammal = getMammal(); val animal: Animal = getAnimal(); val dog: Dog = getDog(); val fish: Fish = getFish(); val animal1: Animal = getAnimal(1); val animal2: Animal = getAnimal(2); val catBox1: Box<Cat> = box(cat1); val catBox2: Box<Cat> = box(cat2); val anyBox1: Box<_> = box(animal1); val anyBox2: Box<_> = box(catBox2); // same type if (flag) \"hello\" else \"world\" // type: string if (flag) 0 else 10 // type: int if (flag) cat1 else cat2 // type: Cat if (flag) animal1 else animal2 // type: Animal if (flag) catBox1 else catBox2 // type: Box<Cat> if (flag) anyBox1 else anyBox2 // type: Box<_> // subtypes if (flag) cat else mammal // type: Mammal if (flag) catBox else anyBox // type: Box<_> // different types if (flag) cat1 else dog // type: Mammal if (flag) dog else cat2 // type: Mammal if (flag) cat2 else fish // type: Animal if (flag) \"hello\" else 2 // type: top type, error","title":"if-else"},{"location":"references/pipelines/expressions/#list-comprehension","text":"List comprehensions apply an expression to every element of a list and return a new list with the new elements. They are special syntax for mapping a list, which would not ordinarily be possible in the PIE DSL because there are no higher-order functions. They have the syntax [$Exp | $Binder <- $Exp] , for example [\"Key: $key; value: $value\" | (key, value) <- pairs] The last expression is the input list and must have type T* for some T . The binder defines names for the elements. It can either be a single binder to bind the complete element, or a tuple binder if the element is a tuple, see value declarations for more explanation The first expression can use the names defined by the binder. The type of that expression is some type Q . The type of the full list comprehension is a list of the type that was mapped to, i.e. Q* . Empty lists A list comprehension over an empty list simply returns a new empty list. A list comprehension will give a warning if the input list is statically known to be empty. List comprehensions over empty lists are compiled to an immediate empty list of the declared type because it is not known what the element type of the empty list is.","title":"List comprehension"},{"location":"references/pipelines/expressions/#function-calls","text":"Function calls invoke a declared function . They have the syntax $ModuleList$FUNCID$TypeArgs($Exps) , for example stdLib:okResult<string>(\"Hello world!\") . The second element is the function name . This function name can either be qualified or left unqualified by the module list. If it is unqualified, the function name must be defined in the current module or be imported with a function import . If it is qualified, the function is looked up in that module . The number of type arguments must match the number of type parameters on the function declaration , and the type arguments must be within bounds for the type parameters. The expressions are the arguments to the function. They must match the number of parameters that the function declared and they must be subtypes of the parameters. The type of a call is the type of the declared function, where type parameters are replaced with their corresponding type arguments. Return type is a generic parameter func id<T>(param: T) -> T = param func test() -> unit = { id<string>(\"Hello world!\"); // type is string id<int>(42); // type is int unit } Arguments can declare values Value declarations in the arguments are legal and are visible after the call. Doing this is bad practice in almost all cases. Declare the value before the call instead. For example: // declares the value `x` with value 9 // also passes 9 as argument to `test` test(val x = 9); x // x is visible after the call. Better way: val x = 9; // declare before call test(x); // refer to declared value x","title":"Function calls"},{"location":"references/pipelines/expressions/#method-calls","text":"Method calls invoke a declared method . They have the syntax $Exp.$FUNCID$TypeArgs($Exps) , for example file.replaceExtension(\"pp.pie\") . The first element is an arbitrary expression. The second element is the method name. This method is looked up on the type of the first expression. The number of type arguments must match the number of type parameters on the method declaration , and the type arguments must be within bounds for the type parameters. The expressions are the arguments to the method. They must match the number of parameters that the method declared and they must be subtypes of the parameters. The type of a method call is the type of the declared method, where type parameters are replaced with their corresponding type arguments. No methods on nullable types There are no methods defined on nullable types . To access the methods of the inner type, cast the expression to non-nullable first: val maybe: Result<string, _ : Exception> = null; maybe.unwrap(); // error: Cannot call method on nullable type maybe!.unwrap(); // type checks, but will throw a run time exception // Better: handle null value before casting if (maybe == null) { // handle null value here \"Cannot get value, result is null\" } else { maybe!.unwrap() } Return type is a generic parameter data Box<T> = foreign java org.example.methodCall.Box { func get() -> T } func test(box1: Box<int>, box2: Box<bool>) -> unit = { box1.get(); // type is int box2.get(); // type is bool unit } Expression and arguments can declare values Value declarations in the expression or the arguments are legal and are visible after the call. Declarations in the expression are also visible to the expressions. Doing this is bad practice in almost all cases. Declare the value before the call instead. For example: // declares the value `name` and `msg` // also passes 9 as argument to `test` // Note: getName returns an stdLib:Result<string, _ : Exception> (val name = getName()).unwrapOrDefault( // `name` is visible inside the arguments val msg = \"Could not get name, exception: ${if (val ok = name.isOk()) \"No error\" else name.unwrapErr() }\" ); (name, ok, msg); // `name`, `ok` and `message` are visible after the call. Better way: val name = getName(); val ok = name.isOk(); val msg = if (ok) \"Could not get name, exception: No error\" else \"Could not get name, exception: ${name.unwrapErr()}\"","title":"Method calls"},{"location":"references/pipelines/expressions/#create-supplier","text":"A supplier for a value can be created with the supplier keyword. It has the syntax supplier$TypeArgs($Exps) , for example supplier(47) or supplier<string>(\"Hello world!\") . The type arguments can either be omitted or must be a single type argument. The expressions are the arguments for the supplier. There should be only one argument, the value that the supplier will supply. The type T for the supplier is the type argument if it was provided, or the type of the argument otherwise. In case a type argument is provided, the argument should be a subtype of that type argument. The type of a supplier creation expression is supplier<T> . Supplier can be treated as a normal function Creating a supplier is like a normal function call , but built into the language grammar for implementation reasons. This is the only function call where the type argument is derived at the moment.","title":"create supplier"},{"location":"references/pipelines/expressions/#task-supplier","text":"A task supplier expression creates a supplier from a function. A task supplier expression does not execute the task yet, but instead defers it until the supplier's get method is called. It has the syntax $ModuleList$FUNCID.supplier$TypeArgs($Exps) , for example lang:java:parse.supplier(file) . The second element is the function name . This function name can either be qualified or left unqualified by the module list. If it is unqualified, the function name must be defined in the current module or be imported with a function import . If it is qualified, the function is looked up in that module . The number of type arguments must match the number of type parameters on the function declaration , and the type arguments must be within bounds for the type parameters. The expressions are the arguments to the function. They must match the number of parameters that the function declared and they must be subtypes of the parameters. The type of a task supplier expression is supplier<T> , where T is the type of the declared function with the type parameters replaced with their corresponding type arguments.","title":"task supplier"},{"location":"references/pipelines/expressions/#requires","text":"A requires expression expresses that the current task depends on the given path . It has the syntax requires $Exp $FilterPart? $StamperPart? , for example requires ./metaborg.yaml by hash or requires sampleDir with extension \"sdf3\" . The expression is the path to depend on. The filter part is optional and adds a filter to filter out any paths that do not match the filter. It is described in the section on common lexical elements . The stamper part is also optional and provides a way to determine if a file or path is up-to-date. It is also described in the section on common lexical elements . The type and value of the expression is unit . Exceptions? What happens if there is another task that provides the path? Does it quietly schedule that task before this one, does it throw an error? What if that other task runs after this task?","title":"requires"},{"location":"references/pipelines/expressions/#generates","text":"Marks the given path as provided by the current task. It has the syntax generates $Exp $StamperPart? , for example generates file by hash . The expression is the path to depend on. The stamper part is optional and provides a way to determine if a file or path is up-to-date. It is described in the section on common lexical elements . The type and value of this expression is unit . Make file modifications before using this expression The contents or metadata of the file at the time that this expression is called may be cached and used for incrementality. Make all modifications to the file before using this expression. Exceptions? Can this mark a directory as provided or only a file? What happens when two tasks generate a file? What happens when this task declares it generates a file after another task has already used it? Can a task both require and provide a file? What happens if this task calls a task that provides a file and then this task also declares it generated that file?","title":"generates"},{"location":"references/pipelines/expressions/#list","text":"Defining list literals This section is about listing children of a directory with the list keyword. To define a literal list value, see list literal . Lists direct children of the given directory. List expressions have the syntax list $Exp $FilterPart? , for example list getProjectRootDir() + ./examples with extension \"pie\" . The expression must have type path and refer to an existing directory. The filter part is optional and adds a filter to filter out any paths that do not match the filter. It is described in the section on common lexical elements . A list expression returns a list of the direct children of the given directory, and its type is path* . Declaring a dependency on the directory You will likely need to declare a dependency on the directory using requires . You may also need to declare dependencies on the individual files if you do not call a task which already does that. Recursive listing List only gets the direct children of the given directory. To recursively get all files and directories in a given directory, use walk . Todo What happens if the starting directory does not exist? What happens if it is not a directory?","title":"list"},{"location":"references/pipelines/expressions/#walk","text":"Recursively gets descendants of the given directory. Walk expressions have the syntax walk $Exp $FilterPart? , for example walk getProjectRootDir() + ./src/test/java with extension \"java\" . The expression must have type path and refer to an existing directory. The filter part is optional and adds a filter to filter out any files that do not match the filter. It is described in the section on common lexical elements . A walk expression returns a list of all the files in the given directory and its descendants, and its type is path* . Declaring a dependency on the directory You will likely need to declare a dependency on the directory and all subdirectories using requires . You may also need to declare dependencies on the individual files if you do not call a task which already does that. Getting only the direct descendants Walk recursively gets all files in the given directory. To only get direct and directories in a given directory, use list . Todo What happens if the starting directory does not exist? What happens if it is not a directory? Does the filter also filter directories or only files? Should recursive directories automatically be declared required ?","title":"walk"},{"location":"references/pipelines/expressions/#exists","text":"Checks if a file or folder exists. The syntax is exists $Exp , for example exists ./config.json . The expression is the path for which it should be checked if it exists. It returns a boolean indicating whether the file or path exists.","title":"exists"},{"location":"references/pipelines/expressions/#read","text":"Reads the contents of the given file into a string . The syntax is read $Exp , for example read pie.sdf3 . The expression is the file to be read, with type path . The file is read with the system default file encoding. It returns a string with the contents of the file.","title":"read"},{"location":"references/pipelines/expressions/#return","text":"Returns from the current function with the provided value. Its syntax is return $Exp , for example return true or return errResult<FileNotFoundException>(createFileNotFoundException(\"could not find $file\")) . The expression is evaluated and its value returned. The type of the expression should be a subtype of the declared return type of the current function . The type of a return expression is unit . Type may get changed to bottom type The type of a return expression may be changed to the bottom type in the future. This would allow using a return expression as a branch in an if-else expression.","title":"Return"},{"location":"references/pipelines/expressions/#fail","text":"Throws an ExecException with the provided string as message. This exits the function, any code after this expression is not evaluated. Its syntax is fail $Exp , for example fail \"Could not open $file, it does not exist\" The expression is the message for the exception and must be of type string . The type of a fail expression is unit . Type may get changed to bottom type The type of a fail expression may be changed to the bottom type in the future. This would allow using a fail expression as a branch in an if-else expression. consider using Result<T, E> Fail throws an exception, which cannot be handled in the PIE DSL. We recommend using Result<T, E> from the PIE standard library instead.","title":"fail"},{"location":"references/pipelines/expressions/#unit-literal","text":"unit is a literal expression of the only value of the unit type .","title":"Unit literal"},{"location":"references/pipelines/expressions/#true","text":"true is the literal expression for one of the two values of the boolean type .","title":"True"},{"location":"references/pipelines/expressions/#false","text":"false is the literal expression for one of the two values of the boolean type .","title":"False"},{"location":"references/pipelines/expressions/#int-literal","text":"Int literals are a literal value of the int type . Their syntax is \"-\"? [0-9]+ , for example 0 , 1 , 2 , -1 , 47 and -30774 . That is an optional dash (unary minus, - ), followed by some digits. This syntax is lexical, meaning that there cannot be any layout between the sign or digits. valid int values Int literals represent literal values of the int type. As such, they must be valid values of the int type, i.e. in the range \\(-2^{31}\\) to \\(2^{31}-1\\) , inclusive. This is currently not enforced, and breaking this constraint will lead to Java compile errors after compiling the PIE code. -0 is allowed and equal to 0 . Examples // Valid integer literals: 0 1 234 2349273 -4 -237894 -0 // same as 0 0010 // same as 10 // invalid integer literals - 12 // not allowed to have a space between the minus and the digits. Unary minus is not supported. 1 024 // spaces between digits are not allowed 2,048 // commas between digits are not allowed 324.346 // periods between digits are not allowed. Floats do not exist in PIE, separators are not supported. DEADBEEF // letters are not allowed 0b0101011110010 // binary notation is not supported 0x234e9617ab7 // hexadecimal notation is not supported abd547 // this is not a hexadecimal value but a value reference ten // this is not an int literal but a value reference","title":"int literal"},{"location":"references/pipelines/expressions/#null","text":"null is the value that is added by making a type nullable . It is also a value of the top type .","title":"null"},{"location":"references/pipelines/expressions/#tuple-literal","text":"Tuple literals express literal values of the tuple type . Their syntax is ($Exps) , for example (\"scala\", \"VF_SCALA\", walk (subjectScalaSrcDir + ./lib/scala.jar)) . The expressions are the elements of the tuple. There must be at least two elements. The type of a tuple literal is the tuple type of the types of the elements, so the literal (1?, \"a string\", true) has type (int?, string, bool) . Tuples with less than two elements It is not possible to express tuples with zero or a single element. Zero element tuples cannot be expressed by design. Their common use case as a unit type is covered by the unit literal instead. Single element tuples will be parsed as a bracketed expression instead. Tuple literals from subtype elements Tuple types cannot be assigned to each other in most cases, so the following is not possible: data Fruit = $DataImpl data Apple : Fruit = $DataImpl data Pear : Fruit = $DataImpl func getApple() -> Apple = $FuncImpl func getPear() -> Pear = $FuncImpl // type (Apple, Pear) cannot be assigned to (Fruit, Fruit) func example() -> (Fruit, Fruit) = (getApple(), getPear()) To create such a tuple, assign the elements explicitly to the correct types first: func example() -> (Fruit, Fruit) = { val apple: Fruit = getApple(); val pear: Fruit = getPear(); (apple, pear) }","title":"Tuple literal"},{"location":"references/pipelines/expressions/#list-literal","text":"List keyword This section is about defining list literals. For information on the list keyword, see list expressions , which list the children of a directory. Define a literal list value. The syntax is [$Exps] , for example [1, 2, 3] , or [apple, banana, pear] . The expressions are the elements of the list. The least upper bound of the types of the expressions is the list element type T . The type of the list literal is a list of T , i.e. T* . The list element type must not be the top type. Empty list literals may lead to Java errors The empty list literal [] has a special type for implementation reasons. It compiles to a list with the bottom type . As such, the generated Java code may have compile errors.","title":"List literal"},{"location":"references/pipelines/expressions/#string-literal","text":"Define a literal value of type string . The syntax is \"$StrParts\" , where $StrParts are parts of the string. String parts are lexical, which means that there cannot be any layout between them (layout between string parts will be part of the string). The possible string parts are: A sequence of characters excluding $ , \" , \\ and newlines, for example A sequence of characters = 123!? . This expresses that exact sequence of characters. $ followed by the name of a value or parameter, for example $dir . This converts the value to a string. It is an error to use an undefined name. ${$Exp} , for example ${1 + 2} . This evaluates the expression and converts the resulting value into a string. \\$ . This represents the literal character $ . \\ followed by another character. This represents a character according to Java semantics . For example, \\n is a newline character, \\\\ is a single backslash, and \\r is a carriage return character. All of the string parts are concatenated into a single string value without separating characters between them.","title":"String literal"},{"location":"references/pipelines/expressions/#path-literal","text":"Define a literal value of type path . The syntax is $PathStart$PathParts , for example /home/alice/very-important-documents or ./src/test/resources/ . $PathStart is either / for absolute paths or ./ for relative paths. $PathParts are parts of the path. Path start and path parts are lexical, which means that there cannot be any layout between them (layout between path parts will result in parse errors). The possible path parts are: A sequence of characters excluding $ , \" , \\ and layout, for example path/to/the/goods . This expresses that exact sequence of characters. $ followed by the name of a value or parameter, for example $rootDir . The value must be of type path. It is an error to use an undefined name. ${$Exp} , for example ${getProjectDir()} . This evaluates the expression. The resulting value must be of type path. \\$ . This represents the literal character $ . \\ followed by another character. This represents a character according to Java semantics . For example, \\\\ is a single backslash. The path start and all of the path parts are concatenated into a single path value without separating characters between them. Path validity and existence (is not checked) The validity or existence of paths literals is not checked. This means that a path literal like .////. is allowed, even though it would be invalid for most file systems. To check if a path exists, use exists .","title":"path literal"},{"location":"references/pipelines/expressions/#common-lexical-elements","text":"This section describes common lexical elements that are used by multiple expressions.","title":"Common lexical elements"},{"location":"references/pipelines/expressions/#filter-and-filterpart","text":"Filters are used expressions that read directories from the filesystem, so requires , list and walk . They are used to keep certain paths and ignore all other paths based on the name and the extension. They have the syntax with $Filter , for example with extension \"str\" or with patterns [\"test-res\", \"result\", \"generated\"] The possible filters are listed in the table below. name expression description Regex regex $Exp Keeps files if they match the provided regular expression. The expression must be a string representing a regular expression. Todo: Figure out what exactly it matches on (full path, name, includes extension?), regex flavor (Java, some other kind?) Pattern pattern $Exp Keeps files if the name contains the provided string. The expression must be a string . TODO: I assume that this only needs to match part of the name and does not include the extension Patterns patterns $Exp Keeps files if the name contains any of the provided strings. The expression must be a list of strings . TODO: I assume that this only needs to match part of the name and does not include the extension Extension extension $Exp Keeps files if the file extension matches the provided string. The extension must match the string exactly, so pie is different from PIE and PIE-simple . The string should not include the period ( . ) separating the filename and the file extension. The expression must be a string . TODO: I assume that it needs to be an exact match. Can it match pp.pie ? Extensions extensions $Exp Keeps files if the file extension matches any of the provided strings. The extension must match one of the strings exactly, so pie is different from PIE and PIE-simple . The strings should not include the period ( . ) separating the filename and the file extension. The expression must be a list of strings . TODO: I assume that it needs to be an exact match. Can it match pp.pie ? Todo Find exact semantics for the filters. Can they handle directories or do they only work on files? See also todos in the table. Multiple filters It is not allowed to use multiple filters. If you need multiple filters, encode your requirements in a regex filter instead.","title":"Filter and FilterPart"},{"location":"references/pipelines/expressions/#stamper-stamperpart-and-stamperkind","text":"A Stamper specifies how it is determined whether a path is up-to-date when executing incrementally. They are used by requires and generates . They use the syntax by $StamperKind , where the stamper kind can be hash or modified . Stamping by hash will calculate the md5 hash of a file and assume that the file is up to date if the hash matches the cached hash. Stamping by modified will check the modification time, and assumes it is up-to-date when that time is at or before the cached time. Checking the full file contents There is currently no way in the PIE DSL to specify that the full file contents should match for a file to be considered up-to-date. If you need this, write the task in Java or use read file . Todo Does it work for directories or only files? Does the hash calculate md5 hash or another hash? What happens when a file is generated by modified but required by hash ? What is the default modifier?","title":"Stamper, StamperPart and StamperKind"},{"location":"references/pipelines/functions/","text":"Functions \u00b6 This section describes functions in the PIE DSL. Note A task is a function with some special semantics in regards to runtime behavior. The PIE DSL does not differentiate between functions and tasks. In the DSL, both are called functions. Todo Write documentation Name \u00b6 Todo describe legal function names, functions must be unique within a module (including function imports) Foreign java functions \u00b6 Type parameters \u00b6 Parameters and values \u00b6 Todo describe how values and parameters behave Return type \u00b6 Todo describe the return value Function invocations \u00b6 Todo describe function invocations","title":"Functions"},{"location":"references/pipelines/functions/#functions","text":"This section describes functions in the PIE DSL. Note A task is a function with some special semantics in regards to runtime behavior. The PIE DSL does not differentiate between functions and tasks. In the DSL, both are called functions. Todo Write documentation","title":"Functions"},{"location":"references/pipelines/functions/#name","text":"Todo describe legal function names, functions must be unique within a module (including function imports)","title":"Name"},{"location":"references/pipelines/functions/#foreign-java-functions","text":"","title":"Foreign java functions"},{"location":"references/pipelines/functions/#type-parameters","text":"","title":"Type parameters"},{"location":"references/pipelines/functions/#parameters-and-values","text":"Todo describe how values and parameters behave","title":"Parameters and values"},{"location":"references/pipelines/functions/#return-type","text":"Todo describe the return value","title":"Return type"},{"location":"references/pipelines/functions/#function-invocations","text":"Todo describe function invocations","title":"Function invocations"},{"location":"references/pipelines/generics/","text":"Generics \u00b6 This section describes generics in the PIE DSL. In a nutshell, it just follows the Java semantics. Todo Write documentation","title":"Generics"},{"location":"references/pipelines/generics/#generics","text":"This section describes generics in the PIE DSL. In a nutshell, it just follows the Java semantics. Todo Write documentation","title":"Generics"},{"location":"references/pipelines/limitations/","text":"Known bugs and limitations \u00b6 This page lists the known bugs and limitations of the PIE DSL, and workarounds if they exist. If you have an issue that is not listed here, please check if it is listed on our Github page, and if not, open a new issue there: https://github.com/metaborg/pie/issues . Todo Also rename this section / page? Compiler generates invalid code for void return types \u00b6 The Java type void does not exist in the PIE DSL. Instead, it uses unit to signify that the return type does not hold a useful value. Right now, the compiler will generate incorrect code for void functions, which results in Java compile errors in the generated code. There is not a specific workaround for this, but the standard workarounds for unsupported Java functions should work. In the future, the compiler will support functions that are declared to have a unit return type but which have void return type. Standard workarounds for unsupported Java features \u00b6 The PIE DSL has less features than Java by design. This sometimes means that Java elements cannot be used as is in the PIE DSL. This sections lists some standard workarounds for these issues. Standard workarounds for unsupported Java functions \u00b6 This section provides workarounds for calling Java functions that are not supported by the PIE DSL. Todo Write a wrapper function Implement the task in Java. (Do not call the function from the PIE DSL, but call it from Java instead) Standard workarounds for unsupported Java data types \u00b6 Todo","title":"Known bugs and limitations"},{"location":"references/pipelines/limitations/#known-bugs-and-limitations","text":"This page lists the known bugs and limitations of the PIE DSL, and workarounds if they exist. If you have an issue that is not listed here, please check if it is listed on our Github page, and if not, open a new issue there: https://github.com/metaborg/pie/issues . Todo Also rename this section / page?","title":"Known bugs and limitations"},{"location":"references/pipelines/limitations/#compiler-generates-invalid-code-for-void-return-types","text":"The Java type void does not exist in the PIE DSL. Instead, it uses unit to signify that the return type does not hold a useful value. Right now, the compiler will generate incorrect code for void functions, which results in Java compile errors in the generated code. There is not a specific workaround for this, but the standard workarounds for unsupported Java functions should work. In the future, the compiler will support functions that are declared to have a unit return type but which have void return type.","title":"Compiler generates invalid code for void return types"},{"location":"references/pipelines/limitations/#standard-workarounds-for-unsupported-java-features","text":"The PIE DSL has less features than Java by design. This sometimes means that Java elements cannot be used as is in the PIE DSL. This sections lists some standard workarounds for these issues.","title":"Standard workarounds for unsupported Java features"},{"location":"references/pipelines/limitations/#standard-workarounds-for-unsupported-java-functions","text":"This section provides workarounds for calling Java functions that are not supported by the PIE DSL. Todo Write a wrapper function Implement the task in Java. (Do not call the function from the PIE DSL, but call it from Java instead)","title":"Standard workarounds for unsupported Java functions"},{"location":"references/pipelines/limitations/#standard-workarounds-for-unsupported-java-data-types","text":"Todo","title":"Standard workarounds for unsupported Java data types"},{"location":"references/pipelines/modules/","text":"Module system \u00b6 This section describes the module system of the PIE DSL. Todo document the module system Imports \u00b6 Function imports \u00b6 Qualified calls \u00b6","title":"Module system"},{"location":"references/pipelines/modules/#module-system","text":"This section describes the module system of the PIE DSL. Todo document the module system","title":"Module system"},{"location":"references/pipelines/modules/#imports","text":"","title":"Imports"},{"location":"references/pipelines/modules/#function-imports","text":"","title":"Function imports"},{"location":"references/pipelines/modules/#qualified-calls","text":"","title":"Qualified calls"},{"location":"references/pipelines/types/","text":"Types \u00b6 This page explains the type system of the PIE DSL and lists all the built-in types. The type system \u00b6 A type system checks for type errors. PIE DSL uses a static type system, so type errors are found before compilation. The PIE DSL supports subtypes, generics and methods on types. Nullability \u00b6 PIE DSL keeps explicit track of nullability, so an expression cannot be null unless the type of the expression is nullable . Todo Fully document nullability. Super types and subtypes \u00b6 When a type X is a subtype of type Y , you can use an expression with type X wherever an expression with type Y is expected. When X is a subtype of Y , Y is the super type of X For example, the method cutFruit(fruit: Fruit) -> Piece* expects a Fruit . If an Apple is declared to be a subtype of Fruit , we can pass an Apple : cutFruit(getApple()) Subtypes are transitive, which means that if A is a subtype of B , and B is a subtype of C , then A is a subtype of C , even if that is not explicitly declared. The PIE DSL allows declaring exactly one super type. The top type is a super type of everything. As such, any expression can be assigned to the top type. The bottom type is a subtype of everything. As such, an expression with the bottom type can be assigned to everything. Generics \u00b6 Generics refer to making a type or a function generic over types, that is, add type parameters to the type or function. You likely already know about these, lists are an example of parameterized types. It's not just any list, it's a list of some type here . Every data type in the PIE DSL is generic. When declaring a datatype in a data definition, there is a list of generic parameters: data Box<T> = $DataImpl . When referring to a type, there is a list of generic parameters: val box: Box<string> = $Exp . These lists can be empty: data Apple<> = $DataImpl . This makes the term \"generic\" a bit meaningless, but Apple is still considered generic for the purposes of the semantics. That is to say, Apple is treated as a generic type that just happens to have zero generic parameters, in the same way that Box happens to have one generic parameter and Foo<A, B, C> happens to have three. Omitting type parameter and argument lists Type parameter/argument list can sometimes be omitted. Parameter lists can be omitted if they are empty: data Apple = $DataImpl , func notGeneric(string) -> unit = $FuncImpl . Type argument lists can be omitted for empty lists and for calls to the built-in supplier function . For example, Apple , notGeneric(\"regular argument\") . A type declares zero or more generic parameters. These can be used in methods of that type to parameterize the method. A function or method can also declare generic parameters to be used in that function or method. PIE does not derive bounds Unlike Java, the PIE DSL does not derive bounds for datatypes based on super types. The following would be possible in Java but will not work in the PIE DSL: data CustomAnimalSet<A : Animal> = $DataImpl data OrderedAnimalSet<T> : CustomAnimalSet<T> = $DataImpl // error on `T` in `CustomAnimalSet<T>`: `T` is not within upper bound `Animal` // Solution: declare the bound on the subtype as well data OrderedAnimalSet2<T : Animal> : CustomAnimalSet<T> = $DataImpl Todo Explain generics Methods and overriding \u00b6 Every type can have methods. For now, the only data types are foreign java datatypes, so methods follow Java semantics for overriding. Todo Explain overriding. Built-in types \u00b6 The PIE DSL has several built-in types. This section explains all of them. Datatype equality with equal Java class PIE DSL does not consider types equivalent when their backing Java class is equal. This means that a built-in type and a custom datatype backed by the same class cannot be used interchangeably. The following table gives a quick overview of the built-in types, click on the name to go to the documentation. name syntax description values methods backing Java class unit unit Unit type, only has a single value. Use as return type for methods without a meaningful return value unit - mb.pie.api.None bool bool Booleans. Used as flags and for conditions in if and if-else . true , false - java.lang.Boolean int int Integers. Range from \\(-2^{31}\\) to \\(2^{31}-1\\) , inclusive. -2147483648, ... -1, 0, 1, 2, 3, ... 2147483647 - java.lang.Integer path path Paths on the file system. Might not exist. E.g. ./src/test/resources/test1.txt , /home/users/me/programming - mb.resource.fs.FSPath null type - Type of the literal null . Subtype of every nullable type . null - - top - The top type is the super type of all other types. every value is an instance of top - java.lang.Object bottom - The bottom type is a subtype of all other types. It has no values by definition. An expression of type bottom will never return normally. No values - - Nullable types $Type? , e.g. string? Makes a type nullable. All values of the original type and null All methods of the original type Original backing class Lists $Type* , e.g. int* A list. Unknown amount of elements, all with the same type. - [] , [e1] , [e1, e2] , [e1, e2, e3] , where e1 , e2 and e3 are valid elements of the list element type. java.util.ArrayList Tuples ($Types) A tuple of elements. Known amount of elements, can be different types. elements of the inner types, e.g. (e1, e2) is a value of (T1, T2) if e1 and e2 are values of T1 and T2 respectively - mb.pie.TupleX , where X is the number of elements. supplier supplier<$Type> A supplier of a value. Useful for performance in certain situations. Can be created using supplier($Exp) or $FUNCID.supplier<$TypeArgs>($Exps) func get<>() -> T for supplier<T> mb.pie.api.Supplier Function types - The type of a function. Functions cannot be used as values, but their type can be seen by hovering over the name - - - Wildcards _$UpperBound$LowerBound Represent a set of types. Can only be used as type argument. Instances of types in the type set - Backed by the Java wildcard: ? Custom datatypes $TYPEID A type defined in a pie file with. Instances of the type, ultimately obtained from foreign java functions The methods that are declared on the type itself, and the methods of its super types The declared backing class unit \u00b6 unit is a type with only a single value: unit . It is meant to be used as return value for functions that have no meaningful return value, for example functions that operate via side effects like writing to a file. It is backed by mb.pie.api.None . bool \u00b6 bool represents booleans and as such has two values: true and false . Booleans are used as flags and as conditions for if and if-else . bool is backed by java.lang.Boolean . int \u00b6 int represents integers. It is backed by java.lang.Integer , and as such has a range of \\(-2^{31}\\) to \\(2^{31}-1\\) , inclusive. string \u00b6 string represents strings. Strings have many built-in methods which have yet to be added to the implementation. It is backed by java String . path \u00b6 path represents a path to a file or directory in the file system. The directory or file need not exist. Paths can be relative or absolute. Paths have many built-in methods which have yet to be added to the implementation. Paths are backed by mb.resource.fs.FSPath null type \u00b6 The null type cannot be expressed in the PIE DSL, meaning that there is no way to specify it as the type of something. Its only value is null . The null type is a subtype of every nullable type. top \u00b6 The top type is a super type of every other type. It cannot be specified as a type. It is backed by java.lang.Object bottom \u00b6 The bottom type is a subtype of every other type. It cannot be specified as a type. The bottom type has no values, and as such an expression with bottom type will never return normally to the function it is defined in. It is the element type of empty lists, and in the future also of return and fail expressions. This type is not backed by any java class. Compiling bottom type Code that has the bottom type will fail to compile. Remove the code that has bottom type to resolve this. Nullable types \u00b6 Nullable types are represented with a question mark after the type. For example, a nullable int is int? . A nullable type X? represents a value that could either be a regular value X or \"missing\", represented with the expression null . A nullable type X? is a super type of both X and the null type. It is an error to make a nullable type nullable again, so X?? is not allowed. Java types are always nullable, so the nullable type X? is backed by Java type X . Lists \u00b6 Lists are represented with an asterisk behind the type. For example, a nullable path is path* . Lists of X can contain any element that could be assigned to X . Lists do not have subtypes besides the bottom type. This means that Apple* is not a subtype of Fruit* . Lists do not have methods yet. Lists are backed by Java java.util.ArrayList . Empty lists The PIE DSL type system keeps track of empty lists for implementation reasons. Because it is doing this anyway, it gives warnings when doing certain non-sensical things such as appending an empty list to another list or list comprehension over empty lists. Tuples \u00b6 Tuple types represent a combination of multiple types. They are specified as the types between parentheses. For example, (string, int*) represents a pair of a string and a list of int s. Tuple types differ from lists because lists have a variable amount of elements of a single type, while tuples have a set number of elements with heterogenous types. Tuple types can be deconstructed to get their values: val pair: (string, int*) = (\"Alice\", [9, 4, 6, 7]); val (name: string, grades: int*) = pair; Tuples are backed by Java classes mb.pie.TupleX , where X is a number representing the amount of elements, e.g. Tuple2 for a pair. This is because Java is not generic in the amount of generic elements. Limits on tuple sizes While the PIE DSL language does not specify a limit on the amount of elements in a tuple, the backing Java TupleX classes only go up to 10. If you run into this limit, use a foreign data type backed by a custom Java class instead. supplier \u00b6 supplier<T> represents a supplier of a value of type T . Suppliers represent a value, either by being created with a value or by deferring a task that returns the value . Suppliers have a single method get<>() -> T , which returns the value of the supplier, either by returning the value if it already existed or by calling the task that the supplier supplies. The main use case for suppliers is as input types for tasks. If the supplier is faster to check for consistency than the value it supplies, the runtime performance is improved. As an example, consider func readFile() -> string = read ./bundled.java func parse1(program: string) -> IStrategoTerm = mb:lang:java:parse(program) func parse2(program: supplier<string>) -> IStrategoTerm = mb:lang:java:parse(program.get<>()) func parseBoth() -> unit = { parse1(readFile()); parse2(readfile.supplier()); unit } Both parse1 and parse2 need to read the file ./bundled.java , strip of any whitespace, and then parse it with mb:lang:java:parse on the initial build. If we modify ./bundled.java before the second build, readFile is now outdated and will need to read again. To check if the input for parse1 is in the cache, the runtime needs to check the contents of the entire file against any cached values to see if it matches. To check if the input for parse2 is in the cache, the runtime only checks if the supplier is in the cache. The supplier is a TaskSupplier , which is in the cache if its task is not outdated. The runtime only has to make a few calls to determine that the input for parse2 is not cached. Suppliers are backed by mb.pie.api.Supplier . Function types \u00b6 Function types are visible when hovering over a function name. They follow the pattern func($Params) -> $Type . For example, func(int, string) -> bool is a function that takes an int and a string and returns a boolean. Function types cannot be specified in PIE DSL, and they can also not be the type of a variable. Because function types cannot be the type of a variable, they are not backed by a Java class. Wildcards \u00b6 Wildcards represent a set of types by using an upper or lower bound. They use the following syntax. The wildcard itself is represented by an underscore: _ The upper bound is specified by a colon followed by a type: : $Type The lower bound is specified with a dash, colon and then a type: -: $Type If a the upper bound is omitted, it is implicitly the top type. If the lower bound is omitted, it is implicitly the bottom type. A wildcard cannot have both an upper and a lower bound. Here are some examples of wildcards and what they mean: _ // unbounded wildcard (bounds are implicitly the top and bottom type) _ : Fruit // upper bounded wildcard. Matches Fruit, Apple, Pear _ : path:to:module:Vegetable // qualified upper bound _ -: Fruit // lower bounded wildcard. Matches Fruit, Food, top type _ : Fruit -: Apple // both upper and lower bounded, gives an error _ : Iterable : Comparable // _not_ a type with multiple upper bounds, parsed as _ : Iterable:Comparable // a single qualified upper bound. _ -: Apple : Fruit // _not_ a lower bound and then upper bound, parsed as _ -: Apple:Fruit // a qualified lower bound Wildcards can only be used as type arguments or as arguments to built-in types. They are useful when we want to allow any of the type arguments within the bounds. For example, func buildZoo(Animal*) -> Zoo will only take a list with type Animal* , but not a list with type Mammal* , even though a zoo of just mammals can be pretty cool already. To allow any list of animals, we use a wildcard: func buildZoo((_ : Animal)*) -> Zoo This buildZoo will take Animal* , Mammal* , Bird* , Insect* and even Chicken* . Wildcards are translated to Java wildcards. Custom datatypes \u00b6 Custom datatypes are definitions using the data keyword. They look like this: $Modifiers data $TYPEID<$GenericParameters> : $SuperType = $DataImpl $Modifier = \"transient\" $DataImpl = foreign java $QID { $FuncHeads } Modifiers change the semantics of a datatype. The only modifier right now is transient . This modifier signifies to the PIE runtime that the datatype cannot be cached. It is an error to repeat modifiers, i.e. transient transient data Foo = $DataImpl is not allowed. The name can be any name that not already a built-in type. The convention is to use PascalCase, meaning that every first letter of a word is a capital letter. Names start with a letter or underscore, and can contain letters, numbers, a dash ( - ) and underscores ( _ ). The list of generic parameters can be omitted. This is syntactic sugar for an empty list, so Foo is the same as Foo<> . For an explanation of generics in the PIE DSL, see generics The super type specifies the super type of this data type. The super type can be omitted, for example data Foo = $DataImpl . If the super type is omitted, the top type implicitly becomes the super type. See the section about super types and subtypes earlier on this page for an explanation of super types. The only implementation right now is foreign java . This implementation is a Java class. It looks like this foreign java $QID { $FuncHeads } The $QID specifies the qualified name of the backing Java class. $FuncHeads is a newline separated list of function headers. These are declarations of the non-static methods of the class. Not all non-static methods of the class need to be declared here. Static methods of the class can be declared as foreign java functions outside this data definition. Separate your imports Define foreign java datatypes in a separate module and import them into your main module to keep your main module cleaner.","title":"Types"},{"location":"references/pipelines/types/#types","text":"This page explains the type system of the PIE DSL and lists all the built-in types.","title":"Types"},{"location":"references/pipelines/types/#the-type-system","text":"A type system checks for type errors. PIE DSL uses a static type system, so type errors are found before compilation. The PIE DSL supports subtypes, generics and methods on types.","title":"The type system"},{"location":"references/pipelines/types/#nullability","text":"PIE DSL keeps explicit track of nullability, so an expression cannot be null unless the type of the expression is nullable . Todo Fully document nullability.","title":"Nullability"},{"location":"references/pipelines/types/#super-types-and-subtypes","text":"When a type X is a subtype of type Y , you can use an expression with type X wherever an expression with type Y is expected. When X is a subtype of Y , Y is the super type of X For example, the method cutFruit(fruit: Fruit) -> Piece* expects a Fruit . If an Apple is declared to be a subtype of Fruit , we can pass an Apple : cutFruit(getApple()) Subtypes are transitive, which means that if A is a subtype of B , and B is a subtype of C , then A is a subtype of C , even if that is not explicitly declared. The PIE DSL allows declaring exactly one super type. The top type is a super type of everything. As such, any expression can be assigned to the top type. The bottom type is a subtype of everything. As such, an expression with the bottom type can be assigned to everything.","title":"Super types and subtypes"},{"location":"references/pipelines/types/#generics","text":"Generics refer to making a type or a function generic over types, that is, add type parameters to the type or function. You likely already know about these, lists are an example of parameterized types. It's not just any list, it's a list of some type here . Every data type in the PIE DSL is generic. When declaring a datatype in a data definition, there is a list of generic parameters: data Box<T> = $DataImpl . When referring to a type, there is a list of generic parameters: val box: Box<string> = $Exp . These lists can be empty: data Apple<> = $DataImpl . This makes the term \"generic\" a bit meaningless, but Apple is still considered generic for the purposes of the semantics. That is to say, Apple is treated as a generic type that just happens to have zero generic parameters, in the same way that Box happens to have one generic parameter and Foo<A, B, C> happens to have three. Omitting type parameter and argument lists Type parameter/argument list can sometimes be omitted. Parameter lists can be omitted if they are empty: data Apple = $DataImpl , func notGeneric(string) -> unit = $FuncImpl . Type argument lists can be omitted for empty lists and for calls to the built-in supplier function . For example, Apple , notGeneric(\"regular argument\") . A type declares zero or more generic parameters. These can be used in methods of that type to parameterize the method. A function or method can also declare generic parameters to be used in that function or method. PIE does not derive bounds Unlike Java, the PIE DSL does not derive bounds for datatypes based on super types. The following would be possible in Java but will not work in the PIE DSL: data CustomAnimalSet<A : Animal> = $DataImpl data OrderedAnimalSet<T> : CustomAnimalSet<T> = $DataImpl // error on `T` in `CustomAnimalSet<T>`: `T` is not within upper bound `Animal` // Solution: declare the bound on the subtype as well data OrderedAnimalSet2<T : Animal> : CustomAnimalSet<T> = $DataImpl Todo Explain generics","title":"Generics"},{"location":"references/pipelines/types/#methods-and-overriding","text":"Every type can have methods. For now, the only data types are foreign java datatypes, so methods follow Java semantics for overriding. Todo Explain overriding.","title":"Methods and overriding"},{"location":"references/pipelines/types/#built-in-types","text":"The PIE DSL has several built-in types. This section explains all of them. Datatype equality with equal Java class PIE DSL does not consider types equivalent when their backing Java class is equal. This means that a built-in type and a custom datatype backed by the same class cannot be used interchangeably. The following table gives a quick overview of the built-in types, click on the name to go to the documentation. name syntax description values methods backing Java class unit unit Unit type, only has a single value. Use as return type for methods without a meaningful return value unit - mb.pie.api.None bool bool Booleans. Used as flags and for conditions in if and if-else . true , false - java.lang.Boolean int int Integers. Range from \\(-2^{31}\\) to \\(2^{31}-1\\) , inclusive. -2147483648, ... -1, 0, 1, 2, 3, ... 2147483647 - java.lang.Integer path path Paths on the file system. Might not exist. E.g. ./src/test/resources/test1.txt , /home/users/me/programming - mb.resource.fs.FSPath null type - Type of the literal null . Subtype of every nullable type . null - - top - The top type is the super type of all other types. every value is an instance of top - java.lang.Object bottom - The bottom type is a subtype of all other types. It has no values by definition. An expression of type bottom will never return normally. No values - - Nullable types $Type? , e.g. string? Makes a type nullable. All values of the original type and null All methods of the original type Original backing class Lists $Type* , e.g. int* A list. Unknown amount of elements, all with the same type. - [] , [e1] , [e1, e2] , [e1, e2, e3] , where e1 , e2 and e3 are valid elements of the list element type. java.util.ArrayList Tuples ($Types) A tuple of elements. Known amount of elements, can be different types. elements of the inner types, e.g. (e1, e2) is a value of (T1, T2) if e1 and e2 are values of T1 and T2 respectively - mb.pie.TupleX , where X is the number of elements. supplier supplier<$Type> A supplier of a value. Useful for performance in certain situations. Can be created using supplier($Exp) or $FUNCID.supplier<$TypeArgs>($Exps) func get<>() -> T for supplier<T> mb.pie.api.Supplier Function types - The type of a function. Functions cannot be used as values, but their type can be seen by hovering over the name - - - Wildcards _$UpperBound$LowerBound Represent a set of types. Can only be used as type argument. Instances of types in the type set - Backed by the Java wildcard: ? Custom datatypes $TYPEID A type defined in a pie file with. Instances of the type, ultimately obtained from foreign java functions The methods that are declared on the type itself, and the methods of its super types The declared backing class","title":"Built-in types"},{"location":"references/pipelines/types/#unit","text":"unit is a type with only a single value: unit . It is meant to be used as return value for functions that have no meaningful return value, for example functions that operate via side effects like writing to a file. It is backed by mb.pie.api.None .","title":"unit"},{"location":"references/pipelines/types/#bool","text":"bool represents booleans and as such has two values: true and false . Booleans are used as flags and as conditions for if and if-else . bool is backed by java.lang.Boolean .","title":"bool"},{"location":"references/pipelines/types/#int","text":"int represents integers. It is backed by java.lang.Integer , and as such has a range of \\(-2^{31}\\) to \\(2^{31}-1\\) , inclusive.","title":"int"},{"location":"references/pipelines/types/#string","text":"string represents strings. Strings have many built-in methods which have yet to be added to the implementation. It is backed by java String .","title":"string"},{"location":"references/pipelines/types/#path","text":"path represents a path to a file or directory in the file system. The directory or file need not exist. Paths can be relative or absolute. Paths have many built-in methods which have yet to be added to the implementation. Paths are backed by mb.resource.fs.FSPath","title":"path"},{"location":"references/pipelines/types/#null-type","text":"The null type cannot be expressed in the PIE DSL, meaning that there is no way to specify it as the type of something. Its only value is null . The null type is a subtype of every nullable type.","title":"null type"},{"location":"references/pipelines/types/#top","text":"The top type is a super type of every other type. It cannot be specified as a type. It is backed by java.lang.Object","title":"top"},{"location":"references/pipelines/types/#bottom","text":"The bottom type is a subtype of every other type. It cannot be specified as a type. The bottom type has no values, and as such an expression with bottom type will never return normally to the function it is defined in. It is the element type of empty lists, and in the future also of return and fail expressions. This type is not backed by any java class. Compiling bottom type Code that has the bottom type will fail to compile. Remove the code that has bottom type to resolve this.","title":"bottom"},{"location":"references/pipelines/types/#nullable-types","text":"Nullable types are represented with a question mark after the type. For example, a nullable int is int? . A nullable type X? represents a value that could either be a regular value X or \"missing\", represented with the expression null . A nullable type X? is a super type of both X and the null type. It is an error to make a nullable type nullable again, so X?? is not allowed. Java types are always nullable, so the nullable type X? is backed by Java type X .","title":"Nullable types"},{"location":"references/pipelines/types/#lists","text":"Lists are represented with an asterisk behind the type. For example, a nullable path is path* . Lists of X can contain any element that could be assigned to X . Lists do not have subtypes besides the bottom type. This means that Apple* is not a subtype of Fruit* . Lists do not have methods yet. Lists are backed by Java java.util.ArrayList . Empty lists The PIE DSL type system keeps track of empty lists for implementation reasons. Because it is doing this anyway, it gives warnings when doing certain non-sensical things such as appending an empty list to another list or list comprehension over empty lists.","title":"Lists"},{"location":"references/pipelines/types/#tuples","text":"Tuple types represent a combination of multiple types. They are specified as the types between parentheses. For example, (string, int*) represents a pair of a string and a list of int s. Tuple types differ from lists because lists have a variable amount of elements of a single type, while tuples have a set number of elements with heterogenous types. Tuple types can be deconstructed to get their values: val pair: (string, int*) = (\"Alice\", [9, 4, 6, 7]); val (name: string, grades: int*) = pair; Tuples are backed by Java classes mb.pie.TupleX , where X is a number representing the amount of elements, e.g. Tuple2 for a pair. This is because Java is not generic in the amount of generic elements. Limits on tuple sizes While the PIE DSL language does not specify a limit on the amount of elements in a tuple, the backing Java TupleX classes only go up to 10. If you run into this limit, use a foreign data type backed by a custom Java class instead.","title":"Tuples"},{"location":"references/pipelines/types/#supplier","text":"supplier<T> represents a supplier of a value of type T . Suppliers represent a value, either by being created with a value or by deferring a task that returns the value . Suppliers have a single method get<>() -> T , which returns the value of the supplier, either by returning the value if it already existed or by calling the task that the supplier supplies. The main use case for suppliers is as input types for tasks. If the supplier is faster to check for consistency than the value it supplies, the runtime performance is improved. As an example, consider func readFile() -> string = read ./bundled.java func parse1(program: string) -> IStrategoTerm = mb:lang:java:parse(program) func parse2(program: supplier<string>) -> IStrategoTerm = mb:lang:java:parse(program.get<>()) func parseBoth() -> unit = { parse1(readFile()); parse2(readfile.supplier()); unit } Both parse1 and parse2 need to read the file ./bundled.java , strip of any whitespace, and then parse it with mb:lang:java:parse on the initial build. If we modify ./bundled.java before the second build, readFile is now outdated and will need to read again. To check if the input for parse1 is in the cache, the runtime needs to check the contents of the entire file against any cached values to see if it matches. To check if the input for parse2 is in the cache, the runtime only checks if the supplier is in the cache. The supplier is a TaskSupplier , which is in the cache if its task is not outdated. The runtime only has to make a few calls to determine that the input for parse2 is not cached. Suppliers are backed by mb.pie.api.Supplier .","title":"supplier"},{"location":"references/pipelines/types/#function-types","text":"Function types are visible when hovering over a function name. They follow the pattern func($Params) -> $Type . For example, func(int, string) -> bool is a function that takes an int and a string and returns a boolean. Function types cannot be specified in PIE DSL, and they can also not be the type of a variable. Because function types cannot be the type of a variable, they are not backed by a Java class.","title":"Function types"},{"location":"references/pipelines/types/#wildcards","text":"Wildcards represent a set of types by using an upper or lower bound. They use the following syntax. The wildcard itself is represented by an underscore: _ The upper bound is specified by a colon followed by a type: : $Type The lower bound is specified with a dash, colon and then a type: -: $Type If a the upper bound is omitted, it is implicitly the top type. If the lower bound is omitted, it is implicitly the bottom type. A wildcard cannot have both an upper and a lower bound. Here are some examples of wildcards and what they mean: _ // unbounded wildcard (bounds are implicitly the top and bottom type) _ : Fruit // upper bounded wildcard. Matches Fruit, Apple, Pear _ : path:to:module:Vegetable // qualified upper bound _ -: Fruit // lower bounded wildcard. Matches Fruit, Food, top type _ : Fruit -: Apple // both upper and lower bounded, gives an error _ : Iterable : Comparable // _not_ a type with multiple upper bounds, parsed as _ : Iterable:Comparable // a single qualified upper bound. _ -: Apple : Fruit // _not_ a lower bound and then upper bound, parsed as _ -: Apple:Fruit // a qualified lower bound Wildcards can only be used as type arguments or as arguments to built-in types. They are useful when we want to allow any of the type arguments within the bounds. For example, func buildZoo(Animal*) -> Zoo will only take a list with type Animal* , but not a list with type Mammal* , even though a zoo of just mammals can be pretty cool already. To allow any list of animals, we use a wildcard: func buildZoo((_ : Animal)*) -> Zoo This buildZoo will take Animal* , Mammal* , Bird* , Insect* and even Chicken* . Wildcards are translated to Java wildcards.","title":"Wildcards"},{"location":"references/pipelines/types/#custom-datatypes","text":"Custom datatypes are definitions using the data keyword. They look like this: $Modifiers data $TYPEID<$GenericParameters> : $SuperType = $DataImpl $Modifier = \"transient\" $DataImpl = foreign java $QID { $FuncHeads } Modifiers change the semantics of a datatype. The only modifier right now is transient . This modifier signifies to the PIE runtime that the datatype cannot be cached. It is an error to repeat modifiers, i.e. transient transient data Foo = $DataImpl is not allowed. The name can be any name that not already a built-in type. The convention is to use PascalCase, meaning that every first letter of a word is a capital letter. Names start with a letter or underscore, and can contain letters, numbers, a dash ( - ) and underscores ( _ ). The list of generic parameters can be omitted. This is syntactic sugar for an empty list, so Foo is the same as Foo<> . For an explanation of generics in the PIE DSL, see generics The super type specifies the super type of this data type. The super type can be omitted, for example data Foo = $DataImpl . If the super type is omitted, the top type implicitly becomes the super type. See the section about super types and subtypes earlier on this page for an explanation of super types. The only implementation right now is foreign java . This implementation is a Java class. It looks like this foreign java $QID { $FuncHeads } The $QID specifies the qualified name of the backing Java class. $FuncHeads is a newline separated list of function headers. These are declarations of the non-static methods of the class. Not all non-static methods of the class need to be declared here. Static methods of the class can be declared as foreign java functions outside this data definition. Separate your imports Define foreign java datatypes in a separate module and import them into your main module to keep your main module cleaner.","title":"Custom datatypes"},{"location":"references/statix/","text":"Statix \u00b6 Statix is a Meta-language for the Specification of Static Semantics. Statix specifications are organised in modules . In Statix, programs, types and all other data are represented using terms . Type-checking a program is performed by solving a set of constraints over terms. In addition to these built-in constraints, specification writers can define their own constraints . Type-checking is closely related to, and strongly intertwined with, name resolution. For that reason, Statix has built-in support for modelling name binding patterns in the form of scope graphs . During type-checking, names can be resolved using queries . When transforming programs using in Stratego , Statix specifications can be executed, and the results accesssed using the Stratego API for Statix . Statix has a special test format , which can be used for isolating issues in a specification, or in the Statix ecosystem. Tip Readers with little or no familiarity with Statix are recommended to read the Language Concepts section first. Sources \u00b6 The sources of the different Statix components can be found at: https://github.com/metaborg/nabl/tree/master/statix.lang : The Statix Language https://github.com/metaborg/nabl/tree/master/statix.runtime : The Statix Runtime https://github.com/metaborg/nabl/tree/master/statix.solver : The Statix Solver","title":"Statix"},{"location":"references/statix/#statix","text":"Statix is a Meta-language for the Specification of Static Semantics. Statix specifications are organised in modules . In Statix, programs, types and all other data are represented using terms . Type-checking a program is performed by solving a set of constraints over terms. In addition to these built-in constraints, specification writers can define their own constraints . Type-checking is closely related to, and strongly intertwined with, name resolution. For that reason, Statix has built-in support for modelling name binding patterns in the form of scope graphs . During type-checking, names can be resolved using queries . When transforming programs using in Stratego , Statix specifications can be executed, and the results accesssed using the Stratego API for Statix . Statix has a special test format , which can be used for isolating issues in a specification, or in the Statix ecosystem. Tip Readers with little or no familiarity with Statix are recommended to read the Language Concepts section first.","title":"Statix"},{"location":"references/statix/#sources","text":"The sources of the different Statix components can be found at: https://github.com/metaborg/nabl/tree/master/statix.lang : The Statix Language https://github.com/metaborg/nabl/tree/master/statix.runtime : The Statix Runtime https://github.com/metaborg/nabl/tree/master/statix.solver : The Statix Solver","title":"Sources"},{"location":"references/statix/basic-constraints/","text":"Basic Constraints \u00b6 As mentioned in the Language Concepts section, the core idea of Statix is to see a type-checking problem as a constraint solving problem. Therefore, it is crucial to be able to express constraints in a specification. In this section, we discuss all constraints that are note related to scope graphs. These constraints are explained in-depth in the sections on Scope Graph Construction and Queries . True \u00b6 true The true constraint is the constraint that is trivially satisfied. False \u00b6 false $ Message ? The false constraint is the constraint that will always fail. Just as all other constraints that can fail, it is possible to add a message . Conjunction \u00b6 $ Constraint , $ Constraint A conjunction of constraints is satisfied when both conjuncts are satisfied. Note that the solving order of the conjuncts is undefined. Equality \u00b6 $ Term == $ Term $ Message ? Asserts that two terms are equal. When necessary, this constraint infers values for free variables in the terms. Statically, both terms should have the same type. Disequality \u00b6 $ Term != $ Term $ Message ? Asserts that two terms are not equal. Statically, both terms should have the same type. Statix treats free variables as different from each other, and as different from concrete terms. Therefore, when any of both terms is not ground (i.e. contains free variables), the constraint will not fail. Exists \u00b6 { $ Var * } $ Constraint An exists constraint introduces new existentially quantified variables in the scope of its subconstraint. The names of the variables in an exists constraint should be unique (i.e. {x x} true is not allowed), but are allowed to shadow outer variables. Try \u00b6 try { $ Constraint } $ Message ? The try constraint validates whether the outer context implies that the inner constraint holds. That is: any model for the outer context (all other constraints than $Constraint ) is also a (not necessarily minimal) model for $Constraint . In order to implement these semantics, the $Constraint is handled differently than regular constraints in two ways. The $Constraint is not allowed to refine the outer context. Therefore, equality constraints will not infer values for variable that were introduced outside the try . Likewise, scopes can not be instantiated, nor edges/declarations added to scopes from outside the try . This behavior also implies that values constructed within a try construct will never escape the try context. Disequalities in $Constraint that involve free variables cause the try to fail, because appearently the disequality does not hold for all models of the outer context. Todo Explain the design choices for try in a background section. AST Identifiers \u00b6 astId ( $ Term , $ Term ) The astId constraint asserts that its second argument is the term index of the first argument. The type of the first argument may be anything, but the type of the second argument is astId . Tip Often, using an astId term will read more natural. AST Property \u00b6 @$ Term . $ Prop $ Op $ Term Statix allows to set properties on AST nodes. These properties can be used to communicate typing results to the outside world, for example to be used in a transformation. For more information on reading these properties, please refer to the Stratego API documentation . The first term is the term (usually an AST node) on which the property is set. Next $Prop specifies which property is set. This property can be any string of the form [a-zA-Z] [a-zA-Z0-9\\_]* . It is not required to declare properties. There are two special properties: ref and type . ref properties are set on (syntactic) variable references, and point to the term they are referencing. The Spoofax reference resolution service uses these properties to offer reference resolution in the editor. The type property contains the type of a term. This type is shown when hovering over a term in an editor. The $Op specifies the operator with which a property is set. There are two possible operators: := : The assignment operator. This operator requires a property to have only a single unique value (although, since Spoofax 2.5.17, that value may be set multiple times). += : The bag insertion operator. Properties using this operator can be set multiple times, and will be aggregated in the eventual property value. Note that using both operators for a single property on a particular node will result in a failed constraint. However, it is allowed to use different operators for a property, as long as the terms on which these operators are used are different. Finally, the last $Term denotes the value of the property. Warning Failing property constraints are ignored (i.e. no error for them is reported). Arithmetic Constraints \u00b6 $ Term $ Op $ ArithExp $ Message ? Statix supports several arithmetic constraints. These constraints consist of a term, an operator and an arithmetic expression. The $Term should have type int , while the $ArithExp is syntactically guaranteed to have type int , given that all variable references have type int . At the $Op position, several comparison operators can be used: #= asserts that both terms are equal #\\\\= asserts that both terms are not equal #>= asserts that the left term is equal or bigger that the left term #=< asserts that the left term is equal or smaller that the left term #> asserts that the left term is strictly bigger that the left term #< asserts that the left term is strictly smaller that the left term Arithmetic expressions can be integer literals, variables and bracketed arithmetic expressions. Variables in an arithmetic expression must have type int . Additionally, the following arithmetic operations can be used: $ArithExp + $ArithExp : computes integer addition $ArithExp - $ArithExp : computes integer subtraction $ArithExp * $ArithExp : computes integer multiplication min($ArithExp, $ArithExp) : computes the minimum of both arguments max($ArithExp, $ArithExp) : computes the maximum of both arguments $ArithExp div $ArithExp : computes the integer divisor (i.e. rounded down regular division). $ArithExp mod $ArithExp : computes the modulus of its arguments. Unlike other constraints, arithmetic constraint do no inference of values in the $ArithExp . Hence, having any free variables in this expression will cause the constraint to fail. Separate Arithmetic Expression Syntax As discussed, Statix has a special syntactic category for arithmetic expressions. Therefore, arithmetic expressions cannot be used at regular term positions. Instead, arithmetic expressions can be embedded in terms using the Arithmetic Expressions term syntax. Java Integers Arithmetic Expressions are implemented using standard Java integers, and hence have the same size limitations. Messages \u00b6 | $ Severity $ MessageBody $ Position ? Constraints that might possibly fail can be provided with a customized message. Such message carry three parameters. First, the $Severity indicates the severity of a message. It may be either error , warning or note . Note however that warnings and notes can only be issued for failing try constraints. Second, the error message string is provided. This message string may be a regular string literal or a template literal. Template literals look as follows: $[$ ContentPart *] Content parts are either message string literals or interpolated terms. The escaping rules for string literals in templates are slightly different than for regular string literals. Message string literals can contain any character, where square brackets and backslashes must be escaped with a backslash. Just as regular string literals, tabs, newlines and carriage returns can be encoded with \\t , \\n and \\r , respectively. Terms can be inserted in a message template by surrounding them with (unescaped) square brackets: [$Term] . The term may have any type, but must be well-formed according to the regular typing rules for terms. Bug Using functional predicates inside message templates will cause an exception when loading the specification. Thirdly, a position can be assigned to the message: @ $ Var Here, the $Var is assumed to be an AST node. When the constraint on which this message is placed fails, the message will be shown inline at the AST node pointed to by this variable. When no message position is provided, or the assigned position is invalid, Statix will scan the the arguments to user-defined constraints on the call trace that led to the failed constraint from left to right for an AST argument, and put the message on the first valid node found. When no AST node could be found (for example when the project constraint fails), the error is positioned at the project resource. Warning Messages on projects are often overlooked by users. Hence it is recommended that project constraints are designed in a way they can never fail. When message is provided for a failed constraint, Statix will scan the call trace for constraints that have a message provided, and use the message it encounters. If no message is found, a rendering of the failed constraint is used as a message.","title":"Basic Constraints"},{"location":"references/statix/basic-constraints/#basic-constraints","text":"As mentioned in the Language Concepts section, the core idea of Statix is to see a type-checking problem as a constraint solving problem. Therefore, it is crucial to be able to express constraints in a specification. In this section, we discuss all constraints that are note related to scope graphs. These constraints are explained in-depth in the sections on Scope Graph Construction and Queries .","title":"Basic Constraints"},{"location":"references/statix/basic-constraints/#true","text":"true The true constraint is the constraint that is trivially satisfied.","title":"True"},{"location":"references/statix/basic-constraints/#false","text":"false $ Message ? The false constraint is the constraint that will always fail. Just as all other constraints that can fail, it is possible to add a message .","title":"False"},{"location":"references/statix/basic-constraints/#conjunction","text":"$ Constraint , $ Constraint A conjunction of constraints is satisfied when both conjuncts are satisfied. Note that the solving order of the conjuncts is undefined.","title":"Conjunction"},{"location":"references/statix/basic-constraints/#equality","text":"$ Term == $ Term $ Message ? Asserts that two terms are equal. When necessary, this constraint infers values for free variables in the terms. Statically, both terms should have the same type.","title":"Equality"},{"location":"references/statix/basic-constraints/#disequality","text":"$ Term != $ Term $ Message ? Asserts that two terms are not equal. Statically, both terms should have the same type. Statix treats free variables as different from each other, and as different from concrete terms. Therefore, when any of both terms is not ground (i.e. contains free variables), the constraint will not fail.","title":"Disequality"},{"location":"references/statix/basic-constraints/#exists","text":"{ $ Var * } $ Constraint An exists constraint introduces new existentially quantified variables in the scope of its subconstraint. The names of the variables in an exists constraint should be unique (i.e. {x x} true is not allowed), but are allowed to shadow outer variables.","title":"Exists"},{"location":"references/statix/basic-constraints/#try","text":"try { $ Constraint } $ Message ? The try constraint validates whether the outer context implies that the inner constraint holds. That is: any model for the outer context (all other constraints than $Constraint ) is also a (not necessarily minimal) model for $Constraint . In order to implement these semantics, the $Constraint is handled differently than regular constraints in two ways. The $Constraint is not allowed to refine the outer context. Therefore, equality constraints will not infer values for variable that were introduced outside the try . Likewise, scopes can not be instantiated, nor edges/declarations added to scopes from outside the try . This behavior also implies that values constructed within a try construct will never escape the try context. Disequalities in $Constraint that involve free variables cause the try to fail, because appearently the disequality does not hold for all models of the outer context. Todo Explain the design choices for try in a background section.","title":"Try"},{"location":"references/statix/basic-constraints/#ast-identifiers","text":"astId ( $ Term , $ Term ) The astId constraint asserts that its second argument is the term index of the first argument. The type of the first argument may be anything, but the type of the second argument is astId . Tip Often, using an astId term will read more natural.","title":"AST Identifiers"},{"location":"references/statix/basic-constraints/#ast-property","text":"@$ Term . $ Prop $ Op $ Term Statix allows to set properties on AST nodes. These properties can be used to communicate typing results to the outside world, for example to be used in a transformation. For more information on reading these properties, please refer to the Stratego API documentation . The first term is the term (usually an AST node) on which the property is set. Next $Prop specifies which property is set. This property can be any string of the form [a-zA-Z] [a-zA-Z0-9\\_]* . It is not required to declare properties. There are two special properties: ref and type . ref properties are set on (syntactic) variable references, and point to the term they are referencing. The Spoofax reference resolution service uses these properties to offer reference resolution in the editor. The type property contains the type of a term. This type is shown when hovering over a term in an editor. The $Op specifies the operator with which a property is set. There are two possible operators: := : The assignment operator. This operator requires a property to have only a single unique value (although, since Spoofax 2.5.17, that value may be set multiple times). += : The bag insertion operator. Properties using this operator can be set multiple times, and will be aggregated in the eventual property value. Note that using both operators for a single property on a particular node will result in a failed constraint. However, it is allowed to use different operators for a property, as long as the terms on which these operators are used are different. Finally, the last $Term denotes the value of the property. Warning Failing property constraints are ignored (i.e. no error for them is reported).","title":"AST Property"},{"location":"references/statix/basic-constraints/#arithmetic-constraints","text":"$ Term $ Op $ ArithExp $ Message ? Statix supports several arithmetic constraints. These constraints consist of a term, an operator and an arithmetic expression. The $Term should have type int , while the $ArithExp is syntactically guaranteed to have type int , given that all variable references have type int . At the $Op position, several comparison operators can be used: #= asserts that both terms are equal #\\\\= asserts that both terms are not equal #>= asserts that the left term is equal or bigger that the left term #=< asserts that the left term is equal or smaller that the left term #> asserts that the left term is strictly bigger that the left term #< asserts that the left term is strictly smaller that the left term Arithmetic expressions can be integer literals, variables and bracketed arithmetic expressions. Variables in an arithmetic expression must have type int . Additionally, the following arithmetic operations can be used: $ArithExp + $ArithExp : computes integer addition $ArithExp - $ArithExp : computes integer subtraction $ArithExp * $ArithExp : computes integer multiplication min($ArithExp, $ArithExp) : computes the minimum of both arguments max($ArithExp, $ArithExp) : computes the maximum of both arguments $ArithExp div $ArithExp : computes the integer divisor (i.e. rounded down regular division). $ArithExp mod $ArithExp : computes the modulus of its arguments. Unlike other constraints, arithmetic constraint do no inference of values in the $ArithExp . Hence, having any free variables in this expression will cause the constraint to fail. Separate Arithmetic Expression Syntax As discussed, Statix has a special syntactic category for arithmetic expressions. Therefore, arithmetic expressions cannot be used at regular term positions. Instead, arithmetic expressions can be embedded in terms using the Arithmetic Expressions term syntax. Java Integers Arithmetic Expressions are implemented using standard Java integers, and hence have the same size limitations.","title":"Arithmetic Constraints"},{"location":"references/statix/basic-constraints/#messages","text":"| $ Severity $ MessageBody $ Position ? Constraints that might possibly fail can be provided with a customized message. Such message carry three parameters. First, the $Severity indicates the severity of a message. It may be either error , warning or note . Note however that warnings and notes can only be issued for failing try constraints. Second, the error message string is provided. This message string may be a regular string literal or a template literal. Template literals look as follows: $[$ ContentPart *] Content parts are either message string literals or interpolated terms. The escaping rules for string literals in templates are slightly different than for regular string literals. Message string literals can contain any character, where square brackets and backslashes must be escaped with a backslash. Just as regular string literals, tabs, newlines and carriage returns can be encoded with \\t , \\n and \\r , respectively. Terms can be inserted in a message template by surrounding them with (unescaped) square brackets: [$Term] . The term may have any type, but must be well-formed according to the regular typing rules for terms. Bug Using functional predicates inside message templates will cause an exception when loading the specification. Thirdly, a position can be assigned to the message: @ $ Var Here, the $Var is assumed to be an AST node. When the constraint on which this message is placed fails, the message will be shown inline at the AST node pointed to by this variable. When no message position is provided, or the assigned position is invalid, Statix will scan the the arguments to user-defined constraints on the call trace that led to the failed constraint from left to right for an AST argument, and put the message on the first valid node found. When no AST node could be found (for example when the project constraint fails), the error is positioned at the project resource. Warning Messages on projects are often overlooked by users. Hence it is recommended that project constraints are designed in a way they can never fail. When message is provided for a failed constraint, Statix will scan the call trace for constraints that have a message provided, and use the message it encounters. If no message is found, a rendering of the failed constraint is used as a message.","title":"Messages"},{"location":"references/statix/concepts/","text":"Language Concepts \u00b6 In this section, a brief description of the main concepts of the Statix language is provided. Terms \u00b6 The data model that underlies all Statix specifications is algebraic data. Besides several built-in primitives, such as integer and string literals, users can build composite terms using term constructors, tuples and lists. Statix is a sorted logic, in the sense that all runtime data should adhere to a multi-sorted signature. Constraints \u00b6 Key to the Statix design philosophy is to view a type-checking problem as a constraint problem. When solving the constraint problem, a minimal model is inferred from the constraints. This model represents a principal typing for the original program. In order to express such constraint problems, a versatile set of built-in constraints is provided by the Statix language. For more information on constraints, see the Basic Constraints section. Rules \u00b6 Besides using built-in constraints, users can define their own constraints using constraint handling rules. Rules consist of a head and a body. The head specifies the arguments to the constraint, and (optionally) a guard, which indicates when to apply the rule. The body is a regular constraint, which, when proven, asserts that the constraint holds. More detailed information about user-defined constraints can be found in the Rules section. Scope Graphs \u00b6 Since Statix is especially designed for type-checking, and type-checking is heavily intertwined with name binding, special support for name binding is integrated in the language. Name binding is modelled using scope graphs , in which scopes are represented as nodes, visibility is modelled using labelled edges between nodes, and declarations using special terminal nodes that are associated with a particular datum. References are modelled using scope graph queries . For more information on scope graph construction and querying, see sections Scope Graph Constraints and Queries , respectively.","title":"Language Concepts"},{"location":"references/statix/concepts/#language-concepts","text":"In this section, a brief description of the main concepts of the Statix language is provided.","title":"Language Concepts"},{"location":"references/statix/concepts/#terms","text":"The data model that underlies all Statix specifications is algebraic data. Besides several built-in primitives, such as integer and string literals, users can build composite terms using term constructors, tuples and lists. Statix is a sorted logic, in the sense that all runtime data should adhere to a multi-sorted signature.","title":"Terms"},{"location":"references/statix/concepts/#constraints","text":"Key to the Statix design philosophy is to view a type-checking problem as a constraint problem. When solving the constraint problem, a minimal model is inferred from the constraints. This model represents a principal typing for the original program. In order to express such constraint problems, a versatile set of built-in constraints is provided by the Statix language. For more information on constraints, see the Basic Constraints section.","title":"Constraints"},{"location":"references/statix/concepts/#rules","text":"Besides using built-in constraints, users can define their own constraints using constraint handling rules. Rules consist of a head and a body. The head specifies the arguments to the constraint, and (optionally) a guard, which indicates when to apply the rule. The body is a regular constraint, which, when proven, asserts that the constraint holds. More detailed information about user-defined constraints can be found in the Rules section.","title":"Rules"},{"location":"references/statix/concepts/#scope-graphs","text":"Since Statix is especially designed for type-checking, and type-checking is heavily intertwined with name binding, special support for name binding is integrated in the language. Name binding is modelled using scope graphs , in which scopes are represented as nodes, visibility is modelled using labelled edges between nodes, and declarations using special terminal nodes that are associated with a particular datum. References are modelled using scope graph queries . For more information on scope graph construction and querying, see sections Scope Graph Constraints and Queries , respectively.","title":"Scope Graphs"},{"location":"references/statix/modules/","text":"Modules \u00b6 A Statix Specification is organised as a collection of modules. Each module corresponds to a file with a .stx extension. Module Structure \u00b6 The structure of a Statix module looks as follows: module $ ModuleName $ Section * Each module declares its name, and subsequently contains a number of sections. The module name should coincide with the relative path of the module with respect to the closest source root. Todo Link to documentation on source roots. Imports \u00b6 In an imports section, definitions from other modules can be brought in scope. imports $ ModuleName * Modules can only be imported with their fully qualified name. That is, for each $ModuleName in an imports section, a module with exactly the same name must exist. Imports of sorts, constructors and predicates are transitive, while imports of labels and relations are non-transitive. Furthermore, overloading by type, shadowing of top-level definitions, and duplicate imports of specification entities are not allowed. Signatures \u00b6 In a signature section, type definitions are located. signature $ Signature * Examples of signatures are: sort and constructor declarations or label and relation declarations. Each of these will be explained in the appropriate subsection. Rules \u00b6 In a rules section, the rules of a specification are defined. For more information on rules, see the Rules section. rules $ RuleDeclaration *","title":"Modules"},{"location":"references/statix/modules/#modules","text":"A Statix Specification is organised as a collection of modules. Each module corresponds to a file with a .stx extension.","title":"Modules"},{"location":"references/statix/modules/#module-structure","text":"The structure of a Statix module looks as follows: module $ ModuleName $ Section * Each module declares its name, and subsequently contains a number of sections. The module name should coincide with the relative path of the module with respect to the closest source root. Todo Link to documentation on source roots.","title":"Module Structure"},{"location":"references/statix/modules/#imports","text":"In an imports section, definitions from other modules can be brought in scope. imports $ ModuleName * Modules can only be imported with their fully qualified name. That is, for each $ModuleName in an imports section, a module with exactly the same name must exist. Imports of sorts, constructors and predicates are transitive, while imports of labels and relations are non-transitive. Furthermore, overloading by type, shadowing of top-level definitions, and duplicate imports of specification entities are not allowed.","title":"Imports"},{"location":"references/statix/modules/#signatures","text":"In a signature section, type definitions are located. signature $ Signature * Examples of signatures are: sort and constructor declarations or label and relation declarations. Each of these will be explained in the appropriate subsection.","title":"Signatures"},{"location":"references/statix/modules/#rules","text":"In a rules section, the rules of a specification are defined. For more information on rules, see the Rules section. rules $ RuleDeclaration *","title":"Rules"},{"location":"references/statix/queries/","text":"Queries \u00b6 Scope Graphs, as introduced in the previous section can be queried. Scope graph queries always start in a particular scope, and traverse the scope graph in order to find declarations under a particular relation. The syntax for queries is as follows: query $ QueryTarget filter $ LabelRE and $ DataWF min $ LabelOrder * and $ DataLeq in $ Scope |-> $ Result The $Scope parameter is a term with type scope . In this scope, the query starts. All other query arguments are explained in the following subsections. Query Targets \u00b6 The query target is the 'thing' that is looked for in the query. This can be one of: $Relation : A relation identifier . In this case, the query will return data that is declared under that relation. () : the end of path query target. In this case, the query will return all paths that match the appropriate filters. () as relation The () query target can be thought of as a regular relation if one assumes #1 |-()-> #1 to exist for every scope #1 in the scope graph. Filters \u00b6 Query results can be filtered using two different filters. First, a regular expression on labels ( $LabelRE ) defines a filter on the paths that the query resolution algorithm will explore. This regular expression can be build from the following components: $Label : Matches paths that travers a single edge with the label $Label . Requires the label to be declared in a signature section, as explained in the section on edges . e : epsilon . Matches the empty path. Queries using this filter will only return declarations in the scope where the query started. 0 : empty set . Matches no path. $LabelRE $LabelRE : concatenation . Matches paths that can be split in two segments p1 and p2 , such that p1 matches the first regular expression, and p2 matches the second. $LabelRE | $LabelRE : disjunction . Matches paths that match either the first or the second regular expression. $LabelRE & $LabelRE : conjunction . Matches paths that match both the first and the second regular expression. ~$LabelRE : negation . Matches all paths that do not match the inner regular expression. $LabelRE* : closure . Matches paths that can be split into zero or more segments that each match the inner regular expression. $LabelRE+ : one or more . Matches paths that can be split into one or more segments that each match the inner regular expression. Equivalent to $LabelRE $LabelRE* $LabelRE? : zero or one . Matches paths that are matched by the inner regular expression, or empty paths. Equivalent to $LabelRE | e . Additionally, regular expression can be grouped by brackets. Second, data well-formedness filters ( $DataWF ) can be applied. These filters restrict which datums are included in the query result. They are expressed as anonymous lambda rules: { $ Pattern : - $ Constraint } This rule is instantiated for every declaration that is reachable according to the path well-formedness expression. When the instantiated body constraint holds, the declaration is included in the query answer. Lambda Instantiation Lambda constraint instantiation is similar to rule instantiation. For more information on rule instantiation, see the section about rule definitions . Entailment semantics Data well-formedness conditions are treated as entailment/implied conditions. Hence, they are not allowed to extend or refine the outer context. For more information on this evaluation mode, see the documentation of the try construct . The type of the predicate that is expected depends on the kind of relation that is used. For predicative relations, all arguments in the relation are provided to the data well-formedness constraint. However, for functional relations, the 'output value' (i.e. the value that the relation maps to) is not provided to the filter. When multiple arguments are provided to a data-wellformedness predicate (when there are multiple 'input' arguments to the queried relation), these arguments must be wrapped in a tuple. When the end-of-path query target () is used, the data-wellformedness constraint expects a single scope as argument. Example of filters A simple query for variables illustrates both filters. Suppose the relation var is in scope with type string -> TYPE . Then a rule (with type ascriptions ) that looks up a variable definition can be defined as follows. resolveVar ( s : scope , name : string ) = R : - query var filter P * I ? and { name' : string : - name' == name } in s |-> R . In this example, the path well-formedness expression P* I? indicates that any number of P edges may be traversed, and then, optionally, a single I edge. This resolution policy excludes e.g. transitive imports, and the traversal of P edges in an imported module. The anonymous data well-formedness condition states that a declaration with name name' may only be included in the query result if name' is equal to name from the enclosing scope. Suppose that resolveVar is instantiated for name |-> \"x\" , and declarations with name \"x\" and \"y\" are in scope. Now the anonymous inner rule is instantiated for both \"x\" and \"y\" . For \"x\" , the constraint \"x\" == \"x\" is generated, which can be solved successfully. For \"y\" however, the constraint \"y\" == \"x\" is generated, which cannot be solved successfully. Hence, only the declaration for \"x\" is included in the eventual query answer. There are three shorthands for common data well-formedness predicates: true , which is equivalent to { _ :- true } . This shorthand will thus include all encountered declaration in the query result. false , which is equivalent to { _ :- false } . This shorthand will include no declarations in the query answer. eq($Term) , is equivalent to { x :- x == $Term } and hence will include all declarations that are equal to $Term . Syntactically, the query filter can be omitted entirely, or the data well-formedness predicate can be omitted, even if a path filter is provided. By default, the path filter is ~0 , meaning that every path is considered valid, and the data well-formedness predicate is true , meaning that every datum will be returned in the query answer. Shadowing \u00b6 For many languages, name resolution involves dealing with shadowing correctly. In Statix queries, it is possible to encode shadowing policies using label orders and data comparison predicates. A declaration shadows another declaration iff its path is 'smaller than' the other path by a prefix order defined over a label comparison relation, and when the declaration data is smaller than or equal to the data of the other declaration according to a data comparison predicate. Label orders ( $LabelOrder ) are expressed as less-than relations on labels. $ Label < $ Label Here, a label is either a declared label symbol , or $ , which denotes the 'end-of-path' label. This label can be used to express orders on path length. For example, $ < P expresses that paths with fewer P labels are preferred over paths with more P labels. Strict Partial Order Label orders must be strict partial orders . That is, they are implicitly transitive, but may not be reflexive or symmetric. Label order specifications that are not strict partial orders will be rejected at specification loading time. Prefix order Note that the label order is a prefix order , not a lexicographical full-path order . That is, paths that diverged by traversing different edges with the same label are not ordered by this relation. In addition to a label ordering relation, a data comparison predicate ( $DataLeq ) can be provided. This predicate can be written as follows: { $ Pattern , $ Pattern : - $ Constraint } This constraint indicates that the left argument is smaller than the right argument, given that the constraint can be satisfied. The types of each patterns is similar to the type of the data-wellformedness predicate. When the queried relation is predicative (i.e. has no 'output'), the pattern is a tuple containing arguments of the declaration that is compared. If the queried relation is functional, a tuple with only the 'input' arguments must be provided. When there is only one declaration argument, the tuple may be omitted. There are several shorthands available for the data comparison constraint: true is equivalent to { _, _ :- true } , and hence ensures that declarations are shadowed based on the label order only. false is equivalent to { _, _ :- false } , and hence ensures that no shadowing is applied, even when paths can be ordered using the label order. $ConstraintName is equivalent to { d1, d2 :- $ConstraintName(d1, d2) } , which means that d1 shadows d2 when $ConstraintName(d1, d2) can be satisfied. { $Pattern, $Pattern } is equivalent to { $Pattern, $Pattern :- true } , which means that the first argument shadows the second if they match the patterns. Non-linear Patterns The data comparison shorthand is mostly used with non-linear patterns. For example, to encode that declarations with equal names shadow each other, the data comparison shorthand { x, x } can be used. When using this pattern however, please ensure that variable names are fresh, because the behavior of shadowing names is planned to change in the future. Partial Order Data comparison functions orders must be (non-strict) partial orders . That is, they are implicitly transitive and reflexive, but may not be symmetric. However, this is not validated for tractability reasons. Therefore, for any predicate that is not a partial order (other than true ), shadowing behavior is undefined. Syntactically, the shadowing parameters can be omitted altogether, or the data comparison predicate can be omitted, even when an label order is specified. The default value of the label order is an empty relation, while the default value of the data comparison predicate is true . On the one hand, this ensures that no shadowing is applied when no shadowing parameters are provided. On the other hand, when a label order, but no data comparison predicate is provided, all declarations shadow each other based on a path comparison only. Operationally, the label order and the data comparison constraints are applied conjunctively . For any declaration d and d' , if the path to d is smaller than the path to d' according to the label order, and the application of the data comparison constraint to d, d' can be satisfied, d' will be excluded from the query answer. Result Pattern \u00b6 When the query resolution is completed, the query result will be unified with the $Result term. This term is a list that contains path-datum entries. Therefore, the type of this term is list((path * R)) , where R is a tuple type with the argument types of the relation that is queried. In case the relation is functional, the 'output' type is included in the result. When the relation is unary, the tuple is omitted. When the query target is () , R is the scope type. For the syntactic structure of the paths, please refer to the section on path terms . Semantically, for any query answer pair (p, d) , the path p represents the path followed from the scope in which the query started to the scope in which the declaration of d was found. Top-level Destination scope Since path terms are left-recursive , have the source scope at the left side, and the destination scope (i.e. the scope in which the paired datum was declared) on the right, it turns out that the target scope is in the third argument of the top-level constructor. This is convenient, since it allows to access the target scope without destructuring the whole path. (Direct access to the source scope is not really important, since it is already available as an argument to the query constraint). On the other side, it is sometimes perceived as not completely intuitive to have the target scope in the top level constructor. When multiple results are returned by the query, Statix has no guarantees on the order of their appearence in the list. Query Sugar \u00b6 Warning Since Spoofax 2.5.15, the query sugar constructs are deprecated.","title":"Queries"},{"location":"references/statix/queries/#queries","text":"Scope Graphs, as introduced in the previous section can be queried. Scope graph queries always start in a particular scope, and traverse the scope graph in order to find declarations under a particular relation. The syntax for queries is as follows: query $ QueryTarget filter $ LabelRE and $ DataWF min $ LabelOrder * and $ DataLeq in $ Scope |-> $ Result The $Scope parameter is a term with type scope . In this scope, the query starts. All other query arguments are explained in the following subsections.","title":"Queries"},{"location":"references/statix/queries/#query-targets","text":"The query target is the 'thing' that is looked for in the query. This can be one of: $Relation : A relation identifier . In this case, the query will return data that is declared under that relation. () : the end of path query target. In this case, the query will return all paths that match the appropriate filters. () as relation The () query target can be thought of as a regular relation if one assumes #1 |-()-> #1 to exist for every scope #1 in the scope graph.","title":"Query Targets"},{"location":"references/statix/queries/#filters","text":"Query results can be filtered using two different filters. First, a regular expression on labels ( $LabelRE ) defines a filter on the paths that the query resolution algorithm will explore. This regular expression can be build from the following components: $Label : Matches paths that travers a single edge with the label $Label . Requires the label to be declared in a signature section, as explained in the section on edges . e : epsilon . Matches the empty path. Queries using this filter will only return declarations in the scope where the query started. 0 : empty set . Matches no path. $LabelRE $LabelRE : concatenation . Matches paths that can be split in two segments p1 and p2 , such that p1 matches the first regular expression, and p2 matches the second. $LabelRE | $LabelRE : disjunction . Matches paths that match either the first or the second regular expression. $LabelRE & $LabelRE : conjunction . Matches paths that match both the first and the second regular expression. ~$LabelRE : negation . Matches all paths that do not match the inner regular expression. $LabelRE* : closure . Matches paths that can be split into zero or more segments that each match the inner regular expression. $LabelRE+ : one or more . Matches paths that can be split into one or more segments that each match the inner regular expression. Equivalent to $LabelRE $LabelRE* $LabelRE? : zero or one . Matches paths that are matched by the inner regular expression, or empty paths. Equivalent to $LabelRE | e . Additionally, regular expression can be grouped by brackets. Second, data well-formedness filters ( $DataWF ) can be applied. These filters restrict which datums are included in the query result. They are expressed as anonymous lambda rules: { $ Pattern : - $ Constraint } This rule is instantiated for every declaration that is reachable according to the path well-formedness expression. When the instantiated body constraint holds, the declaration is included in the query answer. Lambda Instantiation Lambda constraint instantiation is similar to rule instantiation. For more information on rule instantiation, see the section about rule definitions . Entailment semantics Data well-formedness conditions are treated as entailment/implied conditions. Hence, they are not allowed to extend or refine the outer context. For more information on this evaluation mode, see the documentation of the try construct . The type of the predicate that is expected depends on the kind of relation that is used. For predicative relations, all arguments in the relation are provided to the data well-formedness constraint. However, for functional relations, the 'output value' (i.e. the value that the relation maps to) is not provided to the filter. When multiple arguments are provided to a data-wellformedness predicate (when there are multiple 'input' arguments to the queried relation), these arguments must be wrapped in a tuple. When the end-of-path query target () is used, the data-wellformedness constraint expects a single scope as argument. Example of filters A simple query for variables illustrates both filters. Suppose the relation var is in scope with type string -> TYPE . Then a rule (with type ascriptions ) that looks up a variable definition can be defined as follows. resolveVar ( s : scope , name : string ) = R : - query var filter P * I ? and { name' : string : - name' == name } in s |-> R . In this example, the path well-formedness expression P* I? indicates that any number of P edges may be traversed, and then, optionally, a single I edge. This resolution policy excludes e.g. transitive imports, and the traversal of P edges in an imported module. The anonymous data well-formedness condition states that a declaration with name name' may only be included in the query result if name' is equal to name from the enclosing scope. Suppose that resolveVar is instantiated for name |-> \"x\" , and declarations with name \"x\" and \"y\" are in scope. Now the anonymous inner rule is instantiated for both \"x\" and \"y\" . For \"x\" , the constraint \"x\" == \"x\" is generated, which can be solved successfully. For \"y\" however, the constraint \"y\" == \"x\" is generated, which cannot be solved successfully. Hence, only the declaration for \"x\" is included in the eventual query answer. There are three shorthands for common data well-formedness predicates: true , which is equivalent to { _ :- true } . This shorthand will thus include all encountered declaration in the query result. false , which is equivalent to { _ :- false } . This shorthand will include no declarations in the query answer. eq($Term) , is equivalent to { x :- x == $Term } and hence will include all declarations that are equal to $Term . Syntactically, the query filter can be omitted entirely, or the data well-formedness predicate can be omitted, even if a path filter is provided. By default, the path filter is ~0 , meaning that every path is considered valid, and the data well-formedness predicate is true , meaning that every datum will be returned in the query answer.","title":"Filters"},{"location":"references/statix/queries/#shadowing","text":"For many languages, name resolution involves dealing with shadowing correctly. In Statix queries, it is possible to encode shadowing policies using label orders and data comparison predicates. A declaration shadows another declaration iff its path is 'smaller than' the other path by a prefix order defined over a label comparison relation, and when the declaration data is smaller than or equal to the data of the other declaration according to a data comparison predicate. Label orders ( $LabelOrder ) are expressed as less-than relations on labels. $ Label < $ Label Here, a label is either a declared label symbol , or $ , which denotes the 'end-of-path' label. This label can be used to express orders on path length. For example, $ < P expresses that paths with fewer P labels are preferred over paths with more P labels. Strict Partial Order Label orders must be strict partial orders . That is, they are implicitly transitive, but may not be reflexive or symmetric. Label order specifications that are not strict partial orders will be rejected at specification loading time. Prefix order Note that the label order is a prefix order , not a lexicographical full-path order . That is, paths that diverged by traversing different edges with the same label are not ordered by this relation. In addition to a label ordering relation, a data comparison predicate ( $DataLeq ) can be provided. This predicate can be written as follows: { $ Pattern , $ Pattern : - $ Constraint } This constraint indicates that the left argument is smaller than the right argument, given that the constraint can be satisfied. The types of each patterns is similar to the type of the data-wellformedness predicate. When the queried relation is predicative (i.e. has no 'output'), the pattern is a tuple containing arguments of the declaration that is compared. If the queried relation is functional, a tuple with only the 'input' arguments must be provided. When there is only one declaration argument, the tuple may be omitted. There are several shorthands available for the data comparison constraint: true is equivalent to { _, _ :- true } , and hence ensures that declarations are shadowed based on the label order only. false is equivalent to { _, _ :- false } , and hence ensures that no shadowing is applied, even when paths can be ordered using the label order. $ConstraintName is equivalent to { d1, d2 :- $ConstraintName(d1, d2) } , which means that d1 shadows d2 when $ConstraintName(d1, d2) can be satisfied. { $Pattern, $Pattern } is equivalent to { $Pattern, $Pattern :- true } , which means that the first argument shadows the second if they match the patterns. Non-linear Patterns The data comparison shorthand is mostly used with non-linear patterns. For example, to encode that declarations with equal names shadow each other, the data comparison shorthand { x, x } can be used. When using this pattern however, please ensure that variable names are fresh, because the behavior of shadowing names is planned to change in the future. Partial Order Data comparison functions orders must be (non-strict) partial orders . That is, they are implicitly transitive and reflexive, but may not be symmetric. However, this is not validated for tractability reasons. Therefore, for any predicate that is not a partial order (other than true ), shadowing behavior is undefined. Syntactically, the shadowing parameters can be omitted altogether, or the data comparison predicate can be omitted, even when an label order is specified. The default value of the label order is an empty relation, while the default value of the data comparison predicate is true . On the one hand, this ensures that no shadowing is applied when no shadowing parameters are provided. On the other hand, when a label order, but no data comparison predicate is provided, all declarations shadow each other based on a path comparison only. Operationally, the label order and the data comparison constraints are applied conjunctively . For any declaration d and d' , if the path to d is smaller than the path to d' according to the label order, and the application of the data comparison constraint to d, d' can be satisfied, d' will be excluded from the query answer.","title":"Shadowing"},{"location":"references/statix/queries/#result-pattern","text":"When the query resolution is completed, the query result will be unified with the $Result term. This term is a list that contains path-datum entries. Therefore, the type of this term is list((path * R)) , where R is a tuple type with the argument types of the relation that is queried. In case the relation is functional, the 'output' type is included in the result. When the relation is unary, the tuple is omitted. When the query target is () , R is the scope type. For the syntactic structure of the paths, please refer to the section on path terms . Semantically, for any query answer pair (p, d) , the path p represents the path followed from the scope in which the query started to the scope in which the declaration of d was found. Top-level Destination scope Since path terms are left-recursive , have the source scope at the left side, and the destination scope (i.e. the scope in which the paired datum was declared) on the right, it turns out that the target scope is in the third argument of the top-level constructor. This is convenient, since it allows to access the target scope without destructuring the whole path. (Direct access to the source scope is not really important, since it is already available as an argument to the query constraint). On the other side, it is sometimes perceived as not completely intuitive to have the target scope in the top level constructor. When multiple results are returned by the query, Statix has no guarantees on the order of their appearence in the list.","title":"Result Pattern"},{"location":"references/statix/queries/#query-sugar","text":"Warning Since Spoofax 2.5.15, the query sugar constructs are deprecated.","title":"Query Sugar"},{"location":"references/statix/rules/","text":"Rules \u00b6 User-defined constraints and their rules make up the main part of a Statix specification. In this section, we describe the definition and usage of user-defined constraints and their rules. Constraint Definitions \u00b6 In order to define a custom constraint, its type must be declared first. A constraint can be declared in a rules section, or in a constraints subsection of a signature section. A constraint is declared by specifying its name and argument type. For more information on types, please refer to the Terms section. Note that the name of the constraint must be unique within a specification. $ ConstraintName : { $ Type \"*\" } * Terminology In this reference manual, we consistently use the term 'constraint declaration' for the introduction of new user-defined constraints. However, in practise, these are sometimes also referred to as 'predicate' or just simply 'constraint'. When a constraint declaration is provided this way, it can be used as a constraint by providing concrete arguments, separated by comma's. $ ConstraintName ({ $ Term \",\" } * ) $ Message ? The sorts of the argument terms should be equal to the sorts in the constraint declaration. Rule Definitions \u00b6 When solving a user-defined constraint, a rule for that constraint is unfolded in order to infer a model satisfying the constraint. [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ) : - $ Constraint . The part before the turnstile ( :- ) is often referred to as the head of the rule, while the $Constraint after the turnstile is denoted as body . When applying a rule, each head pattern (which is just a term) will be matched with its corresponding actual argument. Statically, the sorts of the terms in $Patterns are type-checked based on the constraint declaration. Any variables in patterns are implicitly introduced in the scope of the rule. Patterns can be non-linear. That is, a variable may occur multiple times in a pattern. Operationally, the subterms at these positions are then required to be structurally equal. Note that multiple rules for a single constraint can, and often will, be provided. For each constraint, the rule that is used for simplification is determined by the guard of the rule. This guard is derived from the head pattern: a rule can only be applied when the constraint arguments match the patterns. During constraint solving, Statix will try at most one rule for each constraint. The appropriate rule is selected by applying the following heuristics in order: 1. Rules with a smaller domain are preferred over rules with a larger domain. 2. When pairwise comparing rules, the rule for which, in left-to-right order, a more specific pattern is encountered first is preferred over the other. For all cases where these heuristics do not decide which rule to use for a constraint, compile time \"Overlapping patterns\" errors will be emitted. The $RuleName is just a name that can be used for documentation purposes. It cannot be referenced from any position in the specification, and may be omitted altogether. Axiom rules \u00b6 In some cases, a constraint trivially holds for particular inputs. For such constraints, an axiom rule can be specified. [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ). This rule is similar to a regular rule, but lacks a body. When applying such a rule, no new constraints are introduced, reflecting the fact that the constraint trivially holds for these arguments. Functional Rules \u00b6 Some user-defined constraints can be thought of more naturally as a function: a constraint where a particular term is inferred by the constraint, rather than validated. Statix allows to write constraints in a functional idiom as follows: First, a constraint declaration for such 'functional constraints' must be provided as follows: $ ConstraintName : { $ Type \"*\" } * -> $ Type In addition to the regular list of input sorts, a sort for the output term is provided to the constraint declaration. Rule definitions for a functional constraint look as follows: [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ) = $ Term : - $ Constraint . Compared to predicative rule definitions as introduced earlier in this section, an additional term after an equality-sign is appended to the rule head. This term denotes the output term (the term inferred by the rule). A functional constraint can be used in a term position, as opposed to a constraint position for predicative rules. Otherwise, their syntax is the same. $ ConstraintName ({ $ Term \",\" } * ) Semantically, the output term of applying the constraint is substituted at the position of the application of the functional predicate. Terminology: Functional vs. Predicative When we want to make the distinction between these two forms of constraints explicit, we usually refer to either groups with 'predicative constraint declarations' and 'predicative constraints', versus 'functional constraint declarations' and 'functional constraints', respectively. Normalization Every specification with functional predicates is normalized to a form with only regular predicates. To show the normal form of a specification in Eclipse, use the Spoofax > Syntax > Format normalized AST menu action. Mapping rules \u00b6 Another common pattern in Statix is defining a predicate that instantiates a predicate for all elements in a list. Statix allows derive such mapping rules using the maps keyword as follows: $ MappingConstraintName maps $ MappedConstraintName ({ $ Lift \",\" }) A lift specifier ( $Lift ) can be one of the following: * : The identity lift . This lift specifier indicates that this argument is passed to the mapped constraint unchanged. list(*) : The list lift : This lift specifier indicates that the mapped constraint will be instantiated for each element in the list at that argument position. Each constraint defined with maps , must contain at least one list lift. Otherwise, the mapping would be a no-op. ({$Lift \",\"}+) : The tuple lift : This lift specifier indicates that arguments are extracted from a tuple. For each tuple argument, a corresponding lifting is applied afterwards. The type of $MappingConstraintName is inferred by inverse application of the lift specifiers to the type of $MappedConstraintName . Therefore, no explicit declaration of the type of the mapping constraint is required. Similar to predicative constraints, functional mapping constraints can be derived: $ MappingConstraintName maps $ MappedConstraintName ({ $ Lift \",\" }) = $ Lift In addition to lift specifiers of the input arguments, a lift specifier for the inferred term must be provided as well. This lift specifier indicates how the inferred terms from the mapped constraints are aggregated and returned by the mapping constraint. Example. A common example where mapping rules are used is when type-checking a list of declarations. A specification snippet for that could look as follows: rules declOk : scope * Decl declsOk maps declOk ( * , list ( * )) // rules for declOk In this snippet, the declsOk constraint instantiates declOk for each declaration in a list of declaration. Its inferred type is scope * list(Decl) . When mapping functional constraints, a lift specifier for the inferred term must be provided as well. This lift specifier indicates how the inferred values of the mapped constraint are returned by the mapping constraint. When using multiple list lifts in the input, the resulting constraint will zip the arguments. This implicitly requires the input lists to be of equal length. The creation of a cartesian product can be achieved by repeated application of the maps construct for each argument. Normalization Similar to functional constraints, constraints derived using the maps construct are normalized to regular predicative constraints. This normalization can be inspected using the Spoofax > Syntax > Format normalized AST menu action. Injections of Namespaces and Relations \u00b6 For convenience, it is possible to declare namespaces, namespace queries (both deprecated) and relations in a rules section as well. rules namespace Var : string resolve Var filter P * I * relation var : string -> TYPE","title":"Rules"},{"location":"references/statix/rules/#rules","text":"User-defined constraints and their rules make up the main part of a Statix specification. In this section, we describe the definition and usage of user-defined constraints and their rules.","title":"Rules"},{"location":"references/statix/rules/#constraint-definitions","text":"In order to define a custom constraint, its type must be declared first. A constraint can be declared in a rules section, or in a constraints subsection of a signature section. A constraint is declared by specifying its name and argument type. For more information on types, please refer to the Terms section. Note that the name of the constraint must be unique within a specification. $ ConstraintName : { $ Type \"*\" } * Terminology In this reference manual, we consistently use the term 'constraint declaration' for the introduction of new user-defined constraints. However, in practise, these are sometimes also referred to as 'predicate' or just simply 'constraint'. When a constraint declaration is provided this way, it can be used as a constraint by providing concrete arguments, separated by comma's. $ ConstraintName ({ $ Term \",\" } * ) $ Message ? The sorts of the argument terms should be equal to the sorts in the constraint declaration.","title":"Constraint Definitions"},{"location":"references/statix/rules/#rule-definitions","text":"When solving a user-defined constraint, a rule for that constraint is unfolded in order to infer a model satisfying the constraint. [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ) : - $ Constraint . The part before the turnstile ( :- ) is often referred to as the head of the rule, while the $Constraint after the turnstile is denoted as body . When applying a rule, each head pattern (which is just a term) will be matched with its corresponding actual argument. Statically, the sorts of the terms in $Patterns are type-checked based on the constraint declaration. Any variables in patterns are implicitly introduced in the scope of the rule. Patterns can be non-linear. That is, a variable may occur multiple times in a pattern. Operationally, the subterms at these positions are then required to be structurally equal. Note that multiple rules for a single constraint can, and often will, be provided. For each constraint, the rule that is used for simplification is determined by the guard of the rule. This guard is derived from the head pattern: a rule can only be applied when the constraint arguments match the patterns. During constraint solving, Statix will try at most one rule for each constraint. The appropriate rule is selected by applying the following heuristics in order: 1. Rules with a smaller domain are preferred over rules with a larger domain. 2. When pairwise comparing rules, the rule for which, in left-to-right order, a more specific pattern is encountered first is preferred over the other. For all cases where these heuristics do not decide which rule to use for a constraint, compile time \"Overlapping patterns\" errors will be emitted. The $RuleName is just a name that can be used for documentation purposes. It cannot be referenced from any position in the specification, and may be omitted altogether.","title":"Rule Definitions"},{"location":"references/statix/rules/#axiom-rules","text":"In some cases, a constraint trivially holds for particular inputs. For such constraints, an axiom rule can be specified. [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ). This rule is similar to a regular rule, but lacks a body. When applying such a rule, no new constraints are introduced, reflecting the fact that the constraint trivially holds for these arguments.","title":"Axiom rules"},{"location":"references/statix/rules/#functional-rules","text":"Some user-defined constraints can be thought of more naturally as a function: a constraint where a particular term is inferred by the constraint, rather than validated. Statix allows to write constraints in a functional idiom as follows: First, a constraint declaration for such 'functional constraints' must be provided as follows: $ ConstraintName : { $ Type \"*\" } * -> $ Type In addition to the regular list of input sorts, a sort for the output term is provided to the constraint declaration. Rule definitions for a functional constraint look as follows: [$ RuleName ]$ ConstraintName ({ $ Pattern \",\" } * ) = $ Term : - $ Constraint . Compared to predicative rule definitions as introduced earlier in this section, an additional term after an equality-sign is appended to the rule head. This term denotes the output term (the term inferred by the rule). A functional constraint can be used in a term position, as opposed to a constraint position for predicative rules. Otherwise, their syntax is the same. $ ConstraintName ({ $ Term \",\" } * ) Semantically, the output term of applying the constraint is substituted at the position of the application of the functional predicate. Terminology: Functional vs. Predicative When we want to make the distinction between these two forms of constraints explicit, we usually refer to either groups with 'predicative constraint declarations' and 'predicative constraints', versus 'functional constraint declarations' and 'functional constraints', respectively. Normalization Every specification with functional predicates is normalized to a form with only regular predicates. To show the normal form of a specification in Eclipse, use the Spoofax > Syntax > Format normalized AST menu action.","title":"Functional Rules"},{"location":"references/statix/rules/#mapping-rules","text":"Another common pattern in Statix is defining a predicate that instantiates a predicate for all elements in a list. Statix allows derive such mapping rules using the maps keyword as follows: $ MappingConstraintName maps $ MappedConstraintName ({ $ Lift \",\" }) A lift specifier ( $Lift ) can be one of the following: * : The identity lift . This lift specifier indicates that this argument is passed to the mapped constraint unchanged. list(*) : The list lift : This lift specifier indicates that the mapped constraint will be instantiated for each element in the list at that argument position. Each constraint defined with maps , must contain at least one list lift. Otherwise, the mapping would be a no-op. ({$Lift \",\"}+) : The tuple lift : This lift specifier indicates that arguments are extracted from a tuple. For each tuple argument, a corresponding lifting is applied afterwards. The type of $MappingConstraintName is inferred by inverse application of the lift specifiers to the type of $MappedConstraintName . Therefore, no explicit declaration of the type of the mapping constraint is required. Similar to predicative constraints, functional mapping constraints can be derived: $ MappingConstraintName maps $ MappedConstraintName ({ $ Lift \",\" }) = $ Lift In addition to lift specifiers of the input arguments, a lift specifier for the inferred term must be provided as well. This lift specifier indicates how the inferred terms from the mapped constraints are aggregated and returned by the mapping constraint. Example. A common example where mapping rules are used is when type-checking a list of declarations. A specification snippet for that could look as follows: rules declOk : scope * Decl declsOk maps declOk ( * , list ( * )) // rules for declOk In this snippet, the declsOk constraint instantiates declOk for each declaration in a list of declaration. Its inferred type is scope * list(Decl) . When mapping functional constraints, a lift specifier for the inferred term must be provided as well. This lift specifier indicates how the inferred values of the mapped constraint are returned by the mapping constraint. When using multiple list lifts in the input, the resulting constraint will zip the arguments. This implicitly requires the input lists to be of equal length. The creation of a cartesian product can be achieved by repeated application of the maps construct for each argument. Normalization Similar to functional constraints, constraints derived using the maps construct are normalized to regular predicative constraints. This normalization can be inspected using the Spoofax > Syntax > Format normalized AST menu action.","title":"Mapping rules"},{"location":"references/statix/rules/#injections-of-namespaces-and-relations","text":"For convenience, it is possible to declare namespaces, namespace queries (both deprecated) and relations in a rules section as well. rules namespace Var : string resolve Var filter P * I * relation var : string -> TYPE","title":"Injections of Namespaces and Relations"},{"location":"references/statix/scope-graphs/","text":"Scope Graph Constraints \u00b6 One of the core concepts of Statix is modelling name binding structures using scope graphs . Scope graphs consist of three different components, summarized in the following table. Component Description Textual Notation Graphical notation Scope Region in a program with uniform behavior wrt. name resolution. Represented as nodes in a graph. #1 Circular node Edge Directed edges with upper-case labels model reachability between scopes. #1 -L-> #2 Labeled arrow Declaration Declarations associate data terms with a particular scope under a particular relation. #1 |-r-> d Labeled block arrow Using these components, and leveraging the fact that scopes are regular terms, many name-binding patterns can be modelled. In the remainder of this section, we will explain how scope graph constraints can be expressed in the Statix language. Scopes \u00b6 First, scopes can be created using the new keyword: new $ Var + For each var in the list of variables provided to the new constraint, a fresh scope is generated, and bound to that particular variable. Note that the $Var s are not introduced by this constraints, but rather have to be introduced earlier in a rule head or using an existential constraint . Statix guarantees that each scope has a unique identity. Scopes that are generated by different new constraints can never be equal under the equality constraint . Edges \u00b6 The existence of edges in a scope graph can be asserted using edge assertion constraints: $ Term - $ Label- > $ Term This constraint ensures that an edge from the first term argument to the last argument (which must have type scope ) with a label $Label exists. Edge Assertions not Idempotent Edge constraints are not idempotent. That is, repeated edge assertions will result in multiple equivalent edges in a scope graph. However, because query results have set semantics, and edges have structural identity, declarations reached via such a duplicated edge will not be duplicated in a query answer. For example (assuming familiarity with queries and tests ), the constraint: { R s1 s2 } new s1 s2 , s1 -P-> s2 , s1 -P-> s2 , query () filter P in s1 |-> R will give the following (slightly simplified) result: substitution R |-> [ ( PathStep ( PathEmpty ( # s1 ), P , # s2 ), # s2 ) ] analysis scope graph # s1 { edges { P : # s2 # s2 } } In this result, the P edge is duplicated in the scope graph, but there is still only a single query result. It is required to declare edge labels in a signature section: signature name-resolution labels $ Label + Label are just uppercase identifiers. They must adhere to the following regular expression: [A-Z] [A-Za-z0-9\\_] . It is not allowed to shadow label names, nor may modules import equivalent label names from different modules. Declarations \u00b6 Declarations in a scope graph can be asserted as follows: !$ Relation [$ Term *] in $ Scope This constraint asserts that the terms inside the square brackets are associated with scope $Scope under relation $Relation . Regarding the static semantics of this constraint, $Scope is a term which should have type scope . Additionally, the term arguments must adhere to the signature of the relation. The signature of a relation must be provided in a signature section: signature relation $ Relation : { $ Type \"*\" } * Just as rules , relations can alternatively be declared in a functional style: $ Relation : { $ Type \"*\" } * -> $ Type The style of declaration is importand when doing queries , but asserting declarations is similar for both types of relations. When asserting a declaration for a functional relation, the terms that the relation maps to should be provided as the last term in the square brackets. Instead of in a signature section, relations can be declared in a rules section as well. It is allowed to have multiple declaration assertions for a single relation in the same scope, even when the data of the different relations are equivalent. In the latter case, multiple equivalent declarations will be inserted in the scope graph. Permission to Extend \u00b6 In order to make query execution sound, Statix statically limits to which scopes new edges or declarations may be added (adding a edge or declaration is often called extending). Scope extension is only allowed in the following cases: Scopes that are freshly instantiated in a rule using new constraints may be extended by that rule, and any user-defined constraint that is instantiated by that rule. Scopes that are passed as direct argument to a rule may be extended by that rule, given that the scope may be extended by rule that instantiated that constraint. This property is validated at the instantiation site. Uninstantiated scope variables that are passed directly to a user-defined constraint in which they are instantiated may be extended by the outer constraint as well. However, this does not hold for input/output values of functional rules These rules prevent extension of scopes that are obtained by pattern matching/ deconstruction. Namespaces and Occurrences \u00b6 Warning Since Spoofax 2.5.15, namespaces and occurrences are deprecated.","title":"Scope Graph Constraints"},{"location":"references/statix/scope-graphs/#scope-graph-constraints","text":"One of the core concepts of Statix is modelling name binding structures using scope graphs . Scope graphs consist of three different components, summarized in the following table. Component Description Textual Notation Graphical notation Scope Region in a program with uniform behavior wrt. name resolution. Represented as nodes in a graph. #1 Circular node Edge Directed edges with upper-case labels model reachability between scopes. #1 -L-> #2 Labeled arrow Declaration Declarations associate data terms with a particular scope under a particular relation. #1 |-r-> d Labeled block arrow Using these components, and leveraging the fact that scopes are regular terms, many name-binding patterns can be modelled. In the remainder of this section, we will explain how scope graph constraints can be expressed in the Statix language.","title":"Scope Graph Constraints"},{"location":"references/statix/scope-graphs/#scopes","text":"First, scopes can be created using the new keyword: new $ Var + For each var in the list of variables provided to the new constraint, a fresh scope is generated, and bound to that particular variable. Note that the $Var s are not introduced by this constraints, but rather have to be introduced earlier in a rule head or using an existential constraint . Statix guarantees that each scope has a unique identity. Scopes that are generated by different new constraints can never be equal under the equality constraint .","title":"Scopes"},{"location":"references/statix/scope-graphs/#edges","text":"The existence of edges in a scope graph can be asserted using edge assertion constraints: $ Term - $ Label- > $ Term This constraint ensures that an edge from the first term argument to the last argument (which must have type scope ) with a label $Label exists. Edge Assertions not Idempotent Edge constraints are not idempotent. That is, repeated edge assertions will result in multiple equivalent edges in a scope graph. However, because query results have set semantics, and edges have structural identity, declarations reached via such a duplicated edge will not be duplicated in a query answer. For example (assuming familiarity with queries and tests ), the constraint: { R s1 s2 } new s1 s2 , s1 -P-> s2 , s1 -P-> s2 , query () filter P in s1 |-> R will give the following (slightly simplified) result: substitution R |-> [ ( PathStep ( PathEmpty ( # s1 ), P , # s2 ), # s2 ) ] analysis scope graph # s1 { edges { P : # s2 # s2 } } In this result, the P edge is duplicated in the scope graph, but there is still only a single query result. It is required to declare edge labels in a signature section: signature name-resolution labels $ Label + Label are just uppercase identifiers. They must adhere to the following regular expression: [A-Z] [A-Za-z0-9\\_] . It is not allowed to shadow label names, nor may modules import equivalent label names from different modules.","title":"Edges"},{"location":"references/statix/scope-graphs/#declarations","text":"Declarations in a scope graph can be asserted as follows: !$ Relation [$ Term *] in $ Scope This constraint asserts that the terms inside the square brackets are associated with scope $Scope under relation $Relation . Regarding the static semantics of this constraint, $Scope is a term which should have type scope . Additionally, the term arguments must adhere to the signature of the relation. The signature of a relation must be provided in a signature section: signature relation $ Relation : { $ Type \"*\" } * Just as rules , relations can alternatively be declared in a functional style: $ Relation : { $ Type \"*\" } * -> $ Type The style of declaration is importand when doing queries , but asserting declarations is similar for both types of relations. When asserting a declaration for a functional relation, the terms that the relation maps to should be provided as the last term in the square brackets. Instead of in a signature section, relations can be declared in a rules section as well. It is allowed to have multiple declaration assertions for a single relation in the same scope, even when the data of the different relations are equivalent. In the latter case, multiple equivalent declarations will be inserted in the scope graph.","title":"Declarations"},{"location":"references/statix/scope-graphs/#permission-to-extend","text":"In order to make query execution sound, Statix statically limits to which scopes new edges or declarations may be added (adding a edge or declaration is often called extending). Scope extension is only allowed in the following cases: Scopes that are freshly instantiated in a rule using new constraints may be extended by that rule, and any user-defined constraint that is instantiated by that rule. Scopes that are passed as direct argument to a rule may be extended by that rule, given that the scope may be extended by rule that instantiated that constraint. This property is validated at the instantiation site. Uninstantiated scope variables that are passed directly to a user-defined constraint in which they are instantiated may be extended by the outer constraint as well. However, this does not hold for input/output values of functional rules These rules prevent extension of scopes that are obtained by pattern matching/ deconstruction.","title":"Permission to Extend"},{"location":"references/statix/scope-graphs/#namespaces-and-occurrences","text":"Warning Since Spoofax 2.5.15, namespaces and occurrences are deprecated.","title":"Namespaces and Occurrences"},{"location":"references/statix/stratego-api/","text":"Stratego API \u00b6 The Statix solver can be called from a Stratego Transformation using the API in the Statix Runtime project. After analysis is executed, the analysis result can be queried using other strategies. In this section, we provide an overview of the availble strategies. The public API of the Statix runtime is available here . This API strongly depends on the Spoofax constraint analysis library, which is available in the Spoofax Meta Library . Single-File Analysis \u00b6 stx-editor-analyze ( pre , post | spec-name , init-constraint ) Type: AnalysisAction -> AnalysisResult . Applies single-file analysis with the specification provided in the arguments. Since this strategy only performs single-file analysis, the current term should always have AnalyzeSingle as top-level constructor. The term and strategy arguments are explained in the following table: Argument Type Default Description pre AST -> AST id Transformation to apply before analyzing. This transformation receives the 'parsed AST'. post AST -> AST id Transformation to apply before analyzing. This transformation receives the result of applying pre to the 'parsed AST', and yields the 'analyzed AST'. spec-name String The full path to the root module of the specification. init-constraint String The name of the constraint that should be applied to the pre-transformed AST. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : Start The post and pre arguments may be omitted (in that order). Note that this strategy will return no results for Cached terms in the analysis action. Single-File Elaboration \u00b6 stx-editor-elaborate ( pre , post | spec-name , init-constraint ) Todo Find out what this is? Multi-File Analysis \u00b6 stx-editor-analyze ( pre , post | spec-name , project-constraint , file-constraint ) Type: AnalysisAction -> AnalysisResult . Applies multi-file analysis with the specification provided in the arguments. Since this strategy only performs multi-file analysis, the current term should always have AnalyzeMulti as top-level constructor. This strategy chooses the solver based on the metaborg.yaml configuration of the project and the language. The term and strategy arguments are explained in the following table: Argument Type Description pre AST -> AST Transformation to apply before analyzing. This transformation receives the 'parsed AST'. post AST -> AST Transformation to apply before analyzing. This transformation receives the result of applying pre to the 'parsed AST', and yields the 'analyzed AST'. spec-name String The full path to the root module of the specification. project-constraint String The name of the constraint that should be applied to the global scope once. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : scope file-constraint String The name of the constraint that should be applied to the pre-transformed AST of each file in the project. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : scope * Start Concurrent Analysis \u00b6 stx-editor-analyze ( pre , group , post | spec-name , project-constraint , group-constraint , file-constraint ) Type: AnalysisAction -> AnalysisResult . Applies concurrent multi-file analysis with the specification provided in the arguments. Since this strategy only performs multi-file analysis, the current term should always have AnalyzeMulti as top-level constructor. This strategy chooses the solver based on the metaborg.yaml configuration of the project and the language, but fails with a fatal error if the concurrent solver is not enabled on this project. The term and strategy arguments are explained in the following table: Argument Type Description pre AST -> AST Transformation to apply before analyzing. This transformation receives the 'parsed AST'. group (Resource * AST) -> List(String) Strategy that decides in which group a resource is analyzed. Should return a list with the identifiers of the groups in which the resource should be analyzed. post AST -> AST Transformation to apply before analyzing. This transformation receives the result of applying pre to the 'parsed AST', and yields the 'analyzed AST'. spec-name String The full path to the root module of the specification. project-constraint String The name of the constraint that should be applied to the global scope once. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : scope group-constraint String The name of the constraint that should be applied to the each group. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : scope * string * scope file-constraint String The name of the constraint that should be applied to the pre-transformed AST of each file in the project. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : scope * Start Constraint Evaluation \u00b6 stx-evaluate (| spec-name , constraint ) Type: List(Term) -> Term Evaluates a functional constraint . The terms in the current term are passed as argument to the constraint, and the result term is the output term of the constraint. Argument Type Description spec-name String The full path to the root module of the specification. constraint String The name of the constraint that should be applied to the term arguments. Analysis Errors \u00b6 stx-analysis-has-errors = stx--analysis-has-errors Type: Analysis -> Analysis Fails if the current term has no errors, succeeds otherwise.","title":"Stratego API"},{"location":"references/statix/stratego-api/#stratego-api","text":"The Statix solver can be called from a Stratego Transformation using the API in the Statix Runtime project. After analysis is executed, the analysis result can be queried using other strategies. In this section, we provide an overview of the availble strategies. The public API of the Statix runtime is available here . This API strongly depends on the Spoofax constraint analysis library, which is available in the Spoofax Meta Library .","title":"Stratego API"},{"location":"references/statix/stratego-api/#single-file-analysis","text":"stx-editor-analyze ( pre , post | spec-name , init-constraint ) Type: AnalysisAction -> AnalysisResult . Applies single-file analysis with the specification provided in the arguments. Since this strategy only performs single-file analysis, the current term should always have AnalyzeSingle as top-level constructor. The term and strategy arguments are explained in the following table: Argument Type Default Description pre AST -> AST id Transformation to apply before analyzing. This transformation receives the 'parsed AST'. post AST -> AST id Transformation to apply before analyzing. This transformation receives the result of applying pre to the 'parsed AST', and yields the 'analyzed AST'. spec-name String The full path to the root module of the specification. init-constraint String The name of the constraint that should be applied to the pre-transformed AST. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : Start The post and pre arguments may be omitted (in that order). Note that this strategy will return no results for Cached terms in the analysis action.","title":"Single-File Analysis"},{"location":"references/statix/stratego-api/#single-file-elaboration","text":"stx-editor-elaborate ( pre , post | spec-name , init-constraint ) Todo Find out what this is?","title":"Single-File Elaboration"},{"location":"references/statix/stratego-api/#multi-file-analysis","text":"stx-editor-analyze ( pre , post | spec-name , project-constraint , file-constraint ) Type: AnalysisAction -> AnalysisResult . Applies multi-file analysis with the specification provided in the arguments. Since this strategy only performs multi-file analysis, the current term should always have AnalyzeMulti as top-level constructor. This strategy chooses the solver based on the metaborg.yaml configuration of the project and the language. The term and strategy arguments are explained in the following table: Argument Type Description pre AST -> AST Transformation to apply before analyzing. This transformation receives the 'parsed AST'. post AST -> AST Transformation to apply before analyzing. This transformation receives the result of applying pre to the 'parsed AST', and yields the 'analyzed AST'. spec-name String The full path to the root module of the specification. project-constraint String The name of the constraint that should be applied to the global scope once. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : scope file-constraint String The name of the constraint that should be applied to the pre-transformed AST of each file in the project. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : scope * Start","title":"Multi-File Analysis"},{"location":"references/statix/stratego-api/#concurrent-analysis","text":"stx-editor-analyze ( pre , group , post | spec-name , project-constraint , group-constraint , file-constraint ) Type: AnalysisAction -> AnalysisResult . Applies concurrent multi-file analysis with the specification provided in the arguments. Since this strategy only performs multi-file analysis, the current term should always have AnalyzeMulti as top-level constructor. This strategy chooses the solver based on the metaborg.yaml configuration of the project and the language, but fails with a fatal error if the concurrent solver is not enabled on this project. The term and strategy arguments are explained in the following table: Argument Type Description pre AST -> AST Transformation to apply before analyzing. This transformation receives the 'parsed AST'. group (Resource * AST) -> List(String) Strategy that decides in which group a resource is analyzed. Should return a list with the identifiers of the groups in which the resource should be analyzed. post AST -> AST Transformation to apply before analyzing. This transformation receives the result of applying pre to the 'parsed AST', and yields the 'analyzed AST'. spec-name String The full path to the root module of the specification. project-constraint String The name of the constraint that should be applied to the global scope once. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : scope group-constraint String The name of the constraint that should be applied to the each group. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : scope * string * scope file-constraint String The name of the constraint that should be applied to the pre-transformed AST of each file in the project. This may be a fully qualified name in the form $Module!$ConstraintName , or just a $ConstraintName . In the latter case, the name will be qualified with the spec-name argument. This constraint should have the (Statix) type : scope * Start","title":"Concurrent Analysis"},{"location":"references/statix/stratego-api/#constraint-evaluation","text":"stx-evaluate (| spec-name , constraint ) Type: List(Term) -> Term Evaluates a functional constraint . The terms in the current term are passed as argument to the constraint, and the result term is the output term of the constraint. Argument Type Description spec-name String The full path to the root module of the specification. constraint String The name of the constraint that should be applied to the term arguments.","title":"Constraint Evaluation"},{"location":"references/statix/stratego-api/#analysis-errors","text":"stx-analysis-has-errors = stx--analysis-has-errors Type: Analysis -> Analysis Fails if the current term has no errors, succeeds otherwise.","title":"Analysis Errors"},{"location":"references/statix/terms/","text":"Terms \u00b6 In Statix, data is represented using terms. This data can be a program, a typing annotation, or anything else that the specification defines. Terms are built from atoms and composites, such as constructors, tuples and lists. Additionally, Statix allows to inline several constraint results in terms. In this section, we explain the various types of terms that Statix supports, and, when appropriate, how their types should be declared. Terminology: Sort vs. Type Throughout this reference manual, we use the term 'sort' for syntactic categories, and 'type' for all other types (such as lists, tuples, scopes, etc.). However, in practice, these terms are both used in both meanings. Numerals \u00b6 Numeric literals are literals of the form [0-9]+ . Negative literals are not supported directly. All integer literals have the built-in type int . Strings \u00b6 String literals are arbitrary, single-line sequences of characters enclosed in double quotes. String literals may not contain unescaped backslashes, double quotes, or tabs. Double quotes and backslashes can be used in a string literal by prefixing them with another backslashes ( \\\" and \\\\ , respectively), while tabs, newlines and carriage returns can be encoded using respectively \\t , \\n and \\r . Otherwise, no escaping is required. String literals have the built-in type string . Identifiers \u00b6 Variables are identifiers of values of the following form: [a-zA-Z] [a-zA-Z0-9\\_]* [\\']* . With respect to type-checking, variables can be handled in two ways. When a variable occurs in the head of a rule , it is implicitly brought into scope with the type inferred from the rule type. Otherwise, it is required that the variable is introduced earlier, with the correct type. Apart from introduction in rule heads, variables can be introduced by existential constraints . In that case, the type of the variable is derived from its usage. Rule patterns may be non-linear (containing multiple occurrences of a variable), variables in an existential must be unique. That is, the constraint {x x} ... will give a type error. Shadowing variables is allowed, but discouraged by a warning. Substitution and Unification Although the Statix language does not distinguish between variables in rule heads and existentials, the solver treats those quite differently. On rule application, occurrences of variables from rule heads in the body of the rule are substituted by the actual arguments. For example, in a specification containing the rule rule ( x ) : - x == (). , simplifying the constraint rule(()) will result in the residual constraint () == () , which trivially holds. However, when simplifying the constraint {x} x == () , simplifying will generate a fresh unification variable (say ?x-1 ), and substitute that in the constraint, yielding ?x-1 == () . Solving that will create a new mapping ?x-1 |-> () in the unifier of the solver result. ``` Wildcards \u00b6 Wildcards are represented as _ , and denote variables without identity. Every occurrence of a wildcard is interpreted as a new variable. Because wildcards cannot reference each other, it is not required that the types of multiple wildcard occurrences coincide. Composite terms \u00b6 Composite terms can be build using constructor applications : $ ConsId ({ $ Term \",\" } * ) Here a term with constructor $ConsId and some term arguments is built. Composite terms must adhere to a signature. A signature describes which term compositions are valid, and must be declared in a signature section: signature sorts $ SortID * constructors $ ConsId : { $ Type \"*\" } + -> $ SortID $ ConsId : $ SortID First, the syntactic categories (which closely correspond to type identifiers in other languages) must be declared in a sorts subsection. Then, the constructor symbols can be declared in a constructors section. For each constructor, the types of the arguments and its sort should be provided. For nullary constructors (constructors without arguments), the arrow preceding the sort should be omitted. When a composite term is built, it is validated that all arguments match the type declaration from the signature. The type of the whole composite term is equal to the sort of the constructor. Tuples \u00b6 A built-in composite data construction is tuples : ({ $ Term \",\" } * ) Tuples have a statically fixed length, but the types of the arguments may differ. The type of the tuple expression is just the product of its arguments. The arity of a tuple may be anything except one, because unary tuples cause syntactic ambiguities with bracketed expressions. Lists \u00b6 Another built-in composite data construction is lists : [ { $ Term \",\" } *] Lists are created by comma-separating terms, enclosing them in square brackets. All terms should have the same type. Given that the type of the terms is T , the type of the list expression will be list(T) . Alternatively, lists can have a variable tail: [ { $ Term \",\" } * | $ Term ] In this syntax, the tail of the list is another term. This term should have type list(T) , where T is again the type of the first terms. Name Ascription \u00b6 It is possible to assign names to terms by prefixing the term with a variable name: $ Var @$ Term Note that this does not introduce a new variable with name $Var (except in a rule head , where all variables are introduced implicitly), but rather requires that a variable with corresponding name and type is already introduced. Ascribe and Equality In terms of equality constraints , the ascribe is equal to $Var == $Term . It is used to prevent the duplication of $Term . Type Ascription \u00b6 Statix allows to add inline type annotations to terms as follows: $ Term : $ Type The type-checker will validate that the term actually has the specified type, but the runtime behavior in not influenced by these ascriptions. Complete Inference In general, the Statix type-checker should be able to infer all types. However, in case of a type error being reported at an incorrect position, these type ascriptions can help tracing the cause of the error. Arithmetic operations \u00b6 Arithmetic expressions can be inserted in terms as follows: # ( $ ArithExp ) Here, the type of the expression is int . For more information on arithmetic expressions, see Arithmetic Constraints Normalization In terms of existential constraints , inline arithmetic expressions have behavior equal to {v} v #= $ArithExp , where v is used at the position of the arithmetic expression. So, for example, {T} T == CONS(#(21 * 2)) is equal to {T v} v #= 21 * 2, T == CONS(v) . AST Identifier \u00b6 In Spoofax, all terms in an AST are assigned a unique identifier (the term index) before analysis. This term identifier can be isolated as follows: astId ( $ Term ) Here, the type of $Term can be anything, and the type of the whole term will be astId . AST Identifiers are used to assign properties . New \u00b6 Statix allows inline creation of scopes : new Statically, the new term has type scope . At runtime, this creates a fresh scope, and inserts that at the position of the new term. Normalization In terms of existential constraints , the inline new operator has behavior equal to {s} new s , where s is used at the position of the new term. So, for example, {T} T == CLASS(new) is equal to {T s} new s, T == CLASS(s) . Paths \u00b6 Part of a query result is the path from the resolved datum back to the scope where the query started. In order to represent paths, Statix has two built-in constructors: _PathEmpty : Unary constructor that carries a single scope. This constructor has type scope -> path . _PathStep : Ternary constructor that represents a traversed edge in a path. This constructor has type path * label * scope -> path . Label Constraints Although the labels in a _PathStep can be bound to a variable, and hence be compared with and included in other terms, no inspection, matching or comparison with label definitions is supported. Occurrences \u00b6 Warning Since Spoofax 2.5.15, namespaces and occurrences are deprecated. Statix has built-in support for namespaces. A term embedded in a particular namespace is called an occurrence . Occurrences can be written as follows: $ NamespaceId { $ SpaceTerms } $ NamespaceId { $ SpaceTerms @$ OccurrenceId } In this structure template, $SpaceTerms means a list of terms, separated by spaces. The occurrence identifier can be any term. In case the term has an AST identifier, that value will be used as the identity of the occurrence. Alternatively, the occurrence identifier can be left out: $ NamespaceId { $ SpaceTerms } The default occurrence identifier is - , which means that the occurrence has no identifier. The type of an occurrence literal is occurrence . For more information about namespaces, see the Queries section. Declaration Match \u00b6 Statix allows to query the current scope for declarations of a particular form: ?$ RelationId [ { $ Term \",\" } ] in $ Scope When using this expression, a functional relation $RelationId must be declared. The terms arguments must correspond to the argument of the relation, and the type of the term is the output type of the relation. For more information on querying the scope graph, see the Queries section. Declaration Match as Query In terms of regular queries , the declaration match is equal to a query with filter e , expecting a single output. E.g. T == ?var[\"x\"] in s is equal to query var filter e and { x' :- x' == \"x\" } in s |-> [(_, (_, T))] .","title":"Terms"},{"location":"references/statix/terms/#terms","text":"In Statix, data is represented using terms. This data can be a program, a typing annotation, or anything else that the specification defines. Terms are built from atoms and composites, such as constructors, tuples and lists. Additionally, Statix allows to inline several constraint results in terms. In this section, we explain the various types of terms that Statix supports, and, when appropriate, how their types should be declared. Terminology: Sort vs. Type Throughout this reference manual, we use the term 'sort' for syntactic categories, and 'type' for all other types (such as lists, tuples, scopes, etc.). However, in practice, these terms are both used in both meanings.","title":"Terms"},{"location":"references/statix/terms/#numerals","text":"Numeric literals are literals of the form [0-9]+ . Negative literals are not supported directly. All integer literals have the built-in type int .","title":"Numerals"},{"location":"references/statix/terms/#strings","text":"String literals are arbitrary, single-line sequences of characters enclosed in double quotes. String literals may not contain unescaped backslashes, double quotes, or tabs. Double quotes and backslashes can be used in a string literal by prefixing them with another backslashes ( \\\" and \\\\ , respectively), while tabs, newlines and carriage returns can be encoded using respectively \\t , \\n and \\r . Otherwise, no escaping is required. String literals have the built-in type string .","title":"Strings"},{"location":"references/statix/terms/#identifiers","text":"Variables are identifiers of values of the following form: [a-zA-Z] [a-zA-Z0-9\\_]* [\\']* . With respect to type-checking, variables can be handled in two ways. When a variable occurs in the head of a rule , it is implicitly brought into scope with the type inferred from the rule type. Otherwise, it is required that the variable is introduced earlier, with the correct type. Apart from introduction in rule heads, variables can be introduced by existential constraints . In that case, the type of the variable is derived from its usage. Rule patterns may be non-linear (containing multiple occurrences of a variable), variables in an existential must be unique. That is, the constraint {x x} ... will give a type error. Shadowing variables is allowed, but discouraged by a warning. Substitution and Unification Although the Statix language does not distinguish between variables in rule heads and existentials, the solver treats those quite differently. On rule application, occurrences of variables from rule heads in the body of the rule are substituted by the actual arguments. For example, in a specification containing the rule rule ( x ) : - x == (). , simplifying the constraint rule(()) will result in the residual constraint () == () , which trivially holds. However, when simplifying the constraint {x} x == () , simplifying will generate a fresh unification variable (say ?x-1 ), and substitute that in the constraint, yielding ?x-1 == () . Solving that will create a new mapping ?x-1 |-> () in the unifier of the solver result. ```","title":"Identifiers"},{"location":"references/statix/terms/#wildcards","text":"Wildcards are represented as _ , and denote variables without identity. Every occurrence of a wildcard is interpreted as a new variable. Because wildcards cannot reference each other, it is not required that the types of multiple wildcard occurrences coincide.","title":"Wildcards"},{"location":"references/statix/terms/#composite-terms","text":"Composite terms can be build using constructor applications : $ ConsId ({ $ Term \",\" } * ) Here a term with constructor $ConsId and some term arguments is built. Composite terms must adhere to a signature. A signature describes which term compositions are valid, and must be declared in a signature section: signature sorts $ SortID * constructors $ ConsId : { $ Type \"*\" } + -> $ SortID $ ConsId : $ SortID First, the syntactic categories (which closely correspond to type identifiers in other languages) must be declared in a sorts subsection. Then, the constructor symbols can be declared in a constructors section. For each constructor, the types of the arguments and its sort should be provided. For nullary constructors (constructors without arguments), the arrow preceding the sort should be omitted. When a composite term is built, it is validated that all arguments match the type declaration from the signature. The type of the whole composite term is equal to the sort of the constructor.","title":"Composite terms"},{"location":"references/statix/terms/#tuples","text":"A built-in composite data construction is tuples : ({ $ Term \",\" } * ) Tuples have a statically fixed length, but the types of the arguments may differ. The type of the tuple expression is just the product of its arguments. The arity of a tuple may be anything except one, because unary tuples cause syntactic ambiguities with bracketed expressions.","title":"Tuples"},{"location":"references/statix/terms/#lists","text":"Another built-in composite data construction is lists : [ { $ Term \",\" } *] Lists are created by comma-separating terms, enclosing them in square brackets. All terms should have the same type. Given that the type of the terms is T , the type of the list expression will be list(T) . Alternatively, lists can have a variable tail: [ { $ Term \",\" } * | $ Term ] In this syntax, the tail of the list is another term. This term should have type list(T) , where T is again the type of the first terms.","title":"Lists"},{"location":"references/statix/terms/#name-ascription","text":"It is possible to assign names to terms by prefixing the term with a variable name: $ Var @$ Term Note that this does not introduce a new variable with name $Var (except in a rule head , where all variables are introduced implicitly), but rather requires that a variable with corresponding name and type is already introduced. Ascribe and Equality In terms of equality constraints , the ascribe is equal to $Var == $Term . It is used to prevent the duplication of $Term .","title":"Name Ascription"},{"location":"references/statix/terms/#type-ascription","text":"Statix allows to add inline type annotations to terms as follows: $ Term : $ Type The type-checker will validate that the term actually has the specified type, but the runtime behavior in not influenced by these ascriptions. Complete Inference In general, the Statix type-checker should be able to infer all types. However, in case of a type error being reported at an incorrect position, these type ascriptions can help tracing the cause of the error.","title":"Type Ascription"},{"location":"references/statix/terms/#arithmetic-operations","text":"Arithmetic expressions can be inserted in terms as follows: # ( $ ArithExp ) Here, the type of the expression is int . For more information on arithmetic expressions, see Arithmetic Constraints Normalization In terms of existential constraints , inline arithmetic expressions have behavior equal to {v} v #= $ArithExp , where v is used at the position of the arithmetic expression. So, for example, {T} T == CONS(#(21 * 2)) is equal to {T v} v #= 21 * 2, T == CONS(v) .","title":"Arithmetic operations"},{"location":"references/statix/terms/#ast-identifier","text":"In Spoofax, all terms in an AST are assigned a unique identifier (the term index) before analysis. This term identifier can be isolated as follows: astId ( $ Term ) Here, the type of $Term can be anything, and the type of the whole term will be astId . AST Identifiers are used to assign properties .","title":"AST Identifier"},{"location":"references/statix/terms/#new","text":"Statix allows inline creation of scopes : new Statically, the new term has type scope . At runtime, this creates a fresh scope, and inserts that at the position of the new term. Normalization In terms of existential constraints , the inline new operator has behavior equal to {s} new s , where s is used at the position of the new term. So, for example, {T} T == CLASS(new) is equal to {T s} new s, T == CLASS(s) .","title":"New"},{"location":"references/statix/terms/#paths","text":"Part of a query result is the path from the resolved datum back to the scope where the query started. In order to represent paths, Statix has two built-in constructors: _PathEmpty : Unary constructor that carries a single scope. This constructor has type scope -> path . _PathStep : Ternary constructor that represents a traversed edge in a path. This constructor has type path * label * scope -> path . Label Constraints Although the labels in a _PathStep can be bound to a variable, and hence be compared with and included in other terms, no inspection, matching or comparison with label definitions is supported.","title":"Paths"},{"location":"references/statix/terms/#occurrences","text":"Warning Since Spoofax 2.5.15, namespaces and occurrences are deprecated. Statix has built-in support for namespaces. A term embedded in a particular namespace is called an occurrence . Occurrences can be written as follows: $ NamespaceId { $ SpaceTerms } $ NamespaceId { $ SpaceTerms @$ OccurrenceId } In this structure template, $SpaceTerms means a list of terms, separated by spaces. The occurrence identifier can be any term. In case the term has an AST identifier, that value will be used as the identity of the occurrence. Alternatively, the occurrence identifier can be left out: $ NamespaceId { $ SpaceTerms } The default occurrence identifier is - , which means that the occurrence has no identifier. The type of an occurrence literal is occurrence . For more information about namespaces, see the Queries section.","title":"Occurrences"},{"location":"references/statix/terms/#declaration-match","text":"Statix allows to query the current scope for declarations of a particular form: ?$ RelationId [ { $ Term \",\" } ] in $ Scope When using this expression, a functional relation $RelationId must be declared. The terms arguments must correspond to the argument of the relation, and the type of the term is the output type of the relation. For more information on querying the scope graph, see the Queries section. Declaration Match as Query In terms of regular queries , the declaration match is equal to a query with filter e , expecting a single output. E.g. T == ?var[\"x\"] in s is equal to query var filter e and { x' :- x' == \"x\" } in s |-> [(_, (_, T))] .","title":"Declaration Match"},{"location":"references/statix/tests/","text":"Tests \u00b6 Statix has its own test format. These tests can help to debug your specification and isolate issues. In this section, we explain the format and the output of such tests. Test Format \u00b6 Statix tests should reside in *.stxtest files, and look as follows: resolve $ Constraint $ Section * At the top level, the resolve keyword indicates this is a test file. After this keyword, the constraint that should be solved when executing the test is provided. Finally, any section that can be found in a regular module can be added to a test. A test can be executed using the traditional or the concurrent solver with the Spoofax > Evaluate > Evaluate Test or Spoofax > Evaluate > Evaluate Test (Concurrent) menu, respectively. Test Output \u00b6 When a test is executed, a .stxresult file with the test result will open. This file contains three sections. First, under the substitution header, a mapping from the top-level existentially quantified variables to their values is provided. Due to the normalization Statix applies, there are sometimes additional entries for wildcards and return values of functional predicates. Secondly, under analysis and scope graph , the scope graph that models the test constraint is shown. A scope graph consists of a list of scope terms, which look as follows $ ScopeName { relations { $ RelationID : $ Term + } edges { $ LabelID : $ Scope + } } That is, for each label and relation for which entries exists in a scope, a list of associated scopes/data is shown. Empty Scopes Note that no entry for an empty scope will be present. Finally, under the errors , warnings and notes headers, the appropriate messages of these types are shown. The message value is equal to the top line of a regular Statix message, but no traces are displayed. Terms in templates are formatted three levels deep.","title":"Tests"},{"location":"references/statix/tests/#tests","text":"Statix has its own test format. These tests can help to debug your specification and isolate issues. In this section, we explain the format and the output of such tests.","title":"Tests"},{"location":"references/statix/tests/#test-format","text":"Statix tests should reside in *.stxtest files, and look as follows: resolve $ Constraint $ Section * At the top level, the resolve keyword indicates this is a test file. After this keyword, the constraint that should be solved when executing the test is provided. Finally, any section that can be found in a regular module can be added to a test. A test can be executed using the traditional or the concurrent solver with the Spoofax > Evaluate > Evaluate Test or Spoofax > Evaluate > Evaluate Test (Concurrent) menu, respectively.","title":"Test Format"},{"location":"references/statix/tests/#test-output","text":"When a test is executed, a .stxresult file with the test result will open. This file contains three sections. First, under the substitution header, a mapping from the top-level existentially quantified variables to their values is provided. Due to the normalization Statix applies, there are sometimes additional entries for wildcards and return values of functional predicates. Secondly, under analysis and scope graph , the scope graph that models the test constraint is shown. A scope graph consists of a list of scope terms, which look as follows $ ScopeName { relations { $ RelationID : $ Term + } edges { $ LabelID : $ Scope + } } That is, for each label and relation for which entries exists in a scope, a list of associated scopes/data is shown. Empty Scopes Note that no entry for an empty scope will be present. Finally, under the errors , warnings and notes headers, the appropriate messages of these types are shown. The message value is equal to the top line of a regular Statix message, but no traces are displayed. Terms in templates are formatted three levels deep.","title":"Test Output"},{"location":"references/stratego/","text":"Stratego \u00b6 The Stratego language caters for the definition of program transformations. Transformations operate on the abstract syntax trees of programs. Abstract syntax trees are represented by means of first-order terms . A program is structured as a collection of modules , which may import each other. Transformations are defined by means of named rewrite rules . Rules may explicitly invoke rules. Alternatively, rules may be invoked by strategies that define how to combine rules into a more complex transformation using strategy combinators . Context-sensitive transformations can be expressed using dynamic rewrite rules . Starting with Stratego 2, terms and transformation strategies are (gradually) typed . Placeholder Convention \u00b6 In this reference manual we use placeholders to indicate the syntactic structure of language constructs. For example, a rewrite rule has the form $ Label : $ Term - > $ Term in which the $Label is the name of the rule, the first $Term the left-hand side, and the second the right-hand side of the rule. This convention should give an indication of the formal structure of a construct, without going down to the precise details of the syntax definition. As a side effect, the schema also shows the preferred indentation of language constructs where that is applicable. Not in Reference Manual \u00b6 Concrete Syntax \u00b6 By using the concrete syntax of a language, transformations can be expressed in the native syntax of the language under transformation, rather than using abstract syntax. Library \u00b6 The Stratego standard library is a collection of modules that are available with each Stratego program and on which the runtime library relies. Find automatically generated documentation at the following sites: http://releases.strategoxt.org/docs/api/libstratego-lib/stable/docs/ https://stratego.martijndwars.nl/ Source \u00b6 The sources of the Stratego implementation can be found at https://github.com/metaborg/stratego : The Stratego language implementation https://github.com/metaborg/strategoxt : The Stratego/XT ecosystem","title":"Stratego"},{"location":"references/stratego/#stratego","text":"The Stratego language caters for the definition of program transformations. Transformations operate on the abstract syntax trees of programs. Abstract syntax trees are represented by means of first-order terms . A program is structured as a collection of modules , which may import each other. Transformations are defined by means of named rewrite rules . Rules may explicitly invoke rules. Alternatively, rules may be invoked by strategies that define how to combine rules into a more complex transformation using strategy combinators . Context-sensitive transformations can be expressed using dynamic rewrite rules . Starting with Stratego 2, terms and transformation strategies are (gradually) typed .","title":"Stratego"},{"location":"references/stratego/#placeholder-convention","text":"In this reference manual we use placeholders to indicate the syntactic structure of language constructs. For example, a rewrite rule has the form $ Label : $ Term - > $ Term in which the $Label is the name of the rule, the first $Term the left-hand side, and the second the right-hand side of the rule. This convention should give an indication of the formal structure of a construct, without going down to the precise details of the syntax definition. As a side effect, the schema also shows the preferred indentation of language constructs where that is applicable.","title":"Placeholder Convention"},{"location":"references/stratego/#not-in-reference-manual","text":"","title":"Not in Reference Manual"},{"location":"references/stratego/#concrete-syntax","text":"By using the concrete syntax of a language, transformations can be expressed in the native syntax of the language under transformation, rather than using abstract syntax.","title":"Concrete Syntax"},{"location":"references/stratego/#library","text":"The Stratego standard library is a collection of modules that are available with each Stratego program and on which the runtime library relies. Find automatically generated documentation at the following sites: http://releases.strategoxt.org/docs/api/libstratego-lib/stable/docs/ https://stratego.martijndwars.nl/","title":"Library"},{"location":"references/stratego/#source","text":"The sources of the Stratego implementation can be found at https://github.com/metaborg/stratego : The Stratego language implementation https://github.com/metaborg/strategoxt : The Stratego/XT ecosystem","title":"Source"},{"location":"references/stratego/dynamic-rules/","text":"Dynamic Rules \u00b6 Plain rewrite rules are context-free, i.e. do not take their context into account. Context-sensitive transformations can be defined by passing context information using additional arguments to rules and strategies. Alternatively, Stratego provides linguistic support to dynamically define rewrite rules based on context information 1 . Defining Dynamic Rules \u00b6 rules ( $ Id : $ Rule ... ) A dynamic rule definition is a regular (conditional) rewrite rule that is defined as part of a strategy rather than at top-level. The difference is that any variables that are bound in the context of the rule, take their binding from the context, rather then being universally quantified. Thus, a dynamic rule instance can be thought of as having the context variables replaced by the corresponding terms from the context. Example. The following strategy DefineInlineCall defines the dynamic rule InlineCall : DefineInlineCall = ? FunDef ( f , args , e ) ; rules ( InlineCall : Call ( f , es ) - > Let ( dec * , e ) where < zip ( \\ ( x , e ) - > VarDec ( x , e ) \\ )> ( args , es ) => dec * ) The variables f , args , and e in the dynamic rule are bound in context, while the variables es and dec* are universally quantified. The variables x and e in the embedded lambda rule are local to that rule. Thus, the application < DefineInlineCall > FunDef ( \"inc\" , [ \"x\" ], Add ( Var ( \"x\" ), Int ( \"1\" ))) can be thought of to give rise to the definition InlineCall : Call ( \"inc\" , es ) - > Let ( dec * , Add ( Var ( \"x\" ), Int ( \"1\" ))) where < zip ( \\ ( x , e ) - > VarDec ( x , e ) \\ )> ([ \"x\" ], es ) => dec * Invoking Dynamic Rules \u00b6 When $Id is defined as a dynamic rule it can be invoke like a regular rule or strategies. The invocation only succeeds when applied to a term that coincide with the left-hand side pattern variables bindings inherited from the context. Thus, in the example above InlineCall can be called to invoke a previously defined dynamic rule. For example, the following are calls to DefineInlineCall and InlineCall < DefineInlineCall > FunDef ( \"inc\" , [ \"x\" ], Add ( Var ( \"x\" ), Int ( \"1\" ))) < InlineCall > Call ( \"inc\" , [ Mul ( Var ( \"y\" ), Int ( \"3\" ))]) => Let ([ VarDec ( \"x\" , Mul ( Var ( \"y\" ), Int ( \"3\" )))], Add ( Var ( \"x\" ), Int ( \"1\" ))) < InlineCall > Call ( \"foo\" , []) // fails Note that the application to Call(\"foo\", []) fails since it does not match the dynamically defined rule. Parameterized Dynamic Rules \u00b6 Dynamic rules can be parameterized like regular rewrite rules and strategies. Multiple Definitions \u00b6 Dynamic rules can be defined for multiple contexts simultaneously. For example, the following applications of DefineInlineCall < DefineInlineCall > FunDef ( \"inc\" , [ \"x\" ], Add ( Var ( \"x\" ), Int ( \"1\" ))) < DefineInlineCall > FunDef ( \"twice\" , [ \"y\" ], Mul ( Var ( \"x\" ), Int ( \"2\" ))) can be thought of defining multiple top-level rewrite rules InlineCall : Call ( \"inc\" , es ) - > Let ( dec * , Add ( Var ( \"x\" ), Int ( \"1\" ))) where < zip ( \\ ( x , e ) - > VarDec ( x , e ) \\ )> ([ \"x\" ], es ) => dec * InlineCall : Call ( \"twice\" , es ) - > Let ( dec * , Mul ( Var ( \"y\" ), Int ( \"2\" ))) where < zip ( \\ ( x , e ) - > VarDec ( x , e ) \\ )> ([ \"y\" ], es ) => dec * Overriding Dynamic Rules \u00b6 A definition of a dynamic rule with the same left-hand side as a previous definition, overrides that previous definition. Thus, if after the applications of DefineInlineCall above, we apply < DefineInlineCall > FunDef ( \"twice\" , [ \"z\" ], Add ( Var ( \"z\" ), Var ( \"z\" ))) Then the dynamic rule for twice above is undefined , and instead the rule InlineCall : Call ( \"twice\" , es ) - > Let ( dec * , Add ( Var ( \"z\" ), Var ( \"z\" ))) where < zip ( \\ ( x , e ) - > VarDec ( x , e ) \\ )> ([ \"z\" ], es ) => dec * is added to the collection of rules. Dynamic Rule Scope \u00b6 It is possible to limit the scope in which dynamic rule definitions are available. The dynamic rule scope {| $Id ... : $Strategy |} limits the availability of dynamic rules named $Id .. defined within the brackets to that scope. After exiting the scope, the state of the dynamic rule definitions before the scope is restored. For example, the following strategy defines inlining rules that are only available during the visit of the body of the Let : inline : Let ( dec1 * , e1 ) - > Let ( dec2 * , e2 ) with < inline > dec1 * => dec2 * with {| InlineCall : < map ( try ( DefineInlineCall ))> dec2 * ; < inline > e1 => e2 |} Application of this strategy to the program term Let ([ FunDef ( \"inc\" , [ .. ], .. ) , FunDef ( \"twice\" , [ .. ], .. )] , Add ( Let ([ FunDef ( \"twice\" , [ .. ], [ .. ])] , Call ( \"twice\" , [ .. ])) // inline second def of twice , Call ( \"twice\" , [ .. ]) // inline first def of twice ) ) will result in locally overriding the first dynamic rule for \"twice\" , but undoing that override at the end of the dynamic rule scope, such that it is available again at the second call to \"twice\" . While dynamic rule scopes can deal with lexical scope systems, the preferred way to deal with scope in programming languages is to perform name (and type) analysis using the Statix meta-language and perform a uniquify transformation to guarantee unique names. Multiple Right-Hand Sides \u00b6 In order to collect multiple ways to rewrite a term use rules( $Id :+ $Rule) . For example, the following is a small API for for emitting nodes in a control-flow graph consisting of blocks. add-cfg-node :: CBlock - > CBlock all -cfg-nodes :: List ( CBlock ) - > List ( CBlock ) add-cfg-node = ? block ; rules ( CFGNode : + _ - > block ) all -cfg-nodes = < bagof-CFGNode < + ! []>() The bagof-$Id strategy is generated automatically and produces all right-hand sides corresponding to a left-hand side. Other Dynamic Rule Extensions \u00b6 The papers by Olmos and Visser 2 and Bravenboer et. al 1 describe more advanced features of dynamic rules, primarily inspired by data-flow transformations. For defining data-flow analyses, Spoofax now provides the FlowSpec meta-language. References \u00b6 Martin Bravenboer, Arthur van Dam, Karina Olmos, and Eelco Visser. Program transformation with scoped dynamic rewrite rules. Fundamenta Informaticae , 69 \\(1\\-2\\) :123\u2013178, 2006. URL: https://content.iospress.com/articles/fundamenta-informaticae/fi69-1-2-06 . \u21a9 \u21a9 Karina Olmos and Eelco Visser. Composing source-to-source data-flow transformations with rewriting strategies and dependent dynamic rewrite rules. In Rastislav Bod\u00edk, editor, Compiler Construction, 14 th International Conference, CC 2005, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2005, Edinburgh, UK, April 4-8, 2005, Proceedings , volume 3443 of Lecture Notes in Computer Science, 204\u2013220. Springer, 2005. URL: https://doi.org/10.1007/978-3-540-31985-6_14 , doi:10.1007/978-3-540-31985-6_14 . \u21a9","title":"Dynamic Rules"},{"location":"references/stratego/dynamic-rules/#dynamic-rules","text":"Plain rewrite rules are context-free, i.e. do not take their context into account. Context-sensitive transformations can be defined by passing context information using additional arguments to rules and strategies. Alternatively, Stratego provides linguistic support to dynamically define rewrite rules based on context information 1 .","title":"Dynamic Rules"},{"location":"references/stratego/dynamic-rules/#defining-dynamic-rules","text":"rules ( $ Id : $ Rule ... ) A dynamic rule definition is a regular (conditional) rewrite rule that is defined as part of a strategy rather than at top-level. The difference is that any variables that are bound in the context of the rule, take their binding from the context, rather then being universally quantified. Thus, a dynamic rule instance can be thought of as having the context variables replaced by the corresponding terms from the context. Example. The following strategy DefineInlineCall defines the dynamic rule InlineCall : DefineInlineCall = ? FunDef ( f , args , e ) ; rules ( InlineCall : Call ( f , es ) - > Let ( dec * , e ) where < zip ( \\ ( x , e ) - > VarDec ( x , e ) \\ )> ( args , es ) => dec * ) The variables f , args , and e in the dynamic rule are bound in context, while the variables es and dec* are universally quantified. The variables x and e in the embedded lambda rule are local to that rule. Thus, the application < DefineInlineCall > FunDef ( \"inc\" , [ \"x\" ], Add ( Var ( \"x\" ), Int ( \"1\" ))) can be thought of to give rise to the definition InlineCall : Call ( \"inc\" , es ) - > Let ( dec * , Add ( Var ( \"x\" ), Int ( \"1\" ))) where < zip ( \\ ( x , e ) - > VarDec ( x , e ) \\ )> ([ \"x\" ], es ) => dec *","title":"Defining Dynamic Rules"},{"location":"references/stratego/dynamic-rules/#invoking-dynamic-rules","text":"When $Id is defined as a dynamic rule it can be invoke like a regular rule or strategies. The invocation only succeeds when applied to a term that coincide with the left-hand side pattern variables bindings inherited from the context. Thus, in the example above InlineCall can be called to invoke a previously defined dynamic rule. For example, the following are calls to DefineInlineCall and InlineCall < DefineInlineCall > FunDef ( \"inc\" , [ \"x\" ], Add ( Var ( \"x\" ), Int ( \"1\" ))) < InlineCall > Call ( \"inc\" , [ Mul ( Var ( \"y\" ), Int ( \"3\" ))]) => Let ([ VarDec ( \"x\" , Mul ( Var ( \"y\" ), Int ( \"3\" )))], Add ( Var ( \"x\" ), Int ( \"1\" ))) < InlineCall > Call ( \"foo\" , []) // fails Note that the application to Call(\"foo\", []) fails since it does not match the dynamically defined rule.","title":"Invoking Dynamic Rules"},{"location":"references/stratego/dynamic-rules/#parameterized-dynamic-rules","text":"Dynamic rules can be parameterized like regular rewrite rules and strategies.","title":"Parameterized Dynamic Rules"},{"location":"references/stratego/dynamic-rules/#multiple-definitions","text":"Dynamic rules can be defined for multiple contexts simultaneously. For example, the following applications of DefineInlineCall < DefineInlineCall > FunDef ( \"inc\" , [ \"x\" ], Add ( Var ( \"x\" ), Int ( \"1\" ))) < DefineInlineCall > FunDef ( \"twice\" , [ \"y\" ], Mul ( Var ( \"x\" ), Int ( \"2\" ))) can be thought of defining multiple top-level rewrite rules InlineCall : Call ( \"inc\" , es ) - > Let ( dec * , Add ( Var ( \"x\" ), Int ( \"1\" ))) where < zip ( \\ ( x , e ) - > VarDec ( x , e ) \\ )> ([ \"x\" ], es ) => dec * InlineCall : Call ( \"twice\" , es ) - > Let ( dec * , Mul ( Var ( \"y\" ), Int ( \"2\" ))) where < zip ( \\ ( x , e ) - > VarDec ( x , e ) \\ )> ([ \"y\" ], es ) => dec *","title":"Multiple Definitions"},{"location":"references/stratego/dynamic-rules/#overriding-dynamic-rules","text":"A definition of a dynamic rule with the same left-hand side as a previous definition, overrides that previous definition. Thus, if after the applications of DefineInlineCall above, we apply < DefineInlineCall > FunDef ( \"twice\" , [ \"z\" ], Add ( Var ( \"z\" ), Var ( \"z\" ))) Then the dynamic rule for twice above is undefined , and instead the rule InlineCall : Call ( \"twice\" , es ) - > Let ( dec * , Add ( Var ( \"z\" ), Var ( \"z\" ))) where < zip ( \\ ( x , e ) - > VarDec ( x , e ) \\ )> ([ \"z\" ], es ) => dec * is added to the collection of rules.","title":"Overriding Dynamic Rules"},{"location":"references/stratego/dynamic-rules/#dynamic-rule-scope","text":"It is possible to limit the scope in which dynamic rule definitions are available. The dynamic rule scope {| $Id ... : $Strategy |} limits the availability of dynamic rules named $Id .. defined within the brackets to that scope. After exiting the scope, the state of the dynamic rule definitions before the scope is restored. For example, the following strategy defines inlining rules that are only available during the visit of the body of the Let : inline : Let ( dec1 * , e1 ) - > Let ( dec2 * , e2 ) with < inline > dec1 * => dec2 * with {| InlineCall : < map ( try ( DefineInlineCall ))> dec2 * ; < inline > e1 => e2 |} Application of this strategy to the program term Let ([ FunDef ( \"inc\" , [ .. ], .. ) , FunDef ( \"twice\" , [ .. ], .. )] , Add ( Let ([ FunDef ( \"twice\" , [ .. ], [ .. ])] , Call ( \"twice\" , [ .. ])) // inline second def of twice , Call ( \"twice\" , [ .. ]) // inline first def of twice ) ) will result in locally overriding the first dynamic rule for \"twice\" , but undoing that override at the end of the dynamic rule scope, such that it is available again at the second call to \"twice\" . While dynamic rule scopes can deal with lexical scope systems, the preferred way to deal with scope in programming languages is to perform name (and type) analysis using the Statix meta-language and perform a uniquify transformation to guarantee unique names.","title":"Dynamic Rule Scope"},{"location":"references/stratego/dynamic-rules/#multiple-right-hand-sides","text":"In order to collect multiple ways to rewrite a term use rules( $Id :+ $Rule) . For example, the following is a small API for for emitting nodes in a control-flow graph consisting of blocks. add-cfg-node :: CBlock - > CBlock all -cfg-nodes :: List ( CBlock ) - > List ( CBlock ) add-cfg-node = ? block ; rules ( CFGNode : + _ - > block ) all -cfg-nodes = < bagof-CFGNode < + ! []>() The bagof-$Id strategy is generated automatically and produces all right-hand sides corresponding to a left-hand side.","title":"Multiple Right-Hand Sides"},{"location":"references/stratego/dynamic-rules/#other-dynamic-rule-extensions","text":"The papers by Olmos and Visser 2 and Bravenboer et. al 1 describe more advanced features of dynamic rules, primarily inspired by data-flow transformations. For defining data-flow analyses, Spoofax now provides the FlowSpec meta-language.","title":"Other Dynamic Rule Extensions"},{"location":"references/stratego/dynamic-rules/#references","text":"Martin Bravenboer, Arthur van Dam, Karina Olmos, and Eelco Visser. Program transformation with scoped dynamic rewrite rules. Fundamenta Informaticae , 69 \\(1\\-2\\) :123\u2013178, 2006. URL: https://content.iospress.com/articles/fundamenta-informaticae/fi69-1-2-06 . \u21a9 \u21a9 Karina Olmos and Eelco Visser. Composing source-to-source data-flow transformations with rewriting strategies and dependent dynamic rewrite rules. In Rastislav Bod\u00edk, editor, Compiler Construction, 14 th International Conference, CC 2005, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2005, Edinburgh, UK, April 4-8, 2005, Proceedings , volume 3443 of Lecture Notes in Computer Science, 204\u2013220. Springer, 2005. URL: https://doi.org/10.1007/978-3-540-31985-6_14 , doi:10.1007/978-3-540-31985-6_14 . \u21a9","title":"References"},{"location":"references/stratego/lexical/","text":"Lexical \u00b6 Identifiers \u00b6 Identifiers used as names of constructors and transformations have the form ID = [ a-zA-Z ][ a-zA-Z0-9 \\ - \\ _ ] * In particular, hyphens can be part of identifiers. Identifiers cannot be followed by identifiers or keywords without intervening whitespace. Reserved Words \u00b6 Todo provide list of reserved words Module Names \u00b6 Module names can be sequences of identifiers separated by / . Integers \u00b6 INT = [ 0 -9 ] + Check syntax of integers Whitespace \u00b6 Spaces, tabs, and newlines are whitespace and can occur between any two tokens. Comments \u00b6 Comments follow the C/Java tradition. That is, the language supports single line comments after // // a single line comment and multi-line comments between /* and */ /* a multi-line comment can be spread over multiple lines */ Comments can occur anywhere. Multi-line comments cannot be nested currently. Todo but this should be changed so that multi-line comments can be nested","title":"Lexical"},{"location":"references/stratego/lexical/#lexical","text":"","title":"Lexical"},{"location":"references/stratego/lexical/#identifiers","text":"Identifiers used as names of constructors and transformations have the form ID = [ a-zA-Z ][ a-zA-Z0-9 \\ - \\ _ ] * In particular, hyphens can be part of identifiers. Identifiers cannot be followed by identifiers or keywords without intervening whitespace.","title":"Identifiers"},{"location":"references/stratego/lexical/#reserved-words","text":"Todo provide list of reserved words","title":"Reserved Words"},{"location":"references/stratego/lexical/#module-names","text":"Module names can be sequences of identifiers separated by / .","title":"Module Names"},{"location":"references/stratego/lexical/#integers","text":"INT = [ 0 -9 ] + Check syntax of integers","title":"Integers"},{"location":"references/stratego/lexical/#whitespace","text":"Spaces, tabs, and newlines are whitespace and can occur between any two tokens.","title":"Whitespace"},{"location":"references/stratego/lexical/#comments","text":"Comments follow the C/Java tradition. That is, the language supports single line comments after // // a single line comment and multi-line comments between /* and */ /* a multi-line comment can be spread over multiple lines */ Comments can occur anywhere. Multi-line comments cannot be nested currently. Todo but this should be changed so that multi-line comments can be nested","title":"Comments"},{"location":"references/stratego/modules/","text":"Modules \u00b6 A Stratego program is organized as a collection of modules, which are imported from a main module. Module Structure \u00b6 module $ ModuleName $ Imports * $ Section * A module starts with a module header followed by a list of imports . The name of a module in the header and imports should correspond to the file name, relative to a 'root' directory. The rest of a module consists of signature , rules , and strategies sections, in any order, and possibly repeated. File Name and File Extension \u00b6 A module coincides with the file it resides in. It is not possible to define more than one module in a file, which precludes nested modules. The name of a module coincides with the file name, which should be fully qualified relative to a root directory. A Stratego is a file with the extension .str2 for Stratego 2. Modules for the Stratego 1 version of the language have extension .str . The file extension does not feature in the module names used in the language. The following is example module header: module compilation / translation imports desugaring / desugar Module Names \u00b6 Module names can be hierarchical. For example, consider the following directory structure - trans - compilation - optimization.str2 - translation.str2 - desugaring - desugar.str2 A declaration of or reference to a module uses its fully qualified name, with / to indicate the directory structure, relative to a 'root' directory. For example, if trans is declared as a root , then the module names for the modules above are - compilation/optimization - compilation/translation - desugaring/desugar Imports \u00b6 imports $ ModuleName + A module should import all other modules from which it uses definitions. Imports are non-transitive and may be mutually recursive. Modules can extend rule and strategy definitions from other modules. This allows the modular extension of a language. When imported, all definitions in a module are visible. There are currently no mechanisms for hiding definitions. An imports can list multiple modules. The form imports A B is equivalent to imports A imports B Signatures \u00b6 A signature section introduces sorts, constructors, and overlays. signature sorts $ Sort * constructors $ ConstructorDef * overlays $ OverlayDef * Rules and Strategies \u00b6 Rule definitions and strategy definitions introduce named transformations. rules $ RuleDef * strategies $ StrategyDef * The rules and strategies section headers are indicative only; rule and strategy definitions can actually be mixed. Libraries \u00b6 A Stratego library is a closed collection of modules. A library can be pre-compiled since client programs may not extend its definitions. A library is used by importing a collection of external definitions of the signatures of constructors and transformations it defines. Even if definitions in a library are not included in a libraries external definition, they cannot be redefined, as that produces link errors. Source Inclusion \u00b6 Todo Concrete Syntax \u00b6 When using concrete syntax in a module, a .meta file accompanying the module indicates the parse table to use.","title":"Modules"},{"location":"references/stratego/modules/#modules","text":"A Stratego program is organized as a collection of modules, which are imported from a main module.","title":"Modules"},{"location":"references/stratego/modules/#module-structure","text":"module $ ModuleName $ Imports * $ Section * A module starts with a module header followed by a list of imports . The name of a module in the header and imports should correspond to the file name, relative to a 'root' directory. The rest of a module consists of signature , rules , and strategies sections, in any order, and possibly repeated.","title":"Module Structure"},{"location":"references/stratego/modules/#file-name-and-file-extension","text":"A module coincides with the file it resides in. It is not possible to define more than one module in a file, which precludes nested modules. The name of a module coincides with the file name, which should be fully qualified relative to a root directory. A Stratego is a file with the extension .str2 for Stratego 2. Modules for the Stratego 1 version of the language have extension .str . The file extension does not feature in the module names used in the language. The following is example module header: module compilation / translation imports desugaring / desugar","title":"File Name and File Extension"},{"location":"references/stratego/modules/#module-names","text":"Module names can be hierarchical. For example, consider the following directory structure - trans - compilation - optimization.str2 - translation.str2 - desugaring - desugar.str2 A declaration of or reference to a module uses its fully qualified name, with / to indicate the directory structure, relative to a 'root' directory. For example, if trans is declared as a root , then the module names for the modules above are - compilation/optimization - compilation/translation - desugaring/desugar","title":"Module Names"},{"location":"references/stratego/modules/#imports","text":"imports $ ModuleName + A module should import all other modules from which it uses definitions. Imports are non-transitive and may be mutually recursive. Modules can extend rule and strategy definitions from other modules. This allows the modular extension of a language. When imported, all definitions in a module are visible. There are currently no mechanisms for hiding definitions. An imports can list multiple modules. The form imports A B is equivalent to imports A imports B","title":"Imports"},{"location":"references/stratego/modules/#signatures","text":"A signature section introduces sorts, constructors, and overlays. signature sorts $ Sort * constructors $ ConstructorDef * overlays $ OverlayDef *","title":"Signatures"},{"location":"references/stratego/modules/#rules-and-strategies","text":"Rule definitions and strategy definitions introduce named transformations. rules $ RuleDef * strategies $ StrategyDef * The rules and strategies section headers are indicative only; rule and strategy definitions can actually be mixed.","title":"Rules and Strategies"},{"location":"references/stratego/modules/#libraries","text":"A Stratego library is a closed collection of modules. A library can be pre-compiled since client programs may not extend its definitions. A library is used by importing a collection of external definitions of the signatures of constructors and transformations it defines. Even if definitions in a library are not included in a libraries external definition, they cannot be redefined, as that produces link errors.","title":"Libraries"},{"location":"references/stratego/modules/#source-inclusion","text":"Todo","title":"Source Inclusion"},{"location":"references/stratego/modules/#concrete-syntax","text":"When using concrete syntax in a module, a .meta file accompanying the module indicates the parse table to use.","title":"Concrete Syntax"},{"location":"references/stratego/rewrite-rules/","text":"Rewrite Rules \u00b6 $ Id ( $ StrategyArg , ... | $ TermArg , ... ) : $ Term - > $ Term $ Condition * A rewrite rule has a name, zero or more strategy arguments, zero or more term arguments, a left-hand side term pattern, a right-hand side term pattern, and zero or more conditions. A rewrite rule application $Id($StrategExp, ... | $Term) to a subject term binds the strategy and term arguments and matches the term pattern in the left-hand side to the term. If the pattern match succeeds, the conditions are applied in turn to the subject term, accumulating bindings to term variables. When all conditions succeed, the right-hand side term pattern is instantiated with the accumulated variable bindings. When the pattern match to the left-hand side or one of the conditions fails, the rule fails. Where Condition \u00b6 where $ StrategyExp A where condition performs a strategy expression in the context of the rule arguments, the left-hand side bindings, and the previous conditions, possibly binding term variables. If the strategy expression fails, the enclosing rule fails. Failure of a where clause is expected. The strategy expression is expected to be discriminating and only succeed in those cases that the rule should be applied. With Condition \u00b6 with $ StrategyExp A with condition performs a strategy expression in the context of the rule arguments, the left-hand side bindings, and the previous conditions, possibly binding term variables. A with condition expresses the expectation that the strategy expression succeeds in all cases. When a with condition fails, this is an indication of a programming error, and the enclosing rule throws a fatal exception, and the program terminates with a stack trace. Simple Rewrite Rules \u00b6 $ Id : $ Term - > $ Term $ Condition * A simple (unparameterized) rewrite rule consists of a name that identifies the rule, a left-hand side term pattern, a right-hand side term pattern, and zero or more conditions. Example DeMorgan : Not ( And ( e1 , e2 )) - > Or ( Not ( e1 ), Not ( e2 )) Rules with the Same Name \u00b6 Multiple rewrite rules may have the same name. When a (simple) rewrite rule fails to apply to a term, the next rule with the same name is tried. For examples, the following rules define desugarings of expressions. rules desugar-exp :: Exp - > Exp desugar-exp : Seq ([], e ) - > e desugar-exp : Seq ([ e ], Unit ()) - > e desugar-exp : Seq ([ e1 , e2 | e * ], e3 ) - > Seq ([ e1 ], Seq ([ e2 | e * ], e3 )) desugar-exp : Seq ([ Seq ( e1 * , e1 ) | e2 * ], e2 ) - > Seq ([ e1 * , e1 | e2 * ], e2 ) desugar-exp : Let ( dec * , [ e1 , e2 | e * ]) - > Let ( dec * , [ Seq ([ e1 , e2 | e * ], Unit ())]) When one rule fails to apply, the next rule is tried. When the left-hand sides are non-overlapping, the order of the rules does not matter. In case of overlap, the rules are tried in textual order. When overlapping rules are defined in separate modules, the order is undefined. Note Consider specificity ordering in the future. Parameterized Rewrite Rules \u00b6 Rewrite rules can be parameterized with transformation strategies and with terms. Example. The following rules define reversal of a list with an accumulator: rules reverse :: List ( a ) - > List ( a ) reverse : xs - > < reverse-acc (|[])> xs reverse-acc (| List ( a )) :: List ( a ) - > List ( a ) reverse-acc (| xs ) : [] - > xs reverse-acc (| xs ) : [ y | ys ] - > < reverse-acc (|[ y | xs ])> ys When leaving out the term parameters, the bar can be left out $ Id ( $ StrategyArg ) : $ Term - > $ Term $ Condition * Example. The map(s) strategy applies transformation s to each element of a list: map ( a - > b ) :: List ( a ) - > List ( b ) map ( s ) : [] - > [] map ( s ) : [ hd | tl ] - > [< s > hd | < map ( s )> tl ] Note In the absence of a type system, the distinction between strategy arguments and term arguments was made based on the syntactic distinction. In a future version of the language, this syntactic distinction may no longer be necessary based on types. Desugaring \u00b6 A conditional rewrite rule can be desugared to a strategy definition using basic strategy combinators . A simple rewrite rule succeeds if the match of the left-hand side succeeds. Sometimes it is useful to place additional requirements on the application of a rule, or to compute some value for use in the right-hand side of the rule. This can be achieved with conditional rewrite rules. A conditional rule L: p1 -> p2 where s is a simple rule extended with an additional computation s which should succeed in order for the rule to apply. The condition can be used to test properties of terms in the left-hand side, or to compute terms to be used in the right-hand side. The latter is done by binding such new terms to variables used in the right-hand side. For example, the EvalPlus rule in the following session uses a condition to compute the sum of i and j : EvalPlus : Plus ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k < EvalPlus > Plus ( Int ( \"14\" ), Int ( \"3\" )) => Int ( \"17\" ) A conditional rule can be desugared similarly to an unconditional rule. That is, a conditional rule of the form L : p1 - > p2 where s is syntactic sugar for L = ? p1 ; where ( s ); ! p2 Thus, after the match with p1 succeeds the strategy s is applied to the subject term. Only if the application of s succeeds, is the right-hand side p2 built. Note that since s is applied within a where, the build !p2 is applied to the original subject term; only variable bindings computed within s can be used in p2 . As an example, consider the following constant folding rule, which reduces an addition of two integer constants to the constant obtained by computing the addition. EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k The addition is computed by applying the primitive strategy addS to the pair of integers (i,j) and matching the result against the variable k , which is then used in the right-hand side. This rule is desugared to EvalPlus = ? Add ( Int ( i ), Int ( j )); where ( ! ( i , j ); addS ; ? k ); ! Int ( k )","title":"Rewrite Rules"},{"location":"references/stratego/rewrite-rules/#rewrite-rules","text":"$ Id ( $ StrategyArg , ... | $ TermArg , ... ) : $ Term - > $ Term $ Condition * A rewrite rule has a name, zero or more strategy arguments, zero or more term arguments, a left-hand side term pattern, a right-hand side term pattern, and zero or more conditions. A rewrite rule application $Id($StrategExp, ... | $Term) to a subject term binds the strategy and term arguments and matches the term pattern in the left-hand side to the term. If the pattern match succeeds, the conditions are applied in turn to the subject term, accumulating bindings to term variables. When all conditions succeed, the right-hand side term pattern is instantiated with the accumulated variable bindings. When the pattern match to the left-hand side or one of the conditions fails, the rule fails.","title":"Rewrite Rules"},{"location":"references/stratego/rewrite-rules/#where-condition","text":"where $ StrategyExp A where condition performs a strategy expression in the context of the rule arguments, the left-hand side bindings, and the previous conditions, possibly binding term variables. If the strategy expression fails, the enclosing rule fails. Failure of a where clause is expected. The strategy expression is expected to be discriminating and only succeed in those cases that the rule should be applied.","title":"Where Condition"},{"location":"references/stratego/rewrite-rules/#with-condition","text":"with $ StrategyExp A with condition performs a strategy expression in the context of the rule arguments, the left-hand side bindings, and the previous conditions, possibly binding term variables. A with condition expresses the expectation that the strategy expression succeeds in all cases. When a with condition fails, this is an indication of a programming error, and the enclosing rule throws a fatal exception, and the program terminates with a stack trace.","title":"With Condition"},{"location":"references/stratego/rewrite-rules/#simple-rewrite-rules","text":"$ Id : $ Term - > $ Term $ Condition * A simple (unparameterized) rewrite rule consists of a name that identifies the rule, a left-hand side term pattern, a right-hand side term pattern, and zero or more conditions. Example DeMorgan : Not ( And ( e1 , e2 )) - > Or ( Not ( e1 ), Not ( e2 ))","title":"Simple Rewrite Rules"},{"location":"references/stratego/rewrite-rules/#rules-with-the-same-name","text":"Multiple rewrite rules may have the same name. When a (simple) rewrite rule fails to apply to a term, the next rule with the same name is tried. For examples, the following rules define desugarings of expressions. rules desugar-exp :: Exp - > Exp desugar-exp : Seq ([], e ) - > e desugar-exp : Seq ([ e ], Unit ()) - > e desugar-exp : Seq ([ e1 , e2 | e * ], e3 ) - > Seq ([ e1 ], Seq ([ e2 | e * ], e3 )) desugar-exp : Seq ([ Seq ( e1 * , e1 ) | e2 * ], e2 ) - > Seq ([ e1 * , e1 | e2 * ], e2 ) desugar-exp : Let ( dec * , [ e1 , e2 | e * ]) - > Let ( dec * , [ Seq ([ e1 , e2 | e * ], Unit ())]) When one rule fails to apply, the next rule is tried. When the left-hand sides are non-overlapping, the order of the rules does not matter. In case of overlap, the rules are tried in textual order. When overlapping rules are defined in separate modules, the order is undefined. Note Consider specificity ordering in the future.","title":"Rules with the Same Name"},{"location":"references/stratego/rewrite-rules/#parameterized-rewrite-rules","text":"Rewrite rules can be parameterized with transformation strategies and with terms. Example. The following rules define reversal of a list with an accumulator: rules reverse :: List ( a ) - > List ( a ) reverse : xs - > < reverse-acc (|[])> xs reverse-acc (| List ( a )) :: List ( a ) - > List ( a ) reverse-acc (| xs ) : [] - > xs reverse-acc (| xs ) : [ y | ys ] - > < reverse-acc (|[ y | xs ])> ys When leaving out the term parameters, the bar can be left out $ Id ( $ StrategyArg ) : $ Term - > $ Term $ Condition * Example. The map(s) strategy applies transformation s to each element of a list: map ( a - > b ) :: List ( a ) - > List ( b ) map ( s ) : [] - > [] map ( s ) : [ hd | tl ] - > [< s > hd | < map ( s )> tl ] Note In the absence of a type system, the distinction between strategy arguments and term arguments was made based on the syntactic distinction. In a future version of the language, this syntactic distinction may no longer be necessary based on types.","title":"Parameterized Rewrite Rules"},{"location":"references/stratego/rewrite-rules/#desugaring","text":"A conditional rewrite rule can be desugared to a strategy definition using basic strategy combinators . A simple rewrite rule succeeds if the match of the left-hand side succeeds. Sometimes it is useful to place additional requirements on the application of a rule, or to compute some value for use in the right-hand side of the rule. This can be achieved with conditional rewrite rules. A conditional rule L: p1 -> p2 where s is a simple rule extended with an additional computation s which should succeed in order for the rule to apply. The condition can be used to test properties of terms in the left-hand side, or to compute terms to be used in the right-hand side. The latter is done by binding such new terms to variables used in the right-hand side. For example, the EvalPlus rule in the following session uses a condition to compute the sum of i and j : EvalPlus : Plus ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k < EvalPlus > Plus ( Int ( \"14\" ), Int ( \"3\" )) => Int ( \"17\" ) A conditional rule can be desugared similarly to an unconditional rule. That is, a conditional rule of the form L : p1 - > p2 where s is syntactic sugar for L = ? p1 ; where ( s ); ! p2 Thus, after the match with p1 succeeds the strategy s is applied to the subject term. Only if the application of s succeeds, is the right-hand side p2 built. Note that since s is applied within a where, the build !p2 is applied to the original subject term; only variable bindings computed within s can be used in p2 . As an example, consider the following constant folding rule, which reduces an addition of two integer constants to the constant obtained by computing the addition. EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k The addition is computed by applying the primitive strategy addS to the pair of integers (i,j) and matching the result against the variable k , which is then used in the right-hand side. This rule is desugared to EvalPlus = ? Add ( Int ( i ), Int ( j )); where ( ! ( i , j ); addS ; ? k ); ! Int ( k )","title":"Desugaring"},{"location":"references/stratego/strategy-combinators/","text":"Strategy Combinators \u00b6 A strategy expression combines the application of rules using strategy combinators . We first provide a concise overview of (the syntax of) all combinators. After that, we provide a more detailed description of each combinator. Sequential Combinators id : identity fail : failure s1 ; s2 : sequential composition s1 <+ s2 : left choice s1 < s2 + s3 : guarded left choice if s1 then s2 else s3 : if-then-else switch ... end : switch s1 + s2 : non-deterministic choice rec x(s) : fixpoint recursion Term Combinators !p : build ?p : match {x, ... : s} : term variable scope Strategy Sugar (p1 -> p2) : anonymous rewrite rule where(s) : where with(s) : with \\ p1 -> p2 where s \\ : lamba rules <s> p : apply s => p : match <s> p1 => p2 : apply and match p1 := p2 : assign Term Sugar <s> p : apply in build !p[<s>] : term wrap ?p[<s>] : term project Traversal Combinators c(s1, ..., sn) : congruence (s1, ..., sn) : tuple congruence [s1, ..., sn] : list congruence [s1, ..., sn | s] : list congruence all(s) : all one(s) : one some(s) : some Generic Term (De)Construction c#(ts) : generic term (de)construction Sequential Combinators \u00b6 Identity and Failure \u00b6 id fail The identity strategy id always succeeds and behaves as the identity function on terms. The failure strategy fail always fails. The operations have no side effects. Sequential Composition \u00b6 $ StrategyExp ; $ StrategyExp The sequential composition s1 ; s2 of the strategies s1 and s2 first applies the strategy s1 to the subject term and then s2 to the result of that first application. The strategy fails if either s1 or s2 fails. Properties. Sequential composition is associative. Identity is a left and right unit for sequential composition; since id always succeeds and leaves the term alone, it has no additional effect to the strategy that it is composed with. Failure is a left zero for sequential composition; since fail always fails the next strategy will never be reached. This leads to the following equations: ( s1 ; s2 ) ; s3 = s1 ; ( s2 ; s3 ) id ; s = s s ; id = s fail ; s = fail However, not for all strategies s we have that failure is a right zero for sequential composition: s ; fail = fail // is not a law Although the composition s; fail will always fail, the execution of s may have side effects that are not performed by fail . For example, consider printing a term in s . Example. Consider the following rewrite rules. A : P ( Z (), x ) - > x B : P ( S ( x ), y ) - > P ( x , S ( y )) The following applications shows the effect of first applying B and then A : < B > ! P ( S ( Z ()), Z ()) => P ( S ( Z ), Z ) < A > P ( Z , S ( Z )) => S ( Z ) Using the sequential composition of the two rules, this effect can be achieved \u2018in one step\u2019: < B ; A > ! P ( S ( Z ()), Z ()) => S ( Z ) The following application shows that the application of a composition fails if the second strategy in the composition fails to apply to the result of the first: < B ; B > ! P ( S ( Z ()), Z ()) // fails Left Choice \u00b6 $ StrategyExp < + $ StrategyExp The left choice or deterministic choice s1 <+ s2 tries to apply s1 and s2 in that order. That is, it first tries to apply s1 , and if that succeeds the choice succeeds. However, if the application of s1 fails, s2 is applied to the original term. Properties. Left choice is associative. Identity is a left zero for left choice; since id always succeeds, the alternative strategy will never be tried. Failure is a left and right unit for left choice; since fail always fails, the choice will always backtrack to the alternative strategy, and use of fail as alternative strategy is pointless. ( s1 < + s2 ) < + s3 = s1 < + ( s2 < + s3 ) id < + s = id fail < + s = s s < + fail = s However, identity is not a right zero for left choice. That is, not for all strategies s we have that s < + id = s // is not a law The expression s <+ id always succeeds, even (especially) in the case that s fails, in which case the right-hand side of the equation fails of course. Local Backtracking. The left choice combinator is a local backtracking combinator. That is, the choice is committed once the left-hand side strategy has succeeded, even if the continuation strategy fails. This is expressed by the fact that the property ( s1 < + s2 ); s3 = ( s1 ; s3 ) < + ( s2 ; s3 ) // is not a law does not hold for all s1 , s2 , and s3 . The difference is illustrated by the following applications: <( B < + id ); B > P ( S ( Z ), Z ) // fails <( B ; B ) < + ( id ; B )> P ( S ( Z ()), Z ()) => P ( Z , S ( Z )) In the application of (B <+ id); B , the first application of B succeeds after which the choice is committed. The subsequent application of B then fails. This is equivalent to first applying (B <+ id) and then applying B to the result. The application of (B; B) <+ (id; B) , however, is successful; the application of B; B fails, after which the choice backtracks to id; B , which succeeds. Example. The typical use of left choice is to create a composite strategy trying one from several possible transformations. If the strategies that are composed are mutually exclusive, that is, don\u2019t succeed for the same terms, their sum is a transformation that (deterministically) covers a larger set of terms. For example, consider the following two rewrite rules: PlusAssoc : Plus ( Plus ( e1 , e2 ), e3 ) - > Plus ( e1 , Plus ( e2 , e3 )) PlusZero : Plus ( Int ( \"0\" ), e ) - > e These rules are mutually exclusive, since there is no term that matches the left-hand sides of both rules. Combining the rules with left choice into PlusAssoc <+ PlusZero creates a strategy that transforms terms matching both rules as illustrated by the following applications: < PlusAssoc > Plus ( Int ( \"0\" ), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Int ( \"0\" ), Int ( \"3\" )) => Int ( \"3\" ) < PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) => Plus ( Var ( \"x\" ), Plus ( Int ( \"42\" ), Int ( \"3\" ))) Example. An application of <+ in combination with id is the reflexive closure of a strategy s : try ( s ) = s < + id The user-defined strategy combinator try tries to apply its argument strategy s , but if that fails, just succeeds using id . Guarded Left Choice \u00b6 $ StrategyExp < $ StrategyExp + $ StrategyExp With the guarded left choice operator s1 < s2 + s3 , if s1 succeeds s2 is applied, else s3 is applied. If s2 fails, the complete expression fails; no backtracking to s3 takes place. Properties. This combinator is a generalization of the left choice combinator <+ . s1 < + s2 = s1 < id + s2 The following laws make clear that the \u2018branches\u2019 of the choice are selected by the success or failure of the guard: id < s2 + s3 = s2 fail < s2 + s3 = s3 If the right branch always fails, the construct reduces to the sequential composition of the guard and the left branch. s1 < s2 + fail = s1 ; s2 Guarded choice is not associative: ( s1 < s2 + s3 ) < s4 + s5 = s1 < s2 + ( s3 < s4 + s5 ) // not a law To see why consider the possible traces of these expressions. For example, when s1 and s2 succeed subsequently, the left-hand side expression calls s4 , while the right-hand side expression does not. However, sequential composition distributes over guarded choice from left and right: ( s1 < s2 + s3 ); s4 = s1 < ( s2 ; s4 ) + ( s3 ; s4 ) s0 ; ( s1 < s2 + s3 ) = ( s0 ; s1 ) < s2 + s3 Examples. The guarded left choice operator is most useful for the implementation of higher-level control-flow strategies. For example, the negation not(s) of a strategy s , succeeds if s fails, and fails when it succeeds: not ( s ) = s < fail + id Since failure discards the effect of a (successful) transformation, this has the effect of testing whether s succeeds. So we have the following laws for not: not ( id ) = fail not ( fail ) = id However, side effects performed by s are not undone, of course. Therefore, the following equation does not hold: not ( not ( s )) = s // not a law Another example of the use of guarded choice is the restore-always combinator: restore-always ( s , r ) = s < r + ( r ; fail ) It applies a \u2018restore\u2019 strategy r after applying a strategy s , even if s fails, and preserves the success/failure behavior of s . Since fail discards the transformation effect of r , this is mostly useful for ensuring that some side-effecting operation is done (or undone) after applying s . If-then-else \u00b6 if $ StrategyExp then $ StrategyExp else $ StrategyExp end The if s1 then s2 else s3 end construct is like the traditional construct since both branches apply to the original term. The condition strategy is only used to test if it succeeds or fails, but its transformation effect is undone. However, the condition strategy s1 is still applied to the subject term. The if s1 then s2 end strategy is similar; if the condition fails, the strategy succeeds. The if-then-else-end strategy is just syntactic sugar for a combination of guarded choice and the where combinator: if s1 then s2 else s3 end ==> // transforms to where ( s1 ) < s2 + s3 The strategy where(s) succeeds if s succeeds, but returns the original subject term. Properties. The following laws show that the branches are selected by success or failure of the condition: if id then s2 else s3 end = s2 if fail then s2 else s3 end = s3 The if-then-end strategy is an abbreviation for the if-then-else-end with the identity strategy as right branch: if s1 then s2 end = where ( s1 ) < s2 + id Examples. The inclusive or or(s1, s2) succeeds if one of the strategies s1 or s2 succeeds, but guarantees that both are applied, in the order s1 first, then s2 : or ( s1 , s2 ) = if s1 then try ( where ( s2 )) else where ( s2 ) end This ensures that any side effects are always performed, in contrast to s1 <\\+ s2 , where s2 is only executed if s1 fails. (Thus, left choice implements a short circuit Boolean or.) Similarly, the following and(s1, s2) combinator is the non-short circuit version of Boolean conjunction: and ( s1 , s2 ) = if s1 then where ( s2 ) else where ( s2 ); fail end Switch \u00b6 switch s0 case s1 : s1 ' case s2 : s2 ' ... otherwise : sdef end The switch construct is an n-ary branching construct similar to its counter parts in other programming languages. It is defined in terms of guarded choice. The switch first applies the s0 strategy to the current term t resulting in a term t' . Then it tries the cases in turn applying each si to t' . As soon as this succeeds the corresponding case is selected and si' is applied to the t , the term to which the switch was applied. If none of the cases applies, the default strategy sdef from the otherwise is applied. The switch construct is syntactic sugar for a nested if-then-else: { x : where ( s0 => x ); if < s1 > x then s1 ' else if < s2 > x then s2 ' else if ... then ... else sdef end end end } Non-Deterministic Choice \u00b6 $ StrategyExp + $ StrategyExp The non-deterministic choice operator s1 + s2 chooses one of the two strategies s1 or s2 to apply, such that the one it chooses succeeds. If both strategies fail, then the choice fails as well. Operationally the choice operator first tries one strategy, and, if that fails, tries the other. The order in which this is done is undefined, i.e., arbitrarily chosen by the compiler or runtime system. The + combinator is used to model modular composition of rewrite rules and strategies with the same name, but in different modules. Multiple definitions with the same name in different modules, are merged into a single definition with that name, where the bodies are composed with + . The following transformation illustrates this: module A f = s1 module B f = s2 module main imports A B => f = s2 + s1 This transformation is somewhat simplified; the complete transformation needs to take care of local variables and parameters. While the + combinator is used internally by the compiler for this purpose, programmers are advised not to use this combinator, but rather use the left choice combinator <+ to avoid surprises. Note. In the past, the + combinator was also used to compose definitions with the same name within a module. This has been replaced by interpreting such compositions with the textual order of the definitions. The following transformation illustrates this: module A f = s1 f = s2 => f = s1 < + s2 Fixpoint Recursion \u00b6 rec $ Id ( $ StrategyExp ) The fixpoint operator rec x(s) , which recurses on applications of x within s . The rec operator allows the definition of an unnamed strategy expression to be recursive. For example, in the definition g ( s ) = foo ; rec x ( ... x ... ); bar the strategy between foo and bar is a recursive strategy that does not recurse to g(s) . Alternative. Originally, the rec operator was the only way to define recursive strategies. Currently, a recursive definition is a normal strategy definition with a recursive call in its body. f ( s ) = ... f ( s ) ... The rec operator is still in the language in the first place because it is widely used in many existing programs, and in the second place because it can be a concise expression of a recursive strategy, since call parameters are not included in the call. Furthermore, all free variables remain in scope. Example. The repeat strategy applies a transformation s until it fails. It is defined as a recursive definition using try as follows: try ( s ) = s < + id repeat ( s ) = try ( s ; repeat ( s )) An equivalent definition using rec is: repeat ( s ) = rec x ( try ( s ; x )) Term Combinators \u00b6 Building Terms \u00b6 ! $ Term The build operation !p replaces the subject term with the instantiation of the pattern p using the bindings from the environment to the variables occurring in p . Example. The strategy !Or(And(x, z), And(y, z)) replaces the subject term with the instantiation of Or(And(x, z), And(y, z)) using bindings to variables x , y and z . ! Int ( \"10\" ) => Int ( \"10\" ) ! Plus ( Var ( \"a\" ), Int ( \"10\" )) => Plus ( Var ( \"a\" ), Int ( \"10\" )) It is possible to build terms with variables. A pattern is a term with meta-variables. The current term is replaced by an instantiation of pattern p . Example. In a context where e is bound to Var(\"b\") ! Plus ( Var ( \"a\" ), e ) => Plus ( Var ( \"a\" ), Var ( \"b\" )) Matching Terms \u00b6 ? $ Term The match operation ?t matches the subject term against the term t . < ? Plus ( Var ( \"a\" ), Int ( \"3\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // succeeds < ? Plus ( Int ( \"3\" ), Var ( \"b\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails Matching against a term pattern with variables binds those variables to (parts of) the current term. The match strategy ?x compares the current term ( t ) to variable x . It binds variable x to term t in the environment. A variable can only be bound once, or to the same term. < ? e > Plus ( Var ( \"a\" ), Int ( \"3\" )) // binds e to Plus(Var(\"a\"),Int(\"3\")) < ? e > ! Int ( \"17\" ) // fails The general case is matching against an arbitrary term pattern. The match strategy ?p compares the current term to a pattern p . It will add bindings for the variables in pattern p to the environment. The wildcard _ in a match will match any term. < ? Plus ( e , _ )> Plus ( Var ( \"a\" ), Int ( \"3\" )) // e is bound to Var(\"a\") Patterns may be non-linear. Multiple occurrences of the same variable can occur and each occurrence has to match the same term. < ? Plus ( e , e )> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails < ? Plus ( e , e )> ! Plus ( Var ( \"a\" ), Var ( \"a\" )) // e is bound to Var(\"a\") Example. Non-linear pattern matching is a way to test equality of terms. Indeed the equality predicates from the Stratego Library are defined using non-linear pattern matching: equal = ? ( x , x ) equal (| x ) = ? x The equal strategy tests whether the current term is a a pair of the same terms. The equal(|x) strategy tests whether the current term is equal to the argument term. < equal >( \"a\" , \"a\" ) // succeeds < equal >( \"a\" , \"b\" ) // fails < equal (| Foo ( Baz ()))> Foo ( Bar ()) // fails < equal (| Foo ( Bar ()))> Foo ( Bar ()) // succeeds Term Variable Scope \u00b6 { $ Id , ... : $ StrategyExp } Once a variable is bound it cannot be rebound to a different term. Thus, when we have applied an anonymous rule once, its variables are bound and the next time it is applied it only succeeds for the same term. For example, in the next session the second application of the rule fails, because e2 is bound to Int(\"3\") and does not match with Var(\"b\") . < ? Plus ( e1 , e2 ) ! Plus ( e2 , e1 ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) // e1 is bound to Var(\"a\") // e2 is bound to Int(\"3\") < ? Plus ( e1 , e2 ) ! Plus ( e2 , e1 )> Plus ( Var ( \"a\" ), Var ( \"b\" )) // fails To use a variable name more than once Stratego provides term variable scope. A scope {x1,...,xn : s} locally undefines the variables xi . That is, the binding to a variable xi outside the scope is not visible inside it, nor is the binding to xi inside the scope visible outside it. For example, to continue the session above, if we wrap the anonymous swap rule in a scope for its variables, it can be applied multiple times. <{ e3 , e4 : ? Plus ( e3 , e4 ); ! Plus ( e4 , e3 )}> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) // e3 is not bound to a term <{ e3 , e4 : ? Plus ( e3 , e4 ); ! Plus ( e4 , e3 )}> Plus ( Var ( \"a\" ), Var ( \"b\" )) => Plus ( Var ( \"b\" ), Var ( \"a\" )) Of course we can name such a scoped rule using a strategy definition, and then invoke it by its name: SwapArgs = { e1 , e2 : ? Plus ( e1 , e2 ); ! Plus ( e2 , e1 )} < SwapArgs > Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) Implicit Variable Scope \u00b6 When using match and build directly in a strategy definition, rather than in the form of a rule, the definition contains free variables. Strictly speaking such variables should be declared using a scope, that is one should write SwapArgs = { e1 , e2 : ? Plus ( e1 , e2 ); ! Plus ( e2 , e1 )} However, since declaring all variables at the top of a definition is distracting and does not add much to the definition, such a scope declaration can be left out. Thus, one can write SwapArgs = ( Plus ( e1 , e2 ) - > Plus ( e2 , e1 )) instead. The scope is automatically inserted by the compiler. This implies that there is no global scope for term variables. Of course, variables in inner scopes should be declared where necessary. In particular, note that variable scope is not inserted for strategy definitions in a let binding, such as let SwapArgs = ? Plus ( e1 , e2 ); Plus ( e2 , e1 ) in ... end While the variables are bound in the enclosing definition, they are not restricted to SwapArgs in this case, since in a let one typically wants to use bindings to variables in the enclosing code. Combining Match and Build \u00b6 Match and build are first-class citizens in Stratego, which means that they can be used and combined just like any other strategy expressions. In particular, we can implement rewrite rules using these operations, since a rewrite rule is basically a match followed by a build. For example, consider the following combination of match and build: < ? Plus ( e1 , e2 ); ! Plus ( e2 , e1 )> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) This combination first recognizes a term, binds variables to the pattern in the match, and then replaces the current term with the instantiation of the build pattern. Note that the variable bindings are propagated from the match to the build. Stratego provides syntactic sugar for various combinations of match and build. Anonymous Rewrite Rule \u00b6 ( $ Pattern - > $ Pattern ) An anonymous rewrite rule (p1 -> p2) transforms a term matching p1 into an instantiation of p2 . Such a rule is equivalent to the sequence ?p1; !p2 . <( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) Where \u00b6 where ( $ StrategyExp ) The where(s) combinator applies s to the current term, but restores that term afterwards. Any bindings to variables are kept, however. The where(s) construct is syntactic sugar for {x: ?x; s; !x} with x a fresh variable not occurring in s . Thus, the current subject term is saved by binding it to a new variable x , then the strategy s is applied, and finally, the original term is restored by building x . Example < where ( ? Plus ( Int ( i ), Int ( j )); < addS >( i , j ) => k )> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Int ( \"14\" ), Int ( \"3\" )) // i is bound to \"14\" // k is bound to \"17\" With \u00b6 with ( s ) The strategy with(s) applies s on the current subject term and then restores the current subject term. In other words, s is executed solely for its side effects, such as binding variables. In this respect, with is like where . However, with(s) differs in a key way: if the strategy s fails, Stratego stops with an error, reporting the strategy that failed. Lambda Rules \u00b6 \\ $ Term - > $ Term where $ Condition \\ A lambda rule of the form \\ p1 -> p2 where s \\ is an anonymous rewrite rule for which the variables in the left-hand side p1 are local to the rule, that is, it is equivalent to an expression of the form { x1 , ... , xn : ( p1 - > p2 where s )} with x1 ,\u2026, xn the variables of p1 . This means that any variables used in s and p2 that do not occur in p1 are bound in the context of the rule. Example. < map ( \\ ( x , y ) - > x \\ )> [( 1 , 2 ),( 3 , 4 ),( 5 , 6 )] => [ 1 , 3 , 5 ] Apply and Match \u00b6 $ StrategyExp => $ Term < $ StrategyExp > $ Term < $ StrategyExp > $ Term => $ Term The operation <s> p captures the notion of applying a strategy to a term, i.e., the scenario !p; s . The operation s => p capture the notion of applying a strategy to the current subject term and then matching the result against the pattern p , i.e., s; ?p . The combined operation <s> p1 => p2 thus captures the notion of applying a strategy to a term p1 and matching the result against p2 , i.e, !p1; s; ?p2 . Example. The conditional rewrite rule EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k can be reformulated as EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where < addS >( i , j ) => k Assignment \u00b6 $ Term := $ Term The strategy p1 := p2 builds p2 and matches the result against p1 , i.e. it is equivalent to !p2; ?p1 . The strategy is often combined with strategy application into p1 := <s>p2 , which is equivalent to <s>p2 => p1 (but more familiar to an audience with an imperative mindset). For example, consider the following rewrite rule EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) with k := < addS >( i , j ) Applying Strategies in Build \u00b6 < $ StrategyExp > $ Term // in build pattern In a build pattern, the application <s>t applies the strategy s to the term t , returning the resulting term. Example. The constant folding rule EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where < addS >( i , j ) => k can be simplified by directly applying the addition in the right-hand side: EvalPlus : Add ( Int ( i ), Int ( j )) - > Int (< addS >( i , j )) Example. The following definition of the map(s) strategy applies a strategy to each term in a list: map ( s ) : [] - > [] map ( s ) : [ x | xs ] - > [< s > x | < map ( s )> xs ] Term Wrap \u00b6 < $ StrategyExp > // in build pattern A term wrap is a build strategy !p[<s>] containing one or more strategy applications <s> that are not applied to a term. When executing the the build operation, each occurrence of such a strategy application <s> is replaced with the term resulting from applying s to the current subject term, i.e., the one that is being replaced by the build. Motivation. One often write rules of the form x -> Foo(Bar(x)) , i.e. wrapping a term pattern around the current term. Using rule syntax this is quite verbose. The syntactic abstraction of term wraps, allows the concise specification of such little transformations as !Foo(Bar(<id>)) . Example. The following applications illustrate some uses of term wraps: < ! (< id >,< id >)> 3 => ( 3 , 3 ) <(< Fst ; inc >,< Snd >)> ( 3 , 3 ) => ( 4 , 3 ) < ! Call (< id >, [])> \"foobar\" => Call ( \"foobar\" , []) mod2 = < mod >(< id >, 2 ) < mod2 > 6 => 0 Desugaring. Term wraps are implemented by translation to a combination of match and build expressions. Thus, a term wrap !p[<s>] is translated to a strategy expression { x : where ( s => x ); ! p [ x ]} where x is a fresh variable not occurring in s . In other words, the strategy s is applied to the current subject term, i.e., the term to which the build is applied. As an example, the term wrap !Foo(Bar(<id>)) is desugared to the strategy { x : where ( id => x ); ! Foo ( Bar ( x ))} which after simplification is equivalent to {x: ?x; !Foo(Bar(x))} , i.e., exactly the original lambda rule \\x -> Foo(Bar(x))\\ . Term Project \u00b6 < $ StrategyExp > // in match pattern Term projections are the match dual of term wraps. Term projections can be used to project a subterm from a term pattern. Examples. For example, the expression ?And(<id>,x) matches terms of the form And(t1,t2) and reduces them to the first subterm t1 . Another example is the strategy map ( ? FunDec (< id >, _ , _ )) which reduces a list of function declarations to a list of the names of the functions, i.e., the first arguments of the FunDec constructor. Here are some more examples: < ? [ _ |< id >]> [ 1 , 2 , 3 ] => [ 2 , 3 ] < ? Call (< id >, [])> Call ( \"foobar\" , []) => \"foobar\" Term projections can also be used to apply additional constraints to subterms in a match pattern. For example, ?Call(x, <?args; length => 3>) matches only with function calls with three arguments. Desugaring. A match expression ?p[<s>] is desugared as { x : ? p [ x ]; < s > x } That is, after the pattern p[x] matches, it is reduced to the subterm bound to x to which s is applied. The result is also the result of the projection. When multiple projects are used within a match the outcome is undefined, i.e., the order in which the projects will be performed can not be counted on. Traversal Combinators \u00b6 Traversal combinators apply strategies to direct subterms of a term and can be combined with other combinators to define full term traversal strategies. Congruence Operators \u00b6 $ Constructor ( $ StrategyExp , ... , $ StrategyExp ) A congruence operator applies a strategy to each direct subterm of a specific constructor. For each n-ary constructor c declared in a signature, there is a corresponding congruence operator c(s1 , ..., sn) , which applies to terms of the form c(t1 , ..., tn) by applying the argument strategies to the corresponding argument terms. A congruence fails if the application of one the argument strategies fails or if constructor of the operator and that of the term do not match. Example. Consider the following signature of expressions: module expressions signature sorts Exp constructors Plus : Exp * Exp - > Exp Times : Exp * Exp - > Exp The following applications apply the congruence operators Plus and Times to a term: < Plus ( ! Var ( \"a\" ), id )> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) < Times ( id , ! Int ( \"42\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails The first application shows how a congruence transforms a specific subterm, that is the strategy applied can be different for each subterm. The second application shows that a congruence only succeeds for terms constructed with the same constructor. Tuple and List Congruences \u00b6 [ $ StrategExp , ... , $ StrategyExp ] [ $ StrategExp , ... , $ StrategyExp | $ StrategyExp ] ( $ StrategExp , ... , $ StrategyExp ) Congruences can also be applied to tuples, (s1,s2,...,sn) , and lists, [s1,s2,...,sn] . Example. The definition of a map(s) strategy using list congruences: map ( s ) = [] < + [ s | map ( s )] Visiting All Subterms \u00b6 all ( $ StrategyExp ) The all(s) strategy transforms a constructor application by applying the parameter strategy s to each direct subterm. An application of all(s) fails if the application to one of the subterms fails. The following example shows how all (1) applies to any term, and (2) applies its argument strategy uniformly to all direct subterms. < all ( ! Var ( \"a\" ))> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Var ( \"a\" )) < all ( ! Var ( \"z\" ))> Times ( Var ( \"b\" ), Int ( \"3\" )) => Times ( Var ( \"z\" ), Var ( \"z\" )) Example. The bottomup(s) is defined as bottomup ( s ) = all ( bottomup ( s )); s and defines a full traversal over the subject term. Visiting One Subterm \u00b6 one ( $ StrategyExp ) The one(s) strategy transforms a constructor application by applying the parameter strategy s to exactly one direct subterm. An application of one(s) fails if the application to all of the subterms fails. The following applications illustrate the behavior of the combinator: < one ( ! Var ( \"a\" ))> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) < one ( \\ Int ( x ) - > Int (< addS >( x , \"1\" )) \\ )> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"4\" )) < one ( ? Plus ( _ , _ ))> Plus ( Var ( \"a\" ), Int ( \"4\" )) // fails Example. A frequently used application of one is the oncetd(s) traversal, which performs a left to right depth first search/transformation that stops as soon as s has been successfully applied. oncetd ( s ) = s < + one ( oncetd ( s )) Thus, s is first applied to the root of the subject term. If that fails, its direct subterms are searched one by one (from left to right), with a recursive call to oncetd(s) . An application of oncetd is the contains(|t) strategy, which checks whether the subject term contains a subterm that is equal to t. contains (| t ) = oncetd ( ? t ) Through the depth first search of oncetd , either an occurrence of t is found, or all subterms are verified to be unequal to t . Visiting Some Subterms \u00b6 some ( $ StrategyExp ) The some(s) strategy transforms a constructor application by applying the parameter strategy s to as many direct subterms as possible and at least one. An application of some(s) fails if the application to all of the subterms fails. Some one-pass traversals based on some: sometd ( s ) = s < + some ( sometd ( s )) somebu ( s ) = some ( somebu ( s )) < + s A fixed-point traversal with some: reduce-par ( s ) = repeat ( rec x ( some ( x ) + s )) Generic Term Deconstruction \u00b6 $ Term # ( $ Term ) // in a match pattern The term pattern expression c#(ts) used in a match pattern succeeds when applied to a constructor application and matches the constructor name (as a string) to c and the list of term arguments to ts . Generic Term Construction \u00b6 $ Term # ( $ Term ) // in a build pattern The term pattern expression c#(ts) used in a build pattern succeeds when c constructs a string and ts constructs a list of terms. It then builds the corresponding constructor application c(ts) . References \u00b6 Rather than defining rewrite rules and high-level strategies as primitives of the language, Stratego provides strategy combinators as basic building blocks from which these can defined 1 . Thus, Stratego consists of a core language 2 and a 'sugar' language defined by reduction to the core language. Warning While it useful to understand the constructs defined in this section, their use should be avoided in favour of the higher-level language constructs, such as rewrite rules , where possible. Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661 \\(05\\) 80027-1 , doi:10.1016/S1571-0661 \\(05\\) 80027-1 . \u21a9","title":"Strategy Combinators"},{"location":"references/stratego/strategy-combinators/#strategy-combinators","text":"A strategy expression combines the application of rules using strategy combinators . We first provide a concise overview of (the syntax of) all combinators. After that, we provide a more detailed description of each combinator. Sequential Combinators id : identity fail : failure s1 ; s2 : sequential composition s1 <+ s2 : left choice s1 < s2 + s3 : guarded left choice if s1 then s2 else s3 : if-then-else switch ... end : switch s1 + s2 : non-deterministic choice rec x(s) : fixpoint recursion Term Combinators !p : build ?p : match {x, ... : s} : term variable scope Strategy Sugar (p1 -> p2) : anonymous rewrite rule where(s) : where with(s) : with \\ p1 -> p2 where s \\ : lamba rules <s> p : apply s => p : match <s> p1 => p2 : apply and match p1 := p2 : assign Term Sugar <s> p : apply in build !p[<s>] : term wrap ?p[<s>] : term project Traversal Combinators c(s1, ..., sn) : congruence (s1, ..., sn) : tuple congruence [s1, ..., sn] : list congruence [s1, ..., sn | s] : list congruence all(s) : all one(s) : one some(s) : some Generic Term (De)Construction c#(ts) : generic term (de)construction","title":"Strategy Combinators"},{"location":"references/stratego/strategy-combinators/#sequential-combinators","text":"","title":"Sequential Combinators"},{"location":"references/stratego/strategy-combinators/#identity-and-failure","text":"id fail The identity strategy id always succeeds and behaves as the identity function on terms. The failure strategy fail always fails. The operations have no side effects.","title":"Identity and Failure"},{"location":"references/stratego/strategy-combinators/#sequential-composition","text":"$ StrategyExp ; $ StrategyExp The sequential composition s1 ; s2 of the strategies s1 and s2 first applies the strategy s1 to the subject term and then s2 to the result of that first application. The strategy fails if either s1 or s2 fails. Properties. Sequential composition is associative. Identity is a left and right unit for sequential composition; since id always succeeds and leaves the term alone, it has no additional effect to the strategy that it is composed with. Failure is a left zero for sequential composition; since fail always fails the next strategy will never be reached. This leads to the following equations: ( s1 ; s2 ) ; s3 = s1 ; ( s2 ; s3 ) id ; s = s s ; id = s fail ; s = fail However, not for all strategies s we have that failure is a right zero for sequential composition: s ; fail = fail // is not a law Although the composition s; fail will always fail, the execution of s may have side effects that are not performed by fail . For example, consider printing a term in s . Example. Consider the following rewrite rules. A : P ( Z (), x ) - > x B : P ( S ( x ), y ) - > P ( x , S ( y )) The following applications shows the effect of first applying B and then A : < B > ! P ( S ( Z ()), Z ()) => P ( S ( Z ), Z ) < A > P ( Z , S ( Z )) => S ( Z ) Using the sequential composition of the two rules, this effect can be achieved \u2018in one step\u2019: < B ; A > ! P ( S ( Z ()), Z ()) => S ( Z ) The following application shows that the application of a composition fails if the second strategy in the composition fails to apply to the result of the first: < B ; B > ! P ( S ( Z ()), Z ()) // fails","title":"Sequential Composition"},{"location":"references/stratego/strategy-combinators/#left-choice","text":"$ StrategyExp < + $ StrategyExp The left choice or deterministic choice s1 <+ s2 tries to apply s1 and s2 in that order. That is, it first tries to apply s1 , and if that succeeds the choice succeeds. However, if the application of s1 fails, s2 is applied to the original term. Properties. Left choice is associative. Identity is a left zero for left choice; since id always succeeds, the alternative strategy will never be tried. Failure is a left and right unit for left choice; since fail always fails, the choice will always backtrack to the alternative strategy, and use of fail as alternative strategy is pointless. ( s1 < + s2 ) < + s3 = s1 < + ( s2 < + s3 ) id < + s = id fail < + s = s s < + fail = s However, identity is not a right zero for left choice. That is, not for all strategies s we have that s < + id = s // is not a law The expression s <+ id always succeeds, even (especially) in the case that s fails, in which case the right-hand side of the equation fails of course. Local Backtracking. The left choice combinator is a local backtracking combinator. That is, the choice is committed once the left-hand side strategy has succeeded, even if the continuation strategy fails. This is expressed by the fact that the property ( s1 < + s2 ); s3 = ( s1 ; s3 ) < + ( s2 ; s3 ) // is not a law does not hold for all s1 , s2 , and s3 . The difference is illustrated by the following applications: <( B < + id ); B > P ( S ( Z ), Z ) // fails <( B ; B ) < + ( id ; B )> P ( S ( Z ()), Z ()) => P ( Z , S ( Z )) In the application of (B <+ id); B , the first application of B succeeds after which the choice is committed. The subsequent application of B then fails. This is equivalent to first applying (B <+ id) and then applying B to the result. The application of (B; B) <+ (id; B) , however, is successful; the application of B; B fails, after which the choice backtracks to id; B , which succeeds. Example. The typical use of left choice is to create a composite strategy trying one from several possible transformations. If the strategies that are composed are mutually exclusive, that is, don\u2019t succeed for the same terms, their sum is a transformation that (deterministically) covers a larger set of terms. For example, consider the following two rewrite rules: PlusAssoc : Plus ( Plus ( e1 , e2 ), e3 ) - > Plus ( e1 , Plus ( e2 , e3 )) PlusZero : Plus ( Int ( \"0\" ), e ) - > e These rules are mutually exclusive, since there is no term that matches the left-hand sides of both rules. Combining the rules with left choice into PlusAssoc <+ PlusZero creates a strategy that transforms terms matching both rules as illustrated by the following applications: < PlusAssoc > Plus ( Int ( \"0\" ), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Int ( \"0\" ), Int ( \"3\" )) => Int ( \"3\" ) < PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) // fails < PlusAssoc < + PlusZero > Plus ( Plus ( Var ( \"x\" ), Int ( \"42\" )), Int ( \"3\" )) => Plus ( Var ( \"x\" ), Plus ( Int ( \"42\" ), Int ( \"3\" ))) Example. An application of <+ in combination with id is the reflexive closure of a strategy s : try ( s ) = s < + id The user-defined strategy combinator try tries to apply its argument strategy s , but if that fails, just succeeds using id .","title":"Left Choice"},{"location":"references/stratego/strategy-combinators/#guarded-left-choice","text":"$ StrategyExp < $ StrategyExp + $ StrategyExp With the guarded left choice operator s1 < s2 + s3 , if s1 succeeds s2 is applied, else s3 is applied. If s2 fails, the complete expression fails; no backtracking to s3 takes place. Properties. This combinator is a generalization of the left choice combinator <+ . s1 < + s2 = s1 < id + s2 The following laws make clear that the \u2018branches\u2019 of the choice are selected by the success or failure of the guard: id < s2 + s3 = s2 fail < s2 + s3 = s3 If the right branch always fails, the construct reduces to the sequential composition of the guard and the left branch. s1 < s2 + fail = s1 ; s2 Guarded choice is not associative: ( s1 < s2 + s3 ) < s4 + s5 = s1 < s2 + ( s3 < s4 + s5 ) // not a law To see why consider the possible traces of these expressions. For example, when s1 and s2 succeed subsequently, the left-hand side expression calls s4 , while the right-hand side expression does not. However, sequential composition distributes over guarded choice from left and right: ( s1 < s2 + s3 ); s4 = s1 < ( s2 ; s4 ) + ( s3 ; s4 ) s0 ; ( s1 < s2 + s3 ) = ( s0 ; s1 ) < s2 + s3 Examples. The guarded left choice operator is most useful for the implementation of higher-level control-flow strategies. For example, the negation not(s) of a strategy s , succeeds if s fails, and fails when it succeeds: not ( s ) = s < fail + id Since failure discards the effect of a (successful) transformation, this has the effect of testing whether s succeeds. So we have the following laws for not: not ( id ) = fail not ( fail ) = id However, side effects performed by s are not undone, of course. Therefore, the following equation does not hold: not ( not ( s )) = s // not a law Another example of the use of guarded choice is the restore-always combinator: restore-always ( s , r ) = s < r + ( r ; fail ) It applies a \u2018restore\u2019 strategy r after applying a strategy s , even if s fails, and preserves the success/failure behavior of s . Since fail discards the transformation effect of r , this is mostly useful for ensuring that some side-effecting operation is done (or undone) after applying s .","title":"Guarded Left Choice"},{"location":"references/stratego/strategy-combinators/#if-then-else","text":"if $ StrategyExp then $ StrategyExp else $ StrategyExp end The if s1 then s2 else s3 end construct is like the traditional construct since both branches apply to the original term. The condition strategy is only used to test if it succeeds or fails, but its transformation effect is undone. However, the condition strategy s1 is still applied to the subject term. The if s1 then s2 end strategy is similar; if the condition fails, the strategy succeeds. The if-then-else-end strategy is just syntactic sugar for a combination of guarded choice and the where combinator: if s1 then s2 else s3 end ==> // transforms to where ( s1 ) < s2 + s3 The strategy where(s) succeeds if s succeeds, but returns the original subject term. Properties. The following laws show that the branches are selected by success or failure of the condition: if id then s2 else s3 end = s2 if fail then s2 else s3 end = s3 The if-then-end strategy is an abbreviation for the if-then-else-end with the identity strategy as right branch: if s1 then s2 end = where ( s1 ) < s2 + id Examples. The inclusive or or(s1, s2) succeeds if one of the strategies s1 or s2 succeeds, but guarantees that both are applied, in the order s1 first, then s2 : or ( s1 , s2 ) = if s1 then try ( where ( s2 )) else where ( s2 ) end This ensures that any side effects are always performed, in contrast to s1 <\\+ s2 , where s2 is only executed if s1 fails. (Thus, left choice implements a short circuit Boolean or.) Similarly, the following and(s1, s2) combinator is the non-short circuit version of Boolean conjunction: and ( s1 , s2 ) = if s1 then where ( s2 ) else where ( s2 ); fail end","title":"If-then-else"},{"location":"references/stratego/strategy-combinators/#switch","text":"switch s0 case s1 : s1 ' case s2 : s2 ' ... otherwise : sdef end The switch construct is an n-ary branching construct similar to its counter parts in other programming languages. It is defined in terms of guarded choice. The switch first applies the s0 strategy to the current term t resulting in a term t' . Then it tries the cases in turn applying each si to t' . As soon as this succeeds the corresponding case is selected and si' is applied to the t , the term to which the switch was applied. If none of the cases applies, the default strategy sdef from the otherwise is applied. The switch construct is syntactic sugar for a nested if-then-else: { x : where ( s0 => x ); if < s1 > x then s1 ' else if < s2 > x then s2 ' else if ... then ... else sdef end end end }","title":"Switch"},{"location":"references/stratego/strategy-combinators/#non-deterministic-choice","text":"$ StrategyExp + $ StrategyExp The non-deterministic choice operator s1 + s2 chooses one of the two strategies s1 or s2 to apply, such that the one it chooses succeeds. If both strategies fail, then the choice fails as well. Operationally the choice operator first tries one strategy, and, if that fails, tries the other. The order in which this is done is undefined, i.e., arbitrarily chosen by the compiler or runtime system. The + combinator is used to model modular composition of rewrite rules and strategies with the same name, but in different modules. Multiple definitions with the same name in different modules, are merged into a single definition with that name, where the bodies are composed with + . The following transformation illustrates this: module A f = s1 module B f = s2 module main imports A B => f = s2 + s1 This transformation is somewhat simplified; the complete transformation needs to take care of local variables and parameters. While the + combinator is used internally by the compiler for this purpose, programmers are advised not to use this combinator, but rather use the left choice combinator <+ to avoid surprises. Note. In the past, the + combinator was also used to compose definitions with the same name within a module. This has been replaced by interpreting such compositions with the textual order of the definitions. The following transformation illustrates this: module A f = s1 f = s2 => f = s1 < + s2","title":"Non-Deterministic Choice"},{"location":"references/stratego/strategy-combinators/#fixpoint-recursion","text":"rec $ Id ( $ StrategyExp ) The fixpoint operator rec x(s) , which recurses on applications of x within s . The rec operator allows the definition of an unnamed strategy expression to be recursive. For example, in the definition g ( s ) = foo ; rec x ( ... x ... ); bar the strategy between foo and bar is a recursive strategy that does not recurse to g(s) . Alternative. Originally, the rec operator was the only way to define recursive strategies. Currently, a recursive definition is a normal strategy definition with a recursive call in its body. f ( s ) = ... f ( s ) ... The rec operator is still in the language in the first place because it is widely used in many existing programs, and in the second place because it can be a concise expression of a recursive strategy, since call parameters are not included in the call. Furthermore, all free variables remain in scope. Example. The repeat strategy applies a transformation s until it fails. It is defined as a recursive definition using try as follows: try ( s ) = s < + id repeat ( s ) = try ( s ; repeat ( s )) An equivalent definition using rec is: repeat ( s ) = rec x ( try ( s ; x ))","title":"Fixpoint Recursion"},{"location":"references/stratego/strategy-combinators/#term-combinators","text":"","title":"Term Combinators"},{"location":"references/stratego/strategy-combinators/#building-terms","text":"! $ Term The build operation !p replaces the subject term with the instantiation of the pattern p using the bindings from the environment to the variables occurring in p . Example. The strategy !Or(And(x, z), And(y, z)) replaces the subject term with the instantiation of Or(And(x, z), And(y, z)) using bindings to variables x , y and z . ! Int ( \"10\" ) => Int ( \"10\" ) ! Plus ( Var ( \"a\" ), Int ( \"10\" )) => Plus ( Var ( \"a\" ), Int ( \"10\" )) It is possible to build terms with variables. A pattern is a term with meta-variables. The current term is replaced by an instantiation of pattern p . Example. In a context where e is bound to Var(\"b\") ! Plus ( Var ( \"a\" ), e ) => Plus ( Var ( \"a\" ), Var ( \"b\" ))","title":"Building Terms"},{"location":"references/stratego/strategy-combinators/#matching-terms","text":"? $ Term The match operation ?t matches the subject term against the term t . < ? Plus ( Var ( \"a\" ), Int ( \"3\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // succeeds < ? Plus ( Int ( \"3\" ), Var ( \"b\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails Matching against a term pattern with variables binds those variables to (parts of) the current term. The match strategy ?x compares the current term ( t ) to variable x . It binds variable x to term t in the environment. A variable can only be bound once, or to the same term. < ? e > Plus ( Var ( \"a\" ), Int ( \"3\" )) // binds e to Plus(Var(\"a\"),Int(\"3\")) < ? e > ! Int ( \"17\" ) // fails The general case is matching against an arbitrary term pattern. The match strategy ?p compares the current term to a pattern p . It will add bindings for the variables in pattern p to the environment. The wildcard _ in a match will match any term. < ? Plus ( e , _ )> Plus ( Var ( \"a\" ), Int ( \"3\" )) // e is bound to Var(\"a\") Patterns may be non-linear. Multiple occurrences of the same variable can occur and each occurrence has to match the same term. < ? Plus ( e , e )> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails < ? Plus ( e , e )> ! Plus ( Var ( \"a\" ), Var ( \"a\" )) // e is bound to Var(\"a\") Example. Non-linear pattern matching is a way to test equality of terms. Indeed the equality predicates from the Stratego Library are defined using non-linear pattern matching: equal = ? ( x , x ) equal (| x ) = ? x The equal strategy tests whether the current term is a a pair of the same terms. The equal(|x) strategy tests whether the current term is equal to the argument term. < equal >( \"a\" , \"a\" ) // succeeds < equal >( \"a\" , \"b\" ) // fails < equal (| Foo ( Baz ()))> Foo ( Bar ()) // fails < equal (| Foo ( Bar ()))> Foo ( Bar ()) // succeeds","title":"Matching Terms"},{"location":"references/stratego/strategy-combinators/#term-variable-scope","text":"{ $ Id , ... : $ StrategyExp } Once a variable is bound it cannot be rebound to a different term. Thus, when we have applied an anonymous rule once, its variables are bound and the next time it is applied it only succeeds for the same term. For example, in the next session the second application of the rule fails, because e2 is bound to Int(\"3\") and does not match with Var(\"b\") . < ? Plus ( e1 , e2 ) ! Plus ( e2 , e1 ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) // e1 is bound to Var(\"a\") // e2 is bound to Int(\"3\") < ? Plus ( e1 , e2 ) ! Plus ( e2 , e1 )> Plus ( Var ( \"a\" ), Var ( \"b\" )) // fails To use a variable name more than once Stratego provides term variable scope. A scope {x1,...,xn : s} locally undefines the variables xi . That is, the binding to a variable xi outside the scope is not visible inside it, nor is the binding to xi inside the scope visible outside it. For example, to continue the session above, if we wrap the anonymous swap rule in a scope for its variables, it can be applied multiple times. <{ e3 , e4 : ? Plus ( e3 , e4 ); ! Plus ( e4 , e3 )}> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) // e3 is not bound to a term <{ e3 , e4 : ? Plus ( e3 , e4 ); ! Plus ( e4 , e3 )}> Plus ( Var ( \"a\" ), Var ( \"b\" )) => Plus ( Var ( \"b\" ), Var ( \"a\" )) Of course we can name such a scoped rule using a strategy definition, and then invoke it by its name: SwapArgs = { e1 , e2 : ? Plus ( e1 , e2 ); ! Plus ( e2 , e1 )} < SwapArgs > Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" ))","title":"Term Variable Scope"},{"location":"references/stratego/strategy-combinators/#implicit-variable-scope","text":"When using match and build directly in a strategy definition, rather than in the form of a rule, the definition contains free variables. Strictly speaking such variables should be declared using a scope, that is one should write SwapArgs = { e1 , e2 : ? Plus ( e1 , e2 ); ! Plus ( e2 , e1 )} However, since declaring all variables at the top of a definition is distracting and does not add much to the definition, such a scope declaration can be left out. Thus, one can write SwapArgs = ( Plus ( e1 , e2 ) - > Plus ( e2 , e1 )) instead. The scope is automatically inserted by the compiler. This implies that there is no global scope for term variables. Of course, variables in inner scopes should be declared where necessary. In particular, note that variable scope is not inserted for strategy definitions in a let binding, such as let SwapArgs = ? Plus ( e1 , e2 ); Plus ( e2 , e1 ) in ... end While the variables are bound in the enclosing definition, they are not restricted to SwapArgs in this case, since in a let one typically wants to use bindings to variables in the enclosing code.","title":"Implicit Variable Scope"},{"location":"references/stratego/strategy-combinators/#combining-match-and-build","text":"Match and build are first-class citizens in Stratego, which means that they can be used and combined just like any other strategy expressions. In particular, we can implement rewrite rules using these operations, since a rewrite rule is basically a match followed by a build. For example, consider the following combination of match and build: < ? Plus ( e1 , e2 ); ! Plus ( e2 , e1 )> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" )) This combination first recognizes a term, binds variables to the pattern in the match, and then replaces the current term with the instantiation of the build pattern. Note that the variable bindings are propagated from the match to the build. Stratego provides syntactic sugar for various combinations of match and build.","title":"Combining Match and Build"},{"location":"references/stratego/strategy-combinators/#anonymous-rewrite-rule","text":"( $ Pattern - > $ Pattern ) An anonymous rewrite rule (p1 -> p2) transforms a term matching p1 into an instantiation of p2 . Such a rule is equivalent to the sequence ?p1; !p2 . <( Plus ( e1 , e2 ) - > Plus ( e2 , e1 ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Int ( \"3\" ), Var ( \"a\" ))","title":"Anonymous Rewrite Rule"},{"location":"references/stratego/strategy-combinators/#where","text":"where ( $ StrategyExp ) The where(s) combinator applies s to the current term, but restores that term afterwards. Any bindings to variables are kept, however. The where(s) construct is syntactic sugar for {x: ?x; s; !x} with x a fresh variable not occurring in s . Thus, the current subject term is saved by binding it to a new variable x , then the strategy s is applied, and finally, the original term is restored by building x . Example < where ( ? Plus ( Int ( i ), Int ( j )); < addS >( i , j ) => k )> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Int ( \"14\" ), Int ( \"3\" )) // i is bound to \"14\" // k is bound to \"17\"","title":"Where"},{"location":"references/stratego/strategy-combinators/#with","text":"with ( s ) The strategy with(s) applies s on the current subject term and then restores the current subject term. In other words, s is executed solely for its side effects, such as binding variables. In this respect, with is like where . However, with(s) differs in a key way: if the strategy s fails, Stratego stops with an error, reporting the strategy that failed.","title":"With"},{"location":"references/stratego/strategy-combinators/#lambda-rules","text":"\\ $ Term - > $ Term where $ Condition \\ A lambda rule of the form \\ p1 -> p2 where s \\ is an anonymous rewrite rule for which the variables in the left-hand side p1 are local to the rule, that is, it is equivalent to an expression of the form { x1 , ... , xn : ( p1 - > p2 where s )} with x1 ,\u2026, xn the variables of p1 . This means that any variables used in s and p2 that do not occur in p1 are bound in the context of the rule. Example. < map ( \\ ( x , y ) - > x \\ )> [( 1 , 2 ),( 3 , 4 ),( 5 , 6 )] => [ 1 , 3 , 5 ]","title":"Lambda Rules"},{"location":"references/stratego/strategy-combinators/#apply-and-match","text":"$ StrategyExp => $ Term < $ StrategyExp > $ Term < $ StrategyExp > $ Term => $ Term The operation <s> p captures the notion of applying a strategy to a term, i.e., the scenario !p; s . The operation s => p capture the notion of applying a strategy to the current subject term and then matching the result against the pattern p , i.e., s; ?p . The combined operation <s> p1 => p2 thus captures the notion of applying a strategy to a term p1 and matching the result against p2 , i.e, !p1; s; ?p2 . Example. The conditional rewrite rule EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where ! ( i , j ); addS ; ? k can be reformulated as EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where < addS >( i , j ) => k","title":"Apply and Match"},{"location":"references/stratego/strategy-combinators/#assignment","text":"$ Term := $ Term The strategy p1 := p2 builds p2 and matches the result against p1 , i.e. it is equivalent to !p2; ?p1 . The strategy is often combined with strategy application into p1 := <s>p2 , which is equivalent to <s>p2 => p1 (but more familiar to an audience with an imperative mindset). For example, consider the following rewrite rule EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) with k := < addS >( i , j )","title":"Assignment"},{"location":"references/stratego/strategy-combinators/#applying-strategies-in-build","text":"< $ StrategyExp > $ Term // in build pattern In a build pattern, the application <s>t applies the strategy s to the term t , returning the resulting term. Example. The constant folding rule EvalPlus : Add ( Int ( i ), Int ( j )) - > Int ( k ) where < addS >( i , j ) => k can be simplified by directly applying the addition in the right-hand side: EvalPlus : Add ( Int ( i ), Int ( j )) - > Int (< addS >( i , j )) Example. The following definition of the map(s) strategy applies a strategy to each term in a list: map ( s ) : [] - > [] map ( s ) : [ x | xs ] - > [< s > x | < map ( s )> xs ]","title":"Applying Strategies in Build"},{"location":"references/stratego/strategy-combinators/#term-wrap","text":"< $ StrategyExp > // in build pattern A term wrap is a build strategy !p[<s>] containing one or more strategy applications <s> that are not applied to a term. When executing the the build operation, each occurrence of such a strategy application <s> is replaced with the term resulting from applying s to the current subject term, i.e., the one that is being replaced by the build. Motivation. One often write rules of the form x -> Foo(Bar(x)) , i.e. wrapping a term pattern around the current term. Using rule syntax this is quite verbose. The syntactic abstraction of term wraps, allows the concise specification of such little transformations as !Foo(Bar(<id>)) . Example. The following applications illustrate some uses of term wraps: < ! (< id >,< id >)> 3 => ( 3 , 3 ) <(< Fst ; inc >,< Snd >)> ( 3 , 3 ) => ( 4 , 3 ) < ! Call (< id >, [])> \"foobar\" => Call ( \"foobar\" , []) mod2 = < mod >(< id >, 2 ) < mod2 > 6 => 0 Desugaring. Term wraps are implemented by translation to a combination of match and build expressions. Thus, a term wrap !p[<s>] is translated to a strategy expression { x : where ( s => x ); ! p [ x ]} where x is a fresh variable not occurring in s . In other words, the strategy s is applied to the current subject term, i.e., the term to which the build is applied. As an example, the term wrap !Foo(Bar(<id>)) is desugared to the strategy { x : where ( id => x ); ! Foo ( Bar ( x ))} which after simplification is equivalent to {x: ?x; !Foo(Bar(x))} , i.e., exactly the original lambda rule \\x -> Foo(Bar(x))\\ .","title":"Term Wrap"},{"location":"references/stratego/strategy-combinators/#term-project","text":"< $ StrategyExp > // in match pattern Term projections are the match dual of term wraps. Term projections can be used to project a subterm from a term pattern. Examples. For example, the expression ?And(<id>,x) matches terms of the form And(t1,t2) and reduces them to the first subterm t1 . Another example is the strategy map ( ? FunDec (< id >, _ , _ )) which reduces a list of function declarations to a list of the names of the functions, i.e., the first arguments of the FunDec constructor. Here are some more examples: < ? [ _ |< id >]> [ 1 , 2 , 3 ] => [ 2 , 3 ] < ? Call (< id >, [])> Call ( \"foobar\" , []) => \"foobar\" Term projections can also be used to apply additional constraints to subterms in a match pattern. For example, ?Call(x, <?args; length => 3>) matches only with function calls with three arguments. Desugaring. A match expression ?p[<s>] is desugared as { x : ? p [ x ]; < s > x } That is, after the pattern p[x] matches, it is reduced to the subterm bound to x to which s is applied. The result is also the result of the projection. When multiple projects are used within a match the outcome is undefined, i.e., the order in which the projects will be performed can not be counted on.","title":"Term Project"},{"location":"references/stratego/strategy-combinators/#traversal-combinators","text":"Traversal combinators apply strategies to direct subterms of a term and can be combined with other combinators to define full term traversal strategies.","title":"Traversal Combinators"},{"location":"references/stratego/strategy-combinators/#congruence-operators","text":"$ Constructor ( $ StrategyExp , ... , $ StrategyExp ) A congruence operator applies a strategy to each direct subterm of a specific constructor. For each n-ary constructor c declared in a signature, there is a corresponding congruence operator c(s1 , ..., sn) , which applies to terms of the form c(t1 , ..., tn) by applying the argument strategies to the corresponding argument terms. A congruence fails if the application of one the argument strategies fails or if constructor of the operator and that of the term do not match. Example. Consider the following signature of expressions: module expressions signature sorts Exp constructors Plus : Exp * Exp - > Exp Times : Exp * Exp - > Exp The following applications apply the congruence operators Plus and Times to a term: < Plus ( ! Var ( \"a\" ), id )> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) < Times ( id , ! Int ( \"42\" ))> Plus ( Var ( \"a\" ), Int ( \"3\" )) // fails The first application shows how a congruence transforms a specific subterm, that is the strategy applied can be different for each subterm. The second application shows that a congruence only succeeds for terms constructed with the same constructor.","title":"Congruence Operators"},{"location":"references/stratego/strategy-combinators/#tuple-and-list-congruences","text":"[ $ StrategExp , ... , $ StrategyExp ] [ $ StrategExp , ... , $ StrategyExp | $ StrategyExp ] ( $ StrategExp , ... , $ StrategyExp ) Congruences can also be applied to tuples, (s1,s2,...,sn) , and lists, [s1,s2,...,sn] . Example. The definition of a map(s) strategy using list congruences: map ( s ) = [] < + [ s | map ( s )]","title":"Tuple and List Congruences"},{"location":"references/stratego/strategy-combinators/#visiting-all-subterms","text":"all ( $ StrategyExp ) The all(s) strategy transforms a constructor application by applying the parameter strategy s to each direct subterm. An application of all(s) fails if the application to one of the subterms fails. The following example shows how all (1) applies to any term, and (2) applies its argument strategy uniformly to all direct subterms. < all ( ! Var ( \"a\" ))> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Var ( \"a\" )) < all ( ! Var ( \"z\" ))> Times ( Var ( \"b\" ), Int ( \"3\" )) => Times ( Var ( \"z\" ), Var ( \"z\" )) Example. The bottomup(s) is defined as bottomup ( s ) = all ( bottomup ( s )); s and defines a full traversal over the subject term.","title":"Visiting All Subterms"},{"location":"references/stratego/strategy-combinators/#visiting-one-subterm","text":"one ( $ StrategyExp ) The one(s) strategy transforms a constructor application by applying the parameter strategy s to exactly one direct subterm. An application of one(s) fails if the application to all of the subterms fails. The following applications illustrate the behavior of the combinator: < one ( ! Var ( \"a\" ))> Plus ( Int ( \"14\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"3\" )) < one ( \\ Int ( x ) - > Int (< addS >( x , \"1\" )) \\ )> Plus ( Var ( \"a\" ), Int ( \"3\" )) => Plus ( Var ( \"a\" ), Int ( \"4\" )) < one ( ? Plus ( _ , _ ))> Plus ( Var ( \"a\" ), Int ( \"4\" )) // fails Example. A frequently used application of one is the oncetd(s) traversal, which performs a left to right depth first search/transformation that stops as soon as s has been successfully applied. oncetd ( s ) = s < + one ( oncetd ( s )) Thus, s is first applied to the root of the subject term. If that fails, its direct subterms are searched one by one (from left to right), with a recursive call to oncetd(s) . An application of oncetd is the contains(|t) strategy, which checks whether the subject term contains a subterm that is equal to t. contains (| t ) = oncetd ( ? t ) Through the depth first search of oncetd , either an occurrence of t is found, or all subterms are verified to be unequal to t .","title":"Visiting One Subterm"},{"location":"references/stratego/strategy-combinators/#visiting-some-subterms","text":"some ( $ StrategyExp ) The some(s) strategy transforms a constructor application by applying the parameter strategy s to as many direct subterms as possible and at least one. An application of some(s) fails if the application to all of the subterms fails. Some one-pass traversals based on some: sometd ( s ) = s < + some ( sometd ( s )) somebu ( s ) = some ( somebu ( s )) < + s A fixed-point traversal with some: reduce-par ( s ) = repeat ( rec x ( some ( x ) + s ))","title":"Visiting Some Subterms"},{"location":"references/stratego/strategy-combinators/#generic-term-deconstruction","text":"$ Term # ( $ Term ) // in a match pattern The term pattern expression c#(ts) used in a match pattern succeeds when applied to a constructor application and matches the constructor name (as a string) to c and the list of term arguments to ts .","title":"Generic Term Deconstruction"},{"location":"references/stratego/strategy-combinators/#generic-term-construction","text":"$ Term # ( $ Term ) // in a build pattern The term pattern expression c#(ts) used in a build pattern succeeds when c constructs a string and ts constructs a list of terms. It then builds the corresponding constructor application c(ts) .","title":"Generic Term Construction"},{"location":"references/stratego/strategy-combinators/#references","text":"Rather than defining rewrite rules and high-level strategies as primitives of the language, Stratego provides strategy combinators as basic building blocks from which these can defined 1 . Thus, Stratego consists of a core language 2 and a 'sugar' language defined by reduction to the core language. Warning While it useful to understand the constructs defined in this section, their use should be avoided in favour of the higher-level language constructs, such as rewrite rules , where possible. Eelco Visser, Zine-El-Abidine Benaissa, and Andrew P. Tolmach. Building program optimizers with rewriting strategies. In Matthias Felleisen, Paul Hudak, and Christian Queinnec, editors, Proceedings of the third ACM SIGPLAN international conference on Functional programming , 13\u201326. Baltimore, Maryland, United States, 1998. ACM. URL: http://doi.acm.org/10.1145/289423.289425 , doi:10.1145/289423.289425 . \u21a9 Eelco Visser and Zine-El-Abidine Benaissa. A core language for rewriting. Electronic Notes in Theoretical Computer Science , 15:422\u2013441, 1998. URL: http://dx.doi.org/10.1016/S1571-0661 \\(05\\) 80027-1 , doi:10.1016/S1571-0661 \\(05\\) 80027-1 . \u21a9","title":"References"},{"location":"references/stratego/strategy-definitions/","text":"Strategy Definitions \u00b6 $ Id ( $ StrategyArg , ... | $ TermArg , ... ) = $ StrategyExp A strategy definition gives a name to a strategy expression, has zero or more strategy arguments, and zero or more term arguments. Simple Definitions \u00b6 A simple strategy definition gave a name to a strategy expression . $ Id = s For example, the following definition defines desugar as an application of the innermost strategy to the rewrite rule(s) desugar-exp . strategies desugar :: Module - > Module desugar = innermost ( desugar-exp ) Parameterized Definitions \u00b6 Just like rewrite rules , strategy definitions can be parameterized with strategies and terms. $ Id ( $ StrategyArg , ... | $ TermArg , ... ) :: $ Type - > $ Type $ Id ( s1 , ... | t1 , ... ) = s When a strategy has no term arguments, the bar can be left out: $ Id ( $ StrategyArg , ... ) :: $ Type - > $ Type $ Id ( s1 , ... ) = s Simple strategy definitions are the special case in which a strategy does not have strategy and term arguments. For example, the following definition defines topdown(s) in terms of sequential composition and generic traversal: topdown ( TP ) :: TP topdown ( s ) = s ; all ( topdown ( s )) Extending Definitions \u00b6 Just like rewrite rules, strategy definitions can have multiple definitions. In case a strategy expression fails to apply, the next definition is applied. When definitions are in the same module, definitions are applied in the textual order they are defined in. When definitions are defined in separate modules, the order is undefined. External Definitions \u00b6 external definitions libraries Todo finish this section on external definitions Local Definitions \u00b6 Todo finish this section on local definitions","title":"Strategy Definitions"},{"location":"references/stratego/strategy-definitions/#strategy-definitions","text":"$ Id ( $ StrategyArg , ... | $ TermArg , ... ) = $ StrategyExp A strategy definition gives a name to a strategy expression, has zero or more strategy arguments, and zero or more term arguments.","title":"Strategy Definitions"},{"location":"references/stratego/strategy-definitions/#simple-definitions","text":"A simple strategy definition gave a name to a strategy expression . $ Id = s For example, the following definition defines desugar as an application of the innermost strategy to the rewrite rule(s) desugar-exp . strategies desugar :: Module - > Module desugar = innermost ( desugar-exp )","title":"Simple Definitions"},{"location":"references/stratego/strategy-definitions/#parameterized-definitions","text":"Just like rewrite rules , strategy definitions can be parameterized with strategies and terms. $ Id ( $ StrategyArg , ... | $ TermArg , ... ) :: $ Type - > $ Type $ Id ( s1 , ... | t1 , ... ) = s When a strategy has no term arguments, the bar can be left out: $ Id ( $ StrategyArg , ... ) :: $ Type - > $ Type $ Id ( s1 , ... ) = s Simple strategy definitions are the special case in which a strategy does not have strategy and term arguments. For example, the following definition defines topdown(s) in terms of sequential composition and generic traversal: topdown ( TP ) :: TP topdown ( s ) = s ; all ( topdown ( s ))","title":"Parameterized Definitions"},{"location":"references/stratego/strategy-definitions/#extending-definitions","text":"Just like rewrite rules, strategy definitions can have multiple definitions. In case a strategy expression fails to apply, the next definition is applied. When definitions are in the same module, definitions are applied in the textual order they are defined in. When definitions are defined in separate modules, the order is undefined.","title":"Extending Definitions"},{"location":"references/stratego/strategy-definitions/#external-definitions","text":"external definitions libraries Todo finish this section on external definitions","title":"External Definitions"},{"location":"references/stratego/strategy-definitions/#local-definitions","text":"Todo finish this section on local definitions","title":"Local Definitions"},{"location":"references/stratego/terms/","text":"Terms \u00b6 Stratego programs transform terms. For example, the code 4 + f(5 * x) might be represented in a term as: Plus ( Int ( \"4\" ), Call ( \"f\" , [ Mul ( Int ( \"5\" ), Var ( \"x\" ))])) Term Forms \u00b6 Terms are constructed from the following forms. Integer \u00b6 $ Digit + An integer constant, that is a list of decimal digits, is an ATerm. Examples: 1 , 12343 . String \u00b6 \"$Char*\" A string constant, that is a list of characters between double quotes is an ATerm. Special characters such as double quotes and newlines should be escaped using a backslash. The backslash character itself should be escaped as well. Examples: \"foobar\" , \"string with quotes\\\"\" , \"escaped escape character\\\\ and a newline\\n\" . String Templates \u00b6 $ [ $ TemplateChar * ] Multiline strings can be constructed using string templates. $ [ if [ code1 ] then [ code2 ]] The indentation will be computed relative to the start of the template. String templates can also include escapes to term expressions producing strings or integers. For example, the string above includes escapes to include code1 and code2 and the following error message $ [ error : variable [ x ] is not defined ] includes the name of the variable x . Constructor application \u00b6 $ Constructor ( $ Term , ... , $ Term ) A constructor is an identifier, that is an alphanumeric string starting with a letter, or a double quoted string. A constructor application c(t1,...,tn) creates a term by applying a constructor to a sequence of zero or more terms. For example, the term Plus(Int(\"4\"),Var(\"x\")) uses the constructors Plus , Int , and Var to create a nested term from the strings \"4\" and \"x\" . The parentheses are needed even when a constructor has no subterms, in order to avoid ambiguity with variables . Thus, True() is a constructor application, but True is a variable. List \u00b6 [ $ Term , ... , $ Term ] A list is a term of the form [t1,...,tn] , that is a list of zero or more terms between square brackets. While all applications of a specific constructor typically have the same number of subterms, lists can have a variable number of subterms. The elements of a list are typically of the same type, while the subterms of a constructor application can vary in type. Example: The second argument of the call to \"f\" in the term Call(\"f\",[Int(\"5\"),Var(\"x\")]) is a list of expressions. Tuple \u00b6 ( $ Term , ... , $ Term ) A tuple (t1,...,tn) is a constructor application without a constructor. Example: (Var(\"x\"), Type(\"int\")) Annotation \u00b6 $ PreTerm { $ Term , ... , $ Term } Any of the term forms above can be annotated with a list of terms. Example: Lt(Var(\"n\"),Int(\"1\")){Type(\"bool\")} . Only 'preterms', i.e. terms without annotations, can be annotated. The form Var(\"a\"){Type(\"bool\")}{Value(3)} is syntactically incorrect. Term Patterns \u00b6 A term pattern , is a term extended with variables . In the term pattern Plus ( e , Int ( \"0\" )) the identifier e is a variable that stands for any term. Linear vs Non-Linear \u00b6 A pattern is linear if each variable occurs at most once, non-linear otherwise. The non-linear pattern Plus ( e , e ) stands for a Plus term with identical arguments. A term pattern without variables (aka term) is ground . Substitution \u00b6 Substitution is the process of applying a map from variables to terms to a term pattern, replacing occurrence of variables in the domain of the map with the corresponding terms in the co-domain of the map. Substitution is also the name for the mapping of variables to terms. Pattern Matching \u00b6 Pattern matching is the process of matching a ground term against a term pattern. A term t matches a term pattern p iff there is a substitution S such that applying the substitution to the pattern S(p) yields the term t . Persistent Representation \u00b6 The term format described above is used in Stratego programs to denote terms, but is also used to exchange terms between programs. Thus, the internal format and the external format exactly coincide. Of course, internally a Stratego program uses a data-structure in memory with pointers rather than manipulating a textual representation of terms. But this is completely hidden from the Stratego programmer. Todo API for reading, writing terms? Namespaces \u00b6 Currently, the constructors of terms live in a global namespace. In the future, we want to support qualified names. References \u00b6 Terms in Stratego are inspired by terms in the Annotated Term Format , or ATerms for short 1 . The ATerm format provides a set of constructs for representing trees, comparable to XML or abstract data types in functional programming languages. Mark G. J. van den Brand, H. A. de Jong, Paul Klint, and Pieter A. Olivier. Efficient annotated terms. Software: Practice and Experience , 30 \\(3\\) :259\u2013291, 2000. \u21a9","title":"Terms"},{"location":"references/stratego/terms/#terms","text":"Stratego programs transform terms. For example, the code 4 + f(5 * x) might be represented in a term as: Plus ( Int ( \"4\" ), Call ( \"f\" , [ Mul ( Int ( \"5\" ), Var ( \"x\" ))]))","title":"Terms"},{"location":"references/stratego/terms/#term-forms","text":"Terms are constructed from the following forms.","title":"Term Forms"},{"location":"references/stratego/terms/#integer","text":"$ Digit + An integer constant, that is a list of decimal digits, is an ATerm. Examples: 1 , 12343 .","title":"Integer"},{"location":"references/stratego/terms/#string","text":"\"$Char*\" A string constant, that is a list of characters between double quotes is an ATerm. Special characters such as double quotes and newlines should be escaped using a backslash. The backslash character itself should be escaped as well. Examples: \"foobar\" , \"string with quotes\\\"\" , \"escaped escape character\\\\ and a newline\\n\" .","title":"String"},{"location":"references/stratego/terms/#string-templates","text":"$ [ $ TemplateChar * ] Multiline strings can be constructed using string templates. $ [ if [ code1 ] then [ code2 ]] The indentation will be computed relative to the start of the template. String templates can also include escapes to term expressions producing strings or integers. For example, the string above includes escapes to include code1 and code2 and the following error message $ [ error : variable [ x ] is not defined ] includes the name of the variable x .","title":"String Templates"},{"location":"references/stratego/terms/#constructor-application","text":"$ Constructor ( $ Term , ... , $ Term ) A constructor is an identifier, that is an alphanumeric string starting with a letter, or a double quoted string. A constructor application c(t1,...,tn) creates a term by applying a constructor to a sequence of zero or more terms. For example, the term Plus(Int(\"4\"),Var(\"x\")) uses the constructors Plus , Int , and Var to create a nested term from the strings \"4\" and \"x\" . The parentheses are needed even when a constructor has no subterms, in order to avoid ambiguity with variables . Thus, True() is a constructor application, but True is a variable.","title":"Constructor application"},{"location":"references/stratego/terms/#list","text":"[ $ Term , ... , $ Term ] A list is a term of the form [t1,...,tn] , that is a list of zero or more terms between square brackets. While all applications of a specific constructor typically have the same number of subterms, lists can have a variable number of subterms. The elements of a list are typically of the same type, while the subterms of a constructor application can vary in type. Example: The second argument of the call to \"f\" in the term Call(\"f\",[Int(\"5\"),Var(\"x\")]) is a list of expressions.","title":"List"},{"location":"references/stratego/terms/#tuple","text":"( $ Term , ... , $ Term ) A tuple (t1,...,tn) is a constructor application without a constructor. Example: (Var(\"x\"), Type(\"int\"))","title":"Tuple"},{"location":"references/stratego/terms/#annotation","text":"$ PreTerm { $ Term , ... , $ Term } Any of the term forms above can be annotated with a list of terms. Example: Lt(Var(\"n\"),Int(\"1\")){Type(\"bool\")} . Only 'preterms', i.e. terms without annotations, can be annotated. The form Var(\"a\"){Type(\"bool\")}{Value(3)} is syntactically incorrect.","title":"Annotation"},{"location":"references/stratego/terms/#term-patterns","text":"A term pattern , is a term extended with variables . In the term pattern Plus ( e , Int ( \"0\" )) the identifier e is a variable that stands for any term.","title":"Term Patterns"},{"location":"references/stratego/terms/#linear-vs-non-linear","text":"A pattern is linear if each variable occurs at most once, non-linear otherwise. The non-linear pattern Plus ( e , e ) stands for a Plus term with identical arguments. A term pattern without variables (aka term) is ground .","title":"Linear vs Non-Linear"},{"location":"references/stratego/terms/#substitution","text":"Substitution is the process of applying a map from variables to terms to a term pattern, replacing occurrence of variables in the domain of the map with the corresponding terms in the co-domain of the map. Substitution is also the name for the mapping of variables to terms.","title":"Substitution"},{"location":"references/stratego/terms/#pattern-matching","text":"Pattern matching is the process of matching a ground term against a term pattern. A term t matches a term pattern p iff there is a substitution S such that applying the substitution to the pattern S(p) yields the term t .","title":"Pattern Matching"},{"location":"references/stratego/terms/#persistent-representation","text":"The term format described above is used in Stratego programs to denote terms, but is also used to exchange terms between programs. Thus, the internal format and the external format exactly coincide. Of course, internally a Stratego program uses a data-structure in memory with pointers rather than manipulating a textual representation of terms. But this is completely hidden from the Stratego programmer. Todo API for reading, writing terms?","title":"Persistent Representation"},{"location":"references/stratego/terms/#namespaces","text":"Currently, the constructors of terms live in a global namespace. In the future, we want to support qualified names.","title":"Namespaces"},{"location":"references/stratego/terms/#references","text":"Terms in Stratego are inspired by terms in the Annotated Term Format , or ATerms for short 1 . The ATerm format provides a set of constructs for representing trees, comparable to XML or abstract data types in functional programming languages. Mark G. J. van den Brand, H. A. de Jong, Paul Klint, and Pieter A. Olivier. Efficient annotated terms. Software: Practice and Experience , 30 \\(3\\) :259\u2013291, 2000. \u21a9","title":"References"},{"location":"references/stratego/troubleshooting/","text":"Stratego Troubleshooting \u00b6 UnsupportedClassVersionError: InteropRegisterer \u00b6 In Eclipse, if you get this error: Caused by: java.lang.UnsupportedClassVersionError: MyLanguage/strategies/InteropRegisterer has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0 It is caused by Eclipse internally compiling using the wrong Java version. Go to the Eclipse Preferences ( Cmd + , on macOS), Java \u2023 Compiler and set the Compiler Compliance Level to 1.8 (which corresponds to class file version 52.0).","title":"Troubleshooting"},{"location":"references/stratego/troubleshooting/#stratego-troubleshooting","text":"","title":"Stratego Troubleshooting"},{"location":"references/stratego/troubleshooting/#unsupportedclassversionerror-interopregisterer","text":"In Eclipse, if you get this error: Caused by: java.lang.UnsupportedClassVersionError: MyLanguage/strategies/InteropRegisterer has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0 It is caused by Eclipse internally compiling using the wrong Java version. Go to the Eclipse Preferences ( Cmd + , on macOS), Java \u2023 Compiler and set the Compiler Compliance Level to 1.8 (which corresponds to class file version 52.0).","title":"UnsupportedClassVersionError: InteropRegisterer"},{"location":"references/stratego/types/","text":"Types \u00b6 Terms provide a generic, untyped format to represent tree-structured data. Stratego transformations can transform such data, but require at least that the arities of term constructors that are used in rules are declared. Starting with Stratego 2, the types of terms and term transformations may be declared and checked in more detail 1 . Signatures \u00b6 signature $ SigSection * A signature declares the shape of well-formed terms using sort declarations, constructor declarations, and overlays. Sorts \u00b6 sorts $ Sort * A sort is determined by an identifier and optionally has arguments. A sort (or type ) identifies a collection of well-formed terms. Convention: Sort identifiers start with a capital letter. Constructors \u00b6 constructors $ ConstructorDecl A constructor declaration has the form $ Constructor : $ Sort * ... * $ Sort - > Decl and declares a constructor name, the sorts of the argument terms, and the sort of the constructed term. Convention: Constructor identifiers start with a capital letter. Example: The constructor declaration Assign : ID * Exp - > Stmt defines the constructor Assign with input sorts ID and Exp and output sort Stmt . Thus, if x and e are terms of sort ID and Exp , respectively, then Assign(x, e) is a term of sort Stmt . When the list of argument sorts is empty the arrow can be omitted: $ Constructor : Decl Example: The constructor declaration True : Bool defines that True() is a term of sort Bool . Note that the parentheses are required. The term True is a variable. The Stratego1 compiler only checks the arity of constructor applications against the signature. The Stratego2 compiler uses signature definitions to type check code if it has been given a type signature. Injections \u00b6 An injection is a constructor without name and with a single argument. : $ Sort - > $ Sort Injections include an entire type as a subtype of another type without cluttering the tree structure. List Type \u00b6 List ( $ Sort ) The type constructor List(_) is for typing homogenous lists. Exmample. The type List(Exp) represents lists of expressions Exp . Polymorphic Types \u00b6 Stratego2 supports user-defined polymorphic types. That is, sorts can have parameters. For example, the following signature defines the type of priority queues, polymorphic in the carrier type, in which the priority is determined by the length of the list. signature sorts PrioQ ( * ) constructors NilQ : PrioQ ( a ) ConsQ : a * int * List ( a ) * PrioQ ( a ) - > PrioQ ( a ) Tuple Type \u00b6 ( $ TermType * ... * $ TermType ) Tuple terms can be typed in strategy types. Currently, tuple types cannot be used in term signatures. Overlays \u00b6 overlays $ OverlayDef An overlay defines a term abbreviation. An overlay definition has the form $ Constructor ( $ ID , ... , $ ID ) = $ Term and defines that applications of the constructor should be expanded to the term. Transformation Types \u00b6 $ Id ( $ StrategyType , ... | $ TermType , ... ) :: $ TermType - > $ TermType A transformation type defines the signature of a transformation with name $Id with the types of its strategy arguments and term arguments, the type of the 'current term' to which the transformation is applied, and the type of the term that is returned, if the transformation succeeds. Transformation types are declared in rules or strategies sections. $ Id ( $ StrategyType , ... ) :: $ TermType - > $ TermType When a transformation only has strategy parameters, the bar can be left out. $ Id :: $ TermType - > $ TermType When a transformation also has no strategy parameters, the parentheses can be left out as well. Strategy Type \u00b6 $ TermType - > $ TermType The type of a transformation/strategy argument is an arrow from a term type to a term type. Note that transformation strategies cannot be reified as terms. Type Dynamic \u00b6 ? Type dynamic , written ? , represents the unknown type. Stratego2 is a gradually typed language in order to facilitate the migration from (mostly) untyped Stratego1 code to typed Stratego2 code. Furthermore, some patterns in Stratego cannot be typed statically. When used as a strategy type ? represents ? -> ? . Type Casts \u00b6 ///syntax of casts Todo syntactic form Gradual type systems allow a term with the dynamic type to be used in any place where a static type is required. Stratego2 will insert a type cast at such a point to check at run time that the term is type-correct. This way, a Stratego program halts execution in predictable places when a run time type error occurs. There can be no run time type errors in fully statically typed code either, only at the boundary between dynamically and statically typed code. Type Preserving \u00b6 TP A type preserving transformation transforms any type to itself (or fails). In signatures, a type preserving transformation is indicated with TP . Example. The type declaration topdown ( s : TP ) :: TP declares that the topdown strategy is type preserving if its argument strategy is. The type-checking for a type preserving transformation is very strict. It should be in terms of other type preserving transformations, or match the input term to a specific type and return a term from that specific type. Is Type \u00b6 is ( $ Sort ) Given the definition of a term sort S , the is(S) strategy checks whether a term is of sort S and fails if that is not the case. Example. The strategy <is(Exp)>t checks that term t conforms to the signature of sort Exp . The is(S) strategy uses the same mechanism as type casts for checking a term type at run time. References \u00b6 Jeff Smits and Eelco Visser. Gradually typing strategies. In Ralf L\u00e4mmel, Laurence Tratt, and Juan de Lara, editors, Proceedings of the 13 th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2020, Virtual Event, USA, November 16-17, 2020 , 1\u201315. ACM, 2020. URL: https://doi.org/10.1145/3426425.3426928 , doi:10.1145/3426425.3426928 . \u21a9","title":"Types"},{"location":"references/stratego/types/#types","text":"Terms provide a generic, untyped format to represent tree-structured data. Stratego transformations can transform such data, but require at least that the arities of term constructors that are used in rules are declared. Starting with Stratego 2, the types of terms and term transformations may be declared and checked in more detail 1 .","title":"Types"},{"location":"references/stratego/types/#signatures","text":"signature $ SigSection * A signature declares the shape of well-formed terms using sort declarations, constructor declarations, and overlays.","title":"Signatures"},{"location":"references/stratego/types/#sorts","text":"sorts $ Sort * A sort is determined by an identifier and optionally has arguments. A sort (or type ) identifies a collection of well-formed terms. Convention: Sort identifiers start with a capital letter.","title":"Sorts"},{"location":"references/stratego/types/#constructors","text":"constructors $ ConstructorDecl A constructor declaration has the form $ Constructor : $ Sort * ... * $ Sort - > Decl and declares a constructor name, the sorts of the argument terms, and the sort of the constructed term. Convention: Constructor identifiers start with a capital letter. Example: The constructor declaration Assign : ID * Exp - > Stmt defines the constructor Assign with input sorts ID and Exp and output sort Stmt . Thus, if x and e are terms of sort ID and Exp , respectively, then Assign(x, e) is a term of sort Stmt . When the list of argument sorts is empty the arrow can be omitted: $ Constructor : Decl Example: The constructor declaration True : Bool defines that True() is a term of sort Bool . Note that the parentheses are required. The term True is a variable. The Stratego1 compiler only checks the arity of constructor applications against the signature. The Stratego2 compiler uses signature definitions to type check code if it has been given a type signature.","title":"Constructors"},{"location":"references/stratego/types/#injections","text":"An injection is a constructor without name and with a single argument. : $ Sort - > $ Sort Injections include an entire type as a subtype of another type without cluttering the tree structure.","title":"Injections"},{"location":"references/stratego/types/#list-type","text":"List ( $ Sort ) The type constructor List(_) is for typing homogenous lists. Exmample. The type List(Exp) represents lists of expressions Exp .","title":"List Type"},{"location":"references/stratego/types/#polymorphic-types","text":"Stratego2 supports user-defined polymorphic types. That is, sorts can have parameters. For example, the following signature defines the type of priority queues, polymorphic in the carrier type, in which the priority is determined by the length of the list. signature sorts PrioQ ( * ) constructors NilQ : PrioQ ( a ) ConsQ : a * int * List ( a ) * PrioQ ( a ) - > PrioQ ( a )","title":"Polymorphic Types"},{"location":"references/stratego/types/#tuple-type","text":"( $ TermType * ... * $ TermType ) Tuple terms can be typed in strategy types. Currently, tuple types cannot be used in term signatures.","title":"Tuple Type"},{"location":"references/stratego/types/#overlays","text":"overlays $ OverlayDef An overlay defines a term abbreviation. An overlay definition has the form $ Constructor ( $ ID , ... , $ ID ) = $ Term and defines that applications of the constructor should be expanded to the term.","title":"Overlays"},{"location":"references/stratego/types/#transformation-types","text":"$ Id ( $ StrategyType , ... | $ TermType , ... ) :: $ TermType - > $ TermType A transformation type defines the signature of a transformation with name $Id with the types of its strategy arguments and term arguments, the type of the 'current term' to which the transformation is applied, and the type of the term that is returned, if the transformation succeeds. Transformation types are declared in rules or strategies sections. $ Id ( $ StrategyType , ... ) :: $ TermType - > $ TermType When a transformation only has strategy parameters, the bar can be left out. $ Id :: $ TermType - > $ TermType When a transformation also has no strategy parameters, the parentheses can be left out as well.","title":"Transformation Types"},{"location":"references/stratego/types/#strategy-type","text":"$ TermType - > $ TermType The type of a transformation/strategy argument is an arrow from a term type to a term type. Note that transformation strategies cannot be reified as terms.","title":"Strategy Type"},{"location":"references/stratego/types/#type-dynamic","text":"? Type dynamic , written ? , represents the unknown type. Stratego2 is a gradually typed language in order to facilitate the migration from (mostly) untyped Stratego1 code to typed Stratego2 code. Furthermore, some patterns in Stratego cannot be typed statically. When used as a strategy type ? represents ? -> ? .","title":"Type Dynamic"},{"location":"references/stratego/types/#type-casts","text":"///syntax of casts Todo syntactic form Gradual type systems allow a term with the dynamic type to be used in any place where a static type is required. Stratego2 will insert a type cast at such a point to check at run time that the term is type-correct. This way, a Stratego program halts execution in predictable places when a run time type error occurs. There can be no run time type errors in fully statically typed code either, only at the boundary between dynamically and statically typed code.","title":"Type Casts"},{"location":"references/stratego/types/#type-preserving","text":"TP A type preserving transformation transforms any type to itself (or fails). In signatures, a type preserving transformation is indicated with TP . Example. The type declaration topdown ( s : TP ) :: TP declares that the topdown strategy is type preserving if its argument strategy is. The type-checking for a type preserving transformation is very strict. It should be in terms of other type preserving transformations, or match the input term to a specific type and return a term from that specific type.","title":"Type Preserving"},{"location":"references/stratego/types/#is-type","text":"is ( $ Sort ) Given the definition of a term sort S , the is(S) strategy checks whether a term is of sort S and fails if that is not the case. Example. The strategy <is(Exp)>t checks that term t conforms to the signature of sort Exp . The is(S) strategy uses the same mechanism as type casts for checking a term type at run time.","title":"Is Type"},{"location":"references/stratego/types/#references","text":"Jeff Smits and Eelco Visser. Gradually typing strategies. In Ralf L\u00e4mmel, Laurence Tratt, and Juan de Lara, editors, Proceedings of the 13 th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2020, Virtual Event, USA, November 16-17, 2020 , 1\u201315. ACM, 2020. URL: https://doi.org/10.1145/3426425.3426928 , doi:10.1145/3426425.3426928 . \u21a9","title":"References"},{"location":"references/syntax/","text":"SDF3 \u00b6 SDF3 is the meta-language in Spoofax for syntax definition. A syntax definition is structured as a collection of modules , which may import each other. Symbols are the building blocks of productions . Productions are defined for lexical , context-free , or kernel syntax. Start symbols indicate the entry point of a syntax definition. SDF3 automatically generates a pretty-printer for template -based productions. Grammars can be disambiguated by means of rejects, priorities, associativity, and restrictions. SDF3 provides additional constructs for the definition of layout-sensitivite languages. Permissive grammars are automatically generated for error-recovery parsing. Handwritten recovery rules can be added to tweak recovery behavior. Several aspects related to syntax definition and parsing can be configured in the metaborg.yaml. file. Source \u00b6 The sources of the SDF3 implementation can be found at https://github.com/metaborg/sdf/tree/master/org.metaborg.meta.lang.template : The SDF3 language implementation (SDF3 was called TemplateLang before and it has not been renamed everywhere yet)","title":"SDF3"},{"location":"references/syntax/#sdf3","text":"SDF3 is the meta-language in Spoofax for syntax definition. A syntax definition is structured as a collection of modules , which may import each other. Symbols are the building blocks of productions . Productions are defined for lexical , context-free , or kernel syntax. Start symbols indicate the entry point of a syntax definition. SDF3 automatically generates a pretty-printer for template -based productions. Grammars can be disambiguated by means of rejects, priorities, associativity, and restrictions. SDF3 provides additional constructs for the definition of layout-sensitivite languages. Permissive grammars are automatically generated for error-recovery parsing. Handwritten recovery rules can be added to tweak recovery behavior. Several aspects related to syntax definition and parsing can be configured in the metaborg.yaml. file.","title":"SDF3"},{"location":"references/syntax/#source","text":"The sources of the SDF3 implementation can be found at https://github.com/metaborg/sdf/tree/master/org.metaborg.meta.lang.template : The SDF3 language implementation (SDF3 was called TemplateLang before and it has not been renamed everywhere yet)","title":"Source"},{"location":"references/syntax/configuration/","text":"Configuration \u00b6 When using SDF3 inside Spoofax, several configuration options are relevant. They allow using the new parser generator, specifying the shape of completion placeholders, or disable SDF altogether. These options should be specified in the metaborg.yaml file. For example, to disable SDF for the current project, use: language : sdf : enabled : false SDF3 allows generating placeholders for code completion. The default \"shape\" of placeholders is $Symbol . However, it is possible to tweak this shape using the configuration below (the configuration for suffix is optional): language : sdf : placeholder : prefix : \"$\" suffix : \"$\" Currently, the path to the parse table is specified in the Syntax.esv file, commonly as table: target/metaborg/sdf.tbl . When the ESV file does not contain this entry, it is also possible to specify the path to the parse table in the metaborg.yaml file. This is useful when testing an external parse table, or using a parse table different from the one being generated in the project. In the example below, the table is loaded from the path tables/sdf.tbl . The same can be applied to the parse table used for code completion. language : sdf : parse-table : \"tables/sdf.tbl\" completion-parse-table : \"tables/sdf-completions.tbl\" In a Spoofax project, it is also possible to use SDF2 instead of SDF3. This enables SDF2 tools such as the SDF2 parenthesizer, signature generator, etc. For example: language : sdf : version : sdf2 By default SDF3 compilation works by generating SDF2 files, and depending on the SDF2 toolchain. However, a new (and experimental) parse table generator can be selected by writing: language : sdf : sdf2table : java This configuration disables the SDF2 generation, and may cause problems when defining grammars to use concrete syntax, since this feature is not supported yet by SDF3. However, the java parse table generator supports Unicode, whereas SDF2 generation does not. Furthermore, dynamic can be used instead of java , to enable lazy parse table generation, where the parse table is generated while the program is parsed. A namespaced grammar can be generated automatically from an SDF3 grammar. This namespacing is done by adding the language name to all module names and sort names. The generated grammar is put in src-gen/syntax . The configuration to enable this is: language : sdf : generate-namespaced : true Note that namespacing doesn't not handle imports of grammar files from other projects very well. JSGLR version \u00b6 An experimental new version of the SGLR parser implementation is available: JSGLR2. It supports parsing, imploding and syntax highlighting. Error reporting, recovery and completions are currently not supported. It can be enabled with: language : sdf : jsglr-version : v2 There are some extensions of JSGLR2 available. To use them, change the jsglr-version by replacing v2 with one of the following: data-dependent : Data-dependent JSGLR2 solves deep priority conflicts using data-dependent parsing, which does not require duplicating the grammar productions. incremental : Incremental JSGLR2 reuses previous parse results to speed up parsing. layout-sensitive : Layout-sensitive JSGLR2, see Layout Sensitivity . recovery : JSGLR2 with recovery tries to recover from parse errors. This extension is experimental. recovery-incremental : Incremental JSGLR2 with recovery. This extension is experimental. JSGLR2 logging \u00b6 Logging is available for JSGLR2. It can be enabled with: language : sdf : jsglr2-logging : all Since logging all parsing events is quite verbose, several other scopes are available in addition to the all option: none : Log nothing (default). minimal : Only log the start and end of a parse, including a measurement of total parse time (including imploding and tokenization). parsing : Log all standard parsing events (such as stack and parse forest operations, action execution, etc.) but no variant-specific events (e.g. related to recovery). recovery : Log the recovery iterations and the recovery productions that are applied.","title":"Configuration"},{"location":"references/syntax/configuration/#configuration","text":"When using SDF3 inside Spoofax, several configuration options are relevant. They allow using the new parser generator, specifying the shape of completion placeholders, or disable SDF altogether. These options should be specified in the metaborg.yaml file. For example, to disable SDF for the current project, use: language : sdf : enabled : false SDF3 allows generating placeholders for code completion. The default \"shape\" of placeholders is $Symbol . However, it is possible to tweak this shape using the configuration below (the configuration for suffix is optional): language : sdf : placeholder : prefix : \"$\" suffix : \"$\" Currently, the path to the parse table is specified in the Syntax.esv file, commonly as table: target/metaborg/sdf.tbl . When the ESV file does not contain this entry, it is also possible to specify the path to the parse table in the metaborg.yaml file. This is useful when testing an external parse table, or using a parse table different from the one being generated in the project. In the example below, the table is loaded from the path tables/sdf.tbl . The same can be applied to the parse table used for code completion. language : sdf : parse-table : \"tables/sdf.tbl\" completion-parse-table : \"tables/sdf-completions.tbl\" In a Spoofax project, it is also possible to use SDF2 instead of SDF3. This enables SDF2 tools such as the SDF2 parenthesizer, signature generator, etc. For example: language : sdf : version : sdf2 By default SDF3 compilation works by generating SDF2 files, and depending on the SDF2 toolchain. However, a new (and experimental) parse table generator can be selected by writing: language : sdf : sdf2table : java This configuration disables the SDF2 generation, and may cause problems when defining grammars to use concrete syntax, since this feature is not supported yet by SDF3. However, the java parse table generator supports Unicode, whereas SDF2 generation does not. Furthermore, dynamic can be used instead of java , to enable lazy parse table generation, where the parse table is generated while the program is parsed. A namespaced grammar can be generated automatically from an SDF3 grammar. This namespacing is done by adding the language name to all module names and sort names. The generated grammar is put in src-gen/syntax . The configuration to enable this is: language : sdf : generate-namespaced : true Note that namespacing doesn't not handle imports of grammar files from other projects very well.","title":"Configuration"},{"location":"references/syntax/configuration/#jsglr-version","text":"An experimental new version of the SGLR parser implementation is available: JSGLR2. It supports parsing, imploding and syntax highlighting. Error reporting, recovery and completions are currently not supported. It can be enabled with: language : sdf : jsglr-version : v2 There are some extensions of JSGLR2 available. To use them, change the jsglr-version by replacing v2 with one of the following: data-dependent : Data-dependent JSGLR2 solves deep priority conflicts using data-dependent parsing, which does not require duplicating the grammar productions. incremental : Incremental JSGLR2 reuses previous parse results to speed up parsing. layout-sensitive : Layout-sensitive JSGLR2, see Layout Sensitivity . recovery : JSGLR2 with recovery tries to recover from parse errors. This extension is experimental. recovery-incremental : Incremental JSGLR2 with recovery. This extension is experimental.","title":"JSGLR version"},{"location":"references/syntax/configuration/#jsglr2-logging","text":"Logging is available for JSGLR2. It can be enabled with: language : sdf : jsglr2-logging : all Since logging all parsing events is quite verbose, several other scopes are available in addition to the all option: none : Log nothing (default). minimal : Only log the start and end of a parse, including a measurement of total parse time (including imploding and tokenization). parsing : Log all standard parsing events (such as stack and parse forest operations, action execution, etc.) but no variant-specific events (e.g. related to recovery). recovery : Log the recovery iterations and the recovery productions that are applied.","title":"JSGLR2 logging"},{"location":"references/syntax/context-free-syntax/","text":"Context-Free Syntax \u00b6 The context-free syntax describes the more high-level syntactic structure of sentences in a language. A context-free syntax contains a list of productions. Elements of the right-hand side of a context-free production are pre-processed in a normalization step before parser generation that adds the LAYOUT? symbol between any two symbols. Context-free syntax has the form: context-free syntax $Production* An example production rule: context-free syntax Block.Block = \"{\" Statement* \"}\" SDF3 automatically allows for layout to be present between the symbols of a rule. This means that a fragment such as: { } will still be recognized as a block (assuming that the newline and line-feed characters are defined as layout).","title":"Context-Free Syntax"},{"location":"references/syntax/context-free-syntax/#context-free-syntax","text":"The context-free syntax describes the more high-level syntactic structure of sentences in a language. A context-free syntax contains a list of productions. Elements of the right-hand side of a context-free production are pre-processed in a normalization step before parser generation that adds the LAYOUT? symbol between any two symbols. Context-free syntax has the form: context-free syntax $Production* An example production rule: context-free syntax Block.Block = \"{\" Statement* \"}\" SDF3 automatically allows for layout to be present between the symbols of a rule. This means that a fragment such as: { } will still be recognized as a block (assuming that the newline and line-feed characters are defined as layout).","title":"Context-Free Syntax"},{"location":"references/syntax/disambiguation/","text":"Disambiguation \u00b6 The semantics of SDF3 can be seen as two-staged. First, the grammar generates all possible derivations. Second, the disambiguation constructs remove a number of derivations that are not valid. Note that SDF3 actually performs some disambiguation both when generating the parse table as during parsing. Rejections \u00b6 Rejections filter derivations. The semantics of a rejection is that the set of valid derivations for the left-hand side of the production will not contain the construction described on the right-hand side. In other words, the language defined by the sort on the left-hand side has become smaller, removing all the constructions generated by the rule on the right-hand side. Disambiguation by reject occurs at parse time (mostly). A rule can be marked as rejected by using the attribute {reject} after the rule: $Sort = ... {reject} The {reject} attribute works well for lexical rejections, especially keyword reservation in the form of productions like: ID = \"keyword\" {reject} Preferences \u00b6 The preferences mechanism is another disambiguation filter that provides a post parse filter to parse forests. The attributes prefer and avoid are the only disambiguation constructs that compare alternative derivations after parsing. Warning prefer and avoid are deprecated and will be removed in a future version of Spoofax. The following definition assumes that derivations are represented using parse forests with \"packaged ambiguity nodes\". This means that whenever in a derivation there is a choice for several sub-derivations, at that point a special choice node (ambiguity constructor) is placed with all alternatives as children. We assume here that the ambiguity constructor is always placed at the location where a choice is needed, and not higher (i.e. a minimal parse forest representation). The preference mechanism compares the top nodes of each alternative: All alternative derivations that have avoid at the top node will be removed, but only if other alternatives derivations are there that do not have avoid at the top node. If there are derivations that have prefer at the top node, all other derivations that do not have prefer at the top node will be removed. The preference attribute can be used to handle the case when two productions can parse the same input. Here is an example: Exp.FunctionApp = <<Expr> <Expr*>> Exp.Constructor = <<ID> <Expr>> {prefer} Priorities \u00b6 Priorities are one of SDF3's most often used disambiguation constructs. A priority section defines the relative priorities between productions. Priorities are a powerful disambiguation construct because it occurs at parse generation time. The idea behind the semantics of priorities is that productions with a higher priority \"bind stronger\" than productions with a lower priority. The essence of the priority disambiguation construct is that certain parse trees are removed from the \"forest\" (the set of all possible parse trees that can be derived from a segment of code). The basic priority syntax looks like this: context-free priorities $ProductionRef > $ProductionRef Where $ProductionRef> can either be $Sort.$Constructor or the entire production itself. Several priorities in a priority grammar are separated by commas. If more productions have the same priority they may be grouped between curly braces on each side of the > sign. context-free priorities {$ProductionRef $ProductionRef} > $ProductionRef, $ProductionRef > $ProductionRef By default, the priority relation is automatically transitively closed (i.e. if A > B and B > C then A > C). To specify a non-transitive priority relation it is necessary to include a dot before the > sign ( .> ). SDF3 provides safe disambiguation, meaning that priority relations only remove ambiguous derivations. Furthermore, SDF3 also allows tree filtering by means of indexed priorities such as: context-free priorities $ProductionRef $Index > $ProductionRef where the symbol at position $Index (starting with 0) in the first production should not derive the second production. An example defining priorities for the addition, subtraction and multiplication operators is listed below. Because addition and subtraction have the same priority, the are grouped together between brackets. context-free priorities {Exp.Times} > {Exp.Plus Exp.Minus} Associativity \u00b6 Like with priorities, the essence of the associativity attribute is that certain parse trees are removed from the \"forest\". The left associativity attribute on a production P filters all occurrences of P as a direct child of P in the right-most argument. This implies that left is only effective on productions that are recursive on the right (as in A B C -> C ). The right associativity attribute on a production P filters all occurrences of P as a direct child of P in the left-most argument. This implies that right is only effective on productions that are recursive on the left ( as in C A B -> C ). The non-assoc associativity attribute on a production P filters all occurrences of P as a direct child of P in any argument. This implement that non-assoc is only effective if a production is indeed recursive (as in A C B -> C ). The assoc attribute means the same as left Associativity declarations occur in two places in SDF3. The first is as production attributes. The second is as associativity declarations in priority groups. An example on how to mention associativity as a production attribute is given below: Exp.Plus = <<Exp> + <Exp>> {left} In priority groups, the associativity has the same semantics as the associativity attributes, except that the filter refers to more nested productions instead of a recursive nesting of one production. The group associativity attribute works pairwise and commutative on all combinations of productions in the group. If there is only one element in the group the attribute is reflexive, otherwise it is not reflexive. context-free priorities {left: Exp.Times} > {left: Exp.Plus Exp.Minus} Restrictions \u00b6 The notion of restrictions enables the formulation of lexical disambiguation strategies. Examples are \"shift before reduce\" and \"longest match\". A restriction filters applications of productions for certain non-terminals if the following character (lookahead) is in a certain class. The result is that specific symbols may not be followed by a character from a given character class. A lookahead may consist of more than one character class (multiple lookahead). Restrictions come in two flavors: lexical restrictions that apply to lexical non-terminals context-free restrictions that apply to context-free non-terminals. The general form of a restriction is: $Symbol+ -/- $Lookahead The semantics of a restriction is to remove all derivations that produce a certain $Symbol . The condition for this removal is that the derivation tree for that symbol is followed immediately by something that matches the lookahead declaration. Note that to be able to check this condition, one must look past derivations that produce the empty language, until the characters to the right of the filtered symbol are found. Also, for finding multiple lookahead matches, one must ignore nullable sub-trees that may occur in the middle of the matched lookahead. In case of lexical restrictions $Symbol may be either a literal or sort. In case of context-free restrictions only a sort or symbol is allowed. The restriction operator -/- should be read as may not be followed by. Before the restriction operator -/- a list of symbols is given for which the restriction holds. As an example, the following restriction rule implements the \u201clongest match\u201d policy: an identifier can not be followed by an alpha-numeric character. ID -/- [a-zA-Z0-9\\_]","title":"Disambiguation"},{"location":"references/syntax/disambiguation/#disambiguation","text":"The semantics of SDF3 can be seen as two-staged. First, the grammar generates all possible derivations. Second, the disambiguation constructs remove a number of derivations that are not valid. Note that SDF3 actually performs some disambiguation both when generating the parse table as during parsing.","title":"Disambiguation"},{"location":"references/syntax/disambiguation/#rejections","text":"Rejections filter derivations. The semantics of a rejection is that the set of valid derivations for the left-hand side of the production will not contain the construction described on the right-hand side. In other words, the language defined by the sort on the left-hand side has become smaller, removing all the constructions generated by the rule on the right-hand side. Disambiguation by reject occurs at parse time (mostly). A rule can be marked as rejected by using the attribute {reject} after the rule: $Sort = ... {reject} The {reject} attribute works well for lexical rejections, especially keyword reservation in the form of productions like: ID = \"keyword\" {reject}","title":"Rejections"},{"location":"references/syntax/disambiguation/#preferences","text":"The preferences mechanism is another disambiguation filter that provides a post parse filter to parse forests. The attributes prefer and avoid are the only disambiguation constructs that compare alternative derivations after parsing. Warning prefer and avoid are deprecated and will be removed in a future version of Spoofax. The following definition assumes that derivations are represented using parse forests with \"packaged ambiguity nodes\". This means that whenever in a derivation there is a choice for several sub-derivations, at that point a special choice node (ambiguity constructor) is placed with all alternatives as children. We assume here that the ambiguity constructor is always placed at the location where a choice is needed, and not higher (i.e. a minimal parse forest representation). The preference mechanism compares the top nodes of each alternative: All alternative derivations that have avoid at the top node will be removed, but only if other alternatives derivations are there that do not have avoid at the top node. If there are derivations that have prefer at the top node, all other derivations that do not have prefer at the top node will be removed. The preference attribute can be used to handle the case when two productions can parse the same input. Here is an example: Exp.FunctionApp = <<Expr> <Expr*>> Exp.Constructor = <<ID> <Expr>> {prefer}","title":"Preferences"},{"location":"references/syntax/disambiguation/#priorities","text":"Priorities are one of SDF3's most often used disambiguation constructs. A priority section defines the relative priorities between productions. Priorities are a powerful disambiguation construct because it occurs at parse generation time. The idea behind the semantics of priorities is that productions with a higher priority \"bind stronger\" than productions with a lower priority. The essence of the priority disambiguation construct is that certain parse trees are removed from the \"forest\" (the set of all possible parse trees that can be derived from a segment of code). The basic priority syntax looks like this: context-free priorities $ProductionRef > $ProductionRef Where $ProductionRef> can either be $Sort.$Constructor or the entire production itself. Several priorities in a priority grammar are separated by commas. If more productions have the same priority they may be grouped between curly braces on each side of the > sign. context-free priorities {$ProductionRef $ProductionRef} > $ProductionRef, $ProductionRef > $ProductionRef By default, the priority relation is automatically transitively closed (i.e. if A > B and B > C then A > C). To specify a non-transitive priority relation it is necessary to include a dot before the > sign ( .> ). SDF3 provides safe disambiguation, meaning that priority relations only remove ambiguous derivations. Furthermore, SDF3 also allows tree filtering by means of indexed priorities such as: context-free priorities $ProductionRef $Index > $ProductionRef where the symbol at position $Index (starting with 0) in the first production should not derive the second production. An example defining priorities for the addition, subtraction and multiplication operators is listed below. Because addition and subtraction have the same priority, the are grouped together between brackets. context-free priorities {Exp.Times} > {Exp.Plus Exp.Minus}","title":"Priorities"},{"location":"references/syntax/disambiguation/#associativity","text":"Like with priorities, the essence of the associativity attribute is that certain parse trees are removed from the \"forest\". The left associativity attribute on a production P filters all occurrences of P as a direct child of P in the right-most argument. This implies that left is only effective on productions that are recursive on the right (as in A B C -> C ). The right associativity attribute on a production P filters all occurrences of P as a direct child of P in the left-most argument. This implies that right is only effective on productions that are recursive on the left ( as in C A B -> C ). The non-assoc associativity attribute on a production P filters all occurrences of P as a direct child of P in any argument. This implement that non-assoc is only effective if a production is indeed recursive (as in A C B -> C ). The assoc attribute means the same as left Associativity declarations occur in two places in SDF3. The first is as production attributes. The second is as associativity declarations in priority groups. An example on how to mention associativity as a production attribute is given below: Exp.Plus = <<Exp> + <Exp>> {left} In priority groups, the associativity has the same semantics as the associativity attributes, except that the filter refers to more nested productions instead of a recursive nesting of one production. The group associativity attribute works pairwise and commutative on all combinations of productions in the group. If there is only one element in the group the attribute is reflexive, otherwise it is not reflexive. context-free priorities {left: Exp.Times} > {left: Exp.Plus Exp.Minus}","title":"Associativity"},{"location":"references/syntax/disambiguation/#restrictions","text":"The notion of restrictions enables the formulation of lexical disambiguation strategies. Examples are \"shift before reduce\" and \"longest match\". A restriction filters applications of productions for certain non-terminals if the following character (lookahead) is in a certain class. The result is that specific symbols may not be followed by a character from a given character class. A lookahead may consist of more than one character class (multiple lookahead). Restrictions come in two flavors: lexical restrictions that apply to lexical non-terminals context-free restrictions that apply to context-free non-terminals. The general form of a restriction is: $Symbol+ -/- $Lookahead The semantics of a restriction is to remove all derivations that produce a certain $Symbol . The condition for this removal is that the derivation tree for that symbol is followed immediately by something that matches the lookahead declaration. Note that to be able to check this condition, one must look past derivations that produce the empty language, until the characters to the right of the filtered symbol are found. Also, for finding multiple lookahead matches, one must ignore nullable sub-trees that may occur in the middle of the matched lookahead. In case of lexical restrictions $Symbol may be either a literal or sort. In case of context-free restrictions only a sort or symbol is allowed. The restriction operator -/- should be read as may not be followed by. Before the restriction operator -/- a list of symbols is given for which the restriction holds. As an example, the following restriction rule implements the \u201clongest match\u201d policy: an identifier can not be followed by an alpha-numeric character. ID -/- [a-zA-Z0-9\\_]","title":"Restrictions"},{"location":"references/syntax/kernel-syntax/","text":"Kernel Syntax \u00b6 The rules from context-free and lexical syntax are translated into kernel syntax by the SDF3 normalizer. When writing kernel syntax, one has more control over the layout between symbols of a production. As part of normalization, among other things, SDF3 renames each symbol in the lexical syntax to include the suffix -LEX and each symbol in the context-free syntax to include the suffix -CF . For example, the two productions lexical syntax BinaryConst = [0-1]+ context-free syntax Block.Block = \"{\" Statement* \"}\" written in kernel syntax look like syntax Block-CF.Block = \"{\" LAYOUT?-CF Statement*-CF LAYOUT?-CF \"}\" BinaryConst-LEX = [0-1]+ Literals and character classes are lexical by definition, thus they do not need any suffix. Note that each symbol in kernel syntax is uniquely identified by its full name including -CF and -LEX . That is, two symbols named Block-CF and Block are different, if both occur in kernel syntax. However, Block-CF is the same symbol as Block if the latter appears in a context-free syntax section. As mentioned before, layout can only occur in between symbols if explicitly specified. For example, the production syntax Block-CF.Block = \"{\" Statement*-CF LAYOUT?-CF \"}\" does not allow layout to occur in between the opening bracket and the list of statements. This means that a fragment such as: { x = 1; } would not be recognized as a block.","title":"Kernel Syntax"},{"location":"references/syntax/kernel-syntax/#kernel-syntax","text":"The rules from context-free and lexical syntax are translated into kernel syntax by the SDF3 normalizer. When writing kernel syntax, one has more control over the layout between symbols of a production. As part of normalization, among other things, SDF3 renames each symbol in the lexical syntax to include the suffix -LEX and each symbol in the context-free syntax to include the suffix -CF . For example, the two productions lexical syntax BinaryConst = [0-1]+ context-free syntax Block.Block = \"{\" Statement* \"}\" written in kernel syntax look like syntax Block-CF.Block = \"{\" LAYOUT?-CF Statement*-CF LAYOUT?-CF \"}\" BinaryConst-LEX = [0-1]+ Literals and character classes are lexical by definition, thus they do not need any suffix. Note that each symbol in kernel syntax is uniquely identified by its full name including -CF and -LEX . That is, two symbols named Block-CF and Block are different, if both occur in kernel syntax. However, Block-CF is the same symbol as Block if the latter appears in a context-free syntax section. As mentioned before, layout can only occur in between symbols if explicitly specified. For example, the production syntax Block-CF.Block = \"{\" Statement*-CF LAYOUT?-CF \"}\" does not allow layout to occur in between the opening bracket and the list of statements. This means that a fragment such as: { x = 1; } would not be recognized as a block.","title":"Kernel Syntax"},{"location":"references/syntax/layout-sensitivity/","text":"Layout Sensitivity \u00b6 SDF3 supports definition of layout sensitive syntax by means of low-level layout constraints and high-level layout declarations . Note If you want to use layout constraints or layout declarations, you should specify the jsglr-version: layout-sensitive parameter for SDF3, see configuration . Layout Constraints \u00b6 While we haven't covered layout constraints in this documentation, the paper of Erdweg et al. 1 describes the concepts. Layout Declarations \u00b6 In the paper of Erdweg et al. 1 , the authors describe layout constraints in terms of restrictions involving the position of the subtree involved in the constraint ( 0 , 1 , ...), token selectors ( first , left , last and right ), and position selectors as lines and columns ( line and col ). This mechanism allows writing layout constraints to express alignment, offside and indentation rules, but writing such constraints is rather cumbersome and error prone. Alternatively, one may write layout constraints using layout declarations , which are more declarative specifications and abstract over lines, columns and token selectors as the original layout constraints from the Erdweg et al. paper 1 . Tree selectors \u00b6 To specify which trees should be subject to a layout constraint, one may use: tree positions, SDF3 labeled non-terminals, or unique literals that occurs in the production. For example: context-free syntax Stmt.IfElse = \"if\" Exp \"then\" Stmts \"else\" else:Stmts {layout( indent \"if\" 3, else && align 3 else && align \"if\" \"else\" )} In the layout constraint for the production above, else refers to the tree for the labeled non-terminal else:Stmts , \"if\" refers to the tree corresponding to the \"if\" literal and the number 3 correspond to the tree at position 3 in the parse tree (starting at 0, ignoring trees for LAYOUT? ). align \u00b6 The layout constraint layout(align x y1, ..., yn) specifies that the trees indicated by the tree selectors yi should be aligned with the tree indicated by the tree selector x , i.e., all these trees should start in the same column. For example, if we consider the production above, the following program is correct according to the align constraints: if x < 0 then \u00b7\u00b7 x = 0 else \u00b7\u00b7 y = 1 Whereas, the following program is incorrect because neither the if and else keyword align ( align \"if\" \"else\" ), nor the statements in the branches ( align 3 else ): if x < 0 then \u00b7\u00b7 x = 0 \u00b7 else \u00b7\u00b7\u00b7 y = 1 align-list \u00b6 The constraint align-list can be used to indicate that all subtrees within a list should be aligned. That is, a constraint layout(align-list x) , where x is a tree selector for a list subtree, can be used to enforce such constraint. For example, consider the following production and its layout constraint: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( align-list then )} This constraint indicates that statements inside the list should be aligned. Therefore, the following program is correct according to this constraint: if x < 0 then \u00b7\u00b7 x = 0 \u00b7\u00b7 y = 4 \u00b7\u00b7 z = 2 And the following program is invalid, as the second statement is misaligned: if x < 0 then \u00b7\u00b7 x = 0 \u00b7\u00b7\u00b7 y = 4 \u00b7\u00b7 z = 2 offside \u00b6 The offside rule is very common in layout-sensitive languages. It states that all lines after the first one should be further to the right compared to the first line. For a description of how the offside rule can be modelled with layout constraints, refer to Erdweg et al. 1 . An example of a declarative specification of the offside rule can be seen in the production below: context-free syntax Stmt.Assign = <<ID> = <Exp>> {layout(offside 3)} The layout constraint specifies that when the expression in the statement spams multiple lines, all following lines should be indented with respect to the column where the expression started. For example, the following program is valid according to this constraint: x = 4 * 10 \u00b7\u00b7\u00b7\u00b7\u00b7 + 2 However, the following program is not valid, as the second line of the expression starts at the same column as the first line: x = 4 * 10 \u00b7\u00b7\u00b7\u00b7 + 2 Note that if the expression is written on a single line, the constraint is also verified. That is, the following program successfully parses: x = 4 * 10 + 2 It is also possible to use the offside relation on different trees. For example, consider the constraint in the following production: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( offside \"if\" then )} This constraint states that all lines (except the first) of the statements in the then branch should be indented with respect to the if literal. Thus, the following program is invalid according to this layout constraint, because the statement x = 2 should be indented with relation to the topmost if . if x < 0 then \u00b7\u00b7 if y < 0 then x = 2 In general, an offside constraint involving more than a single tree is combined with indent constraint to enforce that the column of the first and all subsequent lines should be indented. indent \u00b6 An indent constraint indicates that the column of the first line of a certain tree should be further to the right with respect to another tree. For example, consider the following production: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( indent \"if\" then )} This constraint indicates that the first line of the list of statements should be indented with respect to the if literal. Thus, according to this constraint the following program is valid: if x < 0 then \u00b7\u00b7 x = 2 Note that if the list of statements in the then branch spams multiple lines, the constraint does not apply to its subsequent lines. For example, consider the following program: if x < 0 then \u00b7\u00b7 x = 2 + 10 * 4 y = 3 This program is still valid, since the column of the first line of the first assignment is indented with respect to the if literal. To indicate that the first and all subsequent lines should be indented, an offside constraint should also be included. context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( indent \"if\" then && offside \"if\" then )} With this constraint, the remainder of the expression * 4 should also be further to the right compared to the \"if\" literal. The following program is correct according to these two constraints, since the second line of the first assignment and the second assignment are also indented with respect to the if literal: if x < 0 then \u00b7\u00b7 x = 2 + 10 \u00b7 * 4 \u00b7 y = 3 newline-indent \u00b6 The newline-indent constraint indicates that a certain tree should both be located on a new line, as well as further to the right with respect to another tree. For example, consider the following production: context-free syntax Exp.If = \"if\" Exp \"then\" then:Exp \"else\" Exp {layout( newline-indent \"if\" then )} This constraint indicates that the \"then\" branch of the if-then-else expression needs to be on a new line and indented with respect to the \"if\" keyword. Thus, according to this constraint the following program is valid: if x < 0 then \u00b7\u00b7 x + 2 else x * 2 The newline-indent constraint does not require that the expression is located on the immediate next line, but rather that it is located below the reference tree. Consequently, the following is also allowed: if x < 0 then \u00b7\u00b7 x + 2 else x * 2 Note that the newline-indent constraint is relative to the first character in the reference tree. That means that, given the following syntax: context-free syntax Example.Example = a:FooBar b:Baz {layout(newline-indent a b)} FooBar.FooBar = <foo bar> Baz.Baz = <baz> The following syntax is valid, despite bar being indented further than baz : foo \u00b7\u00b7\u00b7\u00b7bar \u00b7\u00b7baz single-line \u00b6 The single-line constraints indicates that the entirety of the given subtrees must be located on the same line. For example, consider the following production: context-free syntax Exp.If = \"if\" Exp \":\" then:Exp \"else\" Exp {layout( single-line \"if\" \":\" )} This enforces that both the if and : tokens need to be on the same line. As a result, the condition expression also needs to be contained on the same line. Thus, the following program is valid for the given constraints: if x < 3 : x + 2 else x * 2 Note that the entirety of the referenced tree needs to be on the same line. Consider the following syntax: context-free syntax Example.Example = a:Baz b:FooBar {layout(single-line a b)} FooBar.FooBar = <foo bar> Baz.Baz = <baz> With this definition, the following program is invalid, despite baz and the start of foo bar being on the same line: baz foo bar Using the single-line constraint without any parameters will add a constraint that the entire production needs to be on a single line. This can be useful as a shorthand when your grammar requires that the entirety of a production is on the same line: context-free syntax Exp.Add = <<Exp> + <Exp>> {layout(single-line)} // is equivalent to Exp.Add = <<a:Exp> + <b:Exp>> {layout(single-line a b)} Finally, all these layout declarations can be ignored by the parser and used only when generating the pretty-printer. To do that, prefix the constraint with pp- writing, for example, pp-offside or pp-align . Sebastian Erdweg, Tillmann Rendel, Christian K\u00e4stner, and Klaus Ostermann. Layout-sensitive generalized parsing. In Krzysztof Czarnecki and G\u00f6rel Hedin, editors, Software Language Engineering, 5 th International Conference, SLE 2012, Dresden, Germany, September 26-28, 2012, Revised Selected Papers , volume 7745 of Lecture Notes in Computer Science, 244\u2013263. Springer, 2012. doi:10.1007/978-3-642-36089-3\\_14 . \u21a9 \u21a9 \u21a9 \u21a9","title":"Layout Sensitivity"},{"location":"references/syntax/layout-sensitivity/#layout-sensitivity","text":"SDF3 supports definition of layout sensitive syntax by means of low-level layout constraints and high-level layout declarations . Note If you want to use layout constraints or layout declarations, you should specify the jsglr-version: layout-sensitive parameter for SDF3, see configuration .","title":"Layout Sensitivity"},{"location":"references/syntax/layout-sensitivity/#layout-constraints","text":"While we haven't covered layout constraints in this documentation, the paper of Erdweg et al. 1 describes the concepts.","title":"Layout Constraints"},{"location":"references/syntax/layout-sensitivity/#layout-declarations","text":"In the paper of Erdweg et al. 1 , the authors describe layout constraints in terms of restrictions involving the position of the subtree involved in the constraint ( 0 , 1 , ...), token selectors ( first , left , last and right ), and position selectors as lines and columns ( line and col ). This mechanism allows writing layout constraints to express alignment, offside and indentation rules, but writing such constraints is rather cumbersome and error prone. Alternatively, one may write layout constraints using layout declarations , which are more declarative specifications and abstract over lines, columns and token selectors as the original layout constraints from the Erdweg et al. paper 1 .","title":"Layout Declarations"},{"location":"references/syntax/layout-sensitivity/#tree-selectors","text":"To specify which trees should be subject to a layout constraint, one may use: tree positions, SDF3 labeled non-terminals, or unique literals that occurs in the production. For example: context-free syntax Stmt.IfElse = \"if\" Exp \"then\" Stmts \"else\" else:Stmts {layout( indent \"if\" 3, else && align 3 else && align \"if\" \"else\" )} In the layout constraint for the production above, else refers to the tree for the labeled non-terminal else:Stmts , \"if\" refers to the tree corresponding to the \"if\" literal and the number 3 correspond to the tree at position 3 in the parse tree (starting at 0, ignoring trees for LAYOUT? ).","title":"Tree selectors"},{"location":"references/syntax/layout-sensitivity/#align","text":"The layout constraint layout(align x y1, ..., yn) specifies that the trees indicated by the tree selectors yi should be aligned with the tree indicated by the tree selector x , i.e., all these trees should start in the same column. For example, if we consider the production above, the following program is correct according to the align constraints: if x < 0 then \u00b7\u00b7 x = 0 else \u00b7\u00b7 y = 1 Whereas, the following program is incorrect because neither the if and else keyword align ( align \"if\" \"else\" ), nor the statements in the branches ( align 3 else ): if x < 0 then \u00b7\u00b7 x = 0 \u00b7 else \u00b7\u00b7\u00b7 y = 1","title":"align"},{"location":"references/syntax/layout-sensitivity/#align-list","text":"The constraint align-list can be used to indicate that all subtrees within a list should be aligned. That is, a constraint layout(align-list x) , where x is a tree selector for a list subtree, can be used to enforce such constraint. For example, consider the following production and its layout constraint: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( align-list then )} This constraint indicates that statements inside the list should be aligned. Therefore, the following program is correct according to this constraint: if x < 0 then \u00b7\u00b7 x = 0 \u00b7\u00b7 y = 4 \u00b7\u00b7 z = 2 And the following program is invalid, as the second statement is misaligned: if x < 0 then \u00b7\u00b7 x = 0 \u00b7\u00b7\u00b7 y = 4 \u00b7\u00b7 z = 2","title":"align-list"},{"location":"references/syntax/layout-sensitivity/#offside","text":"The offside rule is very common in layout-sensitive languages. It states that all lines after the first one should be further to the right compared to the first line. For a description of how the offside rule can be modelled with layout constraints, refer to Erdweg et al. 1 . An example of a declarative specification of the offside rule can be seen in the production below: context-free syntax Stmt.Assign = <<ID> = <Exp>> {layout(offside 3)} The layout constraint specifies that when the expression in the statement spams multiple lines, all following lines should be indented with respect to the column where the expression started. For example, the following program is valid according to this constraint: x = 4 * 10 \u00b7\u00b7\u00b7\u00b7\u00b7 + 2 However, the following program is not valid, as the second line of the expression starts at the same column as the first line: x = 4 * 10 \u00b7\u00b7\u00b7\u00b7 + 2 Note that if the expression is written on a single line, the constraint is also verified. That is, the following program successfully parses: x = 4 * 10 + 2 It is also possible to use the offside relation on different trees. For example, consider the constraint in the following production: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( offside \"if\" then )} This constraint states that all lines (except the first) of the statements in the then branch should be indented with respect to the if literal. Thus, the following program is invalid according to this layout constraint, because the statement x = 2 should be indented with relation to the topmost if . if x < 0 then \u00b7\u00b7 if y < 0 then x = 2 In general, an offside constraint involving more than a single tree is combined with indent constraint to enforce that the column of the first and all subsequent lines should be indented.","title":"offside"},{"location":"references/syntax/layout-sensitivity/#indent","text":"An indent constraint indicates that the column of the first line of a certain tree should be further to the right with respect to another tree. For example, consider the following production: context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( indent \"if\" then )} This constraint indicates that the first line of the list of statements should be indented with respect to the if literal. Thus, according to this constraint the following program is valid: if x < 0 then \u00b7\u00b7 x = 2 Note that if the list of statements in the then branch spams multiple lines, the constraint does not apply to its subsequent lines. For example, consider the following program: if x < 0 then \u00b7\u00b7 x = 2 + 10 * 4 y = 3 This program is still valid, since the column of the first line of the first assignment is indented with respect to the if literal. To indicate that the first and all subsequent lines should be indented, an offside constraint should also be included. context-free syntax Stmt.If = \"if\" Exp \"then\" then:Stmt* {layout( indent \"if\" then && offside \"if\" then )} With this constraint, the remainder of the expression * 4 should also be further to the right compared to the \"if\" literal. The following program is correct according to these two constraints, since the second line of the first assignment and the second assignment are also indented with respect to the if literal: if x < 0 then \u00b7\u00b7 x = 2 + 10 \u00b7 * 4 \u00b7 y = 3","title":"indent"},{"location":"references/syntax/layout-sensitivity/#newline-indent","text":"The newline-indent constraint indicates that a certain tree should both be located on a new line, as well as further to the right with respect to another tree. For example, consider the following production: context-free syntax Exp.If = \"if\" Exp \"then\" then:Exp \"else\" Exp {layout( newline-indent \"if\" then )} This constraint indicates that the \"then\" branch of the if-then-else expression needs to be on a new line and indented with respect to the \"if\" keyword. Thus, according to this constraint the following program is valid: if x < 0 then \u00b7\u00b7 x + 2 else x * 2 The newline-indent constraint does not require that the expression is located on the immediate next line, but rather that it is located below the reference tree. Consequently, the following is also allowed: if x < 0 then \u00b7\u00b7 x + 2 else x * 2 Note that the newline-indent constraint is relative to the first character in the reference tree. That means that, given the following syntax: context-free syntax Example.Example = a:FooBar b:Baz {layout(newline-indent a b)} FooBar.FooBar = <foo bar> Baz.Baz = <baz> The following syntax is valid, despite bar being indented further than baz : foo \u00b7\u00b7\u00b7\u00b7bar \u00b7\u00b7baz","title":"newline-indent"},{"location":"references/syntax/layout-sensitivity/#single-line","text":"The single-line constraints indicates that the entirety of the given subtrees must be located on the same line. For example, consider the following production: context-free syntax Exp.If = \"if\" Exp \":\" then:Exp \"else\" Exp {layout( single-line \"if\" \":\" )} This enforces that both the if and : tokens need to be on the same line. As a result, the condition expression also needs to be contained on the same line. Thus, the following program is valid for the given constraints: if x < 3 : x + 2 else x * 2 Note that the entirety of the referenced tree needs to be on the same line. Consider the following syntax: context-free syntax Example.Example = a:Baz b:FooBar {layout(single-line a b)} FooBar.FooBar = <foo bar> Baz.Baz = <baz> With this definition, the following program is invalid, despite baz and the start of foo bar being on the same line: baz foo bar Using the single-line constraint without any parameters will add a constraint that the entire production needs to be on a single line. This can be useful as a shorthand when your grammar requires that the entirety of a production is on the same line: context-free syntax Exp.Add = <<Exp> + <Exp>> {layout(single-line)} // is equivalent to Exp.Add = <<a:Exp> + <b:Exp>> {layout(single-line a b)} Finally, all these layout declarations can be ignored by the parser and used only when generating the pretty-printer. To do that, prefix the constraint with pp- writing, for example, pp-offside or pp-align . Sebastian Erdweg, Tillmann Rendel, Christian K\u00e4stner, and Klaus Ostermann. Layout-sensitive generalized parsing. In Krzysztof Czarnecki and G\u00f6rel Hedin, editors, Software Language Engineering, 5 th International Conference, SLE 2012, Dresden, Germany, September 26-28, 2012, Revised Selected Papers , volume 7745 of Lecture Notes in Computer Science, 244\u2013263. Springer, 2012. doi:10.1007/978-3-642-36089-3\\_14 . \u21a9 \u21a9 \u21a9 \u21a9","title":"single-line"},{"location":"references/syntax/lexical-syntax/","text":"Lexical Syntax \u00b6 The lexical syntax usually describes the low level structure of programs (often referred to as lexical tokens). However, in SDF3, the token concept is not really relevant, since only character classes are terminals. The lexical syntax sections in SDF3 are simply a convenient notation for the low level syntax of a language. The LAYOUT symbol should also be defined in a lexical syntax section. A lexical syntax consists of a list of productions. Lexical syntax is described as follows: lexical syntax $Production* An example of a production in lexical syntax: lexical syntax BinaryConst = [0-1]+","title":"Lexical Syntax"},{"location":"references/syntax/lexical-syntax/#lexical-syntax","text":"The lexical syntax usually describes the low level structure of programs (often referred to as lexical tokens). However, in SDF3, the token concept is not really relevant, since only character classes are terminals. The lexical syntax sections in SDF3 are simply a convenient notation for the low level syntax of a language. The LAYOUT symbol should also be defined in a lexical syntax section. A lexical syntax consists of a list of productions. Lexical syntax is described as follows: lexical syntax $Production* An example of a production in lexical syntax: lexical syntax BinaryConst = [0-1]+","title":"Lexical Syntax"},{"location":"references/syntax/modules/","text":"Modules \u00b6 An SDF3 specification consists of a number of module declarations. Each module defines sections and may import other modules. Imports \u00b6 Modules may import other modules for reuse or separation of concerns. A module may extend the definition of a non-terminal in another module. A module may compose the definition of a language by importing the parts of the language. The structure of a module is as follows: module $ModuleName $ImportSection* $Section* The module keyword is followed by the module name, then a series of imports can be made, followed by sections that contain the actual definition of the syntax. An import section is structured as follows: imports $ModuleName* Note that SDF3 does not support parameterized modules. Sections \u00b6 An SDF3 module may constitute of zero or more sections. All sections contribute to the final grammar that defines a language: sorts , lexical sorts , context-free sorts (see Symbols#Sorts ) lexical syntax (see Lexical Syntax ) context-free syntax (see Context-Free Syntax ) syntax (see Kernel Syntax ) lexical start-symbols , context-free start-symbols , start-symbols (see Start Symbols ) context-free priorities , priorities (see Disambiguation ) template options (see Templates )","title":"Modules"},{"location":"references/syntax/modules/#modules","text":"An SDF3 specification consists of a number of module declarations. Each module defines sections and may import other modules.","title":"Modules"},{"location":"references/syntax/modules/#imports","text":"Modules may import other modules for reuse or separation of concerns. A module may extend the definition of a non-terminal in another module. A module may compose the definition of a language by importing the parts of the language. The structure of a module is as follows: module $ModuleName $ImportSection* $Section* The module keyword is followed by the module name, then a series of imports can be made, followed by sections that contain the actual definition of the syntax. An import section is structured as follows: imports $ModuleName* Note that SDF3 does not support parameterized modules.","title":"Imports"},{"location":"references/syntax/modules/#sections","text":"An SDF3 module may constitute of zero or more sections. All sections contribute to the final grammar that defines a language: sorts , lexical sorts , context-free sorts (see Symbols#Sorts ) lexical syntax (see Lexical Syntax ) context-free syntax (see Context-Free Syntax ) syntax (see Kernel Syntax ) lexical start-symbols , context-free start-symbols , start-symbols (see Start Symbols ) context-free priorities , priorities (see Disambiguation ) template options (see Templates )","title":"Sections"},{"location":"references/syntax/productions/","text":"Productions \u00b6 The basic building block of syntax sections is the production. The left-hand side of a regular production rule can be either just a symbol or a symbol followed by . and a constructor name. The right-hand side consists of zero or more symbols. Both sides are separated by = : $Symbol = $Symbol* $Symbol.$Constructor = $Symbol* A production is read as the definition. The symbol on the left-hand side is defined by the right-hand side of the production. Productions are used to describe lexical , context-free , and kernel syntax. Productions may also occur in priority sections , but might also be referred to by its $Symbol.$Constructor . All productions with the same symbol together define the alternatives for that symbol. Attributes \u00b6 The definition of productions may be followed by attributes that define additional (syntactic or semantic) properties of that production. The attributes are written between curly brackets after the right-hand side of a production. If a production has more than one attribute they are separated by commas. Attributes have thus the following form: $Sort = $Symbol* { $Attribute1, $Attribute2, ...} $Sort.$Constructor = $Symbol* { $Attribute1, $Attribute2, ...} The following syntax-related attributes exist: bracket is an important attribute in combination with priorities. The parenthesizer tool uses the bracket attribute to find productions to add to a parse tree before pretty printing (when the tree violates priority constraints). Note that most of these tools demand the production with a bracket attribute to have the shape: X = \"(\" X \")\" {bracket} with any kind of bracket syntax but the X being the same symbol on the left-hand side and the right-hand side. The connection with priorities and associativity is that when a non-terminal is disambiguated using either of them, a production rule with the bracket attribute is probably also needed. reject is a disambiguation construct that implements language difference. It is used for keyword reservation. See Disambiguation#Rejections . left , right , non-assoc , assoc are disambiguation constructs used to define the associativity of productions. See Disambiguation#Associativity . prefer and avoid are deprecated disambiguation constructs to define preference of one derivation over others. See Disambiguation#Preferences .","title":"Productions"},{"location":"references/syntax/productions/#productions","text":"The basic building block of syntax sections is the production. The left-hand side of a regular production rule can be either just a symbol or a symbol followed by . and a constructor name. The right-hand side consists of zero or more symbols. Both sides are separated by = : $Symbol = $Symbol* $Symbol.$Constructor = $Symbol* A production is read as the definition. The symbol on the left-hand side is defined by the right-hand side of the production. Productions are used to describe lexical , context-free , and kernel syntax. Productions may also occur in priority sections , but might also be referred to by its $Symbol.$Constructor . All productions with the same symbol together define the alternatives for that symbol.","title":"Productions"},{"location":"references/syntax/productions/#attributes","text":"The definition of productions may be followed by attributes that define additional (syntactic or semantic) properties of that production. The attributes are written between curly brackets after the right-hand side of a production. If a production has more than one attribute they are separated by commas. Attributes have thus the following form: $Sort = $Symbol* { $Attribute1, $Attribute2, ...} $Sort.$Constructor = $Symbol* { $Attribute1, $Attribute2, ...} The following syntax-related attributes exist: bracket is an important attribute in combination with priorities. The parenthesizer tool uses the bracket attribute to find productions to add to a parse tree before pretty printing (when the tree violates priority constraints). Note that most of these tools demand the production with a bracket attribute to have the shape: X = \"(\" X \")\" {bracket} with any kind of bracket syntax but the X being the same symbol on the left-hand side and the right-hand side. The connection with priorities and associativity is that when a non-terminal is disambiguated using either of them, a production rule with the bracket attribute is probably also needed. reject is a disambiguation construct that implements language difference. It is used for keyword reservation. See Disambiguation#Rejections . left , right , non-assoc , assoc are disambiguation constructs used to define the associativity of productions. See Disambiguation#Associativity . prefer and avoid are deprecated disambiguation constructs to define preference of one derivation over others. See Disambiguation#Preferences .","title":"Attributes"},{"location":"references/syntax/recovery/","text":"Recovery \u00b6 SDF3 automatically generates permissive grammars for supporting error-recovery parsing 1 . The permissive grammars contain recovery productions that can be used to recover from syntactic errors. The recovery productions are either deletions or insertions. Deletions can skip over an erroneous part of the input. Insertions recover from a missing part of the input, e.g. a missing closing bracket. Handwritten recovery rules can be added to tweak the automatically generated permissive grammar by using the recover attribute. For example, the following insertion enables recovery for missing if literals: lexical syntax \"if\" = {recover} The empty right-hand side makes sure that, in recovery mode, the if literal can be parsed even when it is not present in the input. Maartje de Jonge, Lennart C. L. Kats, Eelco Visser, and Emma S\u00f6derberg. Natural and flexible error recovery for generated modular language environments. ACM Transactions on Programming Languages and Systems , 34 \\(4\\) :15, 2012. URL: http://doi.acm.org/10.1145/2400676.2400678 , doi:10.1145/2400676.2400678 . \u21a9","title":"Recovery"},{"location":"references/syntax/recovery/#recovery","text":"SDF3 automatically generates permissive grammars for supporting error-recovery parsing 1 . The permissive grammars contain recovery productions that can be used to recover from syntactic errors. The recovery productions are either deletions or insertions. Deletions can skip over an erroneous part of the input. Insertions recover from a missing part of the input, e.g. a missing closing bracket. Handwritten recovery rules can be added to tweak the automatically generated permissive grammar by using the recover attribute. For example, the following insertion enables recovery for missing if literals: lexical syntax \"if\" = {recover} The empty right-hand side makes sure that, in recovery mode, the if literal can be parsed even when it is not present in the input. Maartje de Jonge, Lennart C. L. Kats, Eelco Visser, and Emma S\u00f6derberg. Natural and flexible error recovery for generated modular language environments. ACM Transactions on Programming Languages and Systems , 34 \\(4\\) :15, 2012. URL: http://doi.acm.org/10.1145/2400676.2400678 , doi:10.1145/2400676.2400678 . \u21a9","title":"Recovery"},{"location":"references/syntax/start-symbols/","text":"Start Symbols \u00b6 The lexical or context-free start symbols sections explicitly define the symbols which will serve as start symbols when parsing terms. If no start symbols are defined it is not possible to recognize terms. This has the effect that input sentences corresponding to these symbols can be parsed. So, if we want to recognize boolean terms we have to define explicitly the sort Boolean as a start symbol in the module Booleans . Any symbol and also lists, optionals, etc., can serve as a start-symbol. A definition of lexical start symbols looks like: lexical start-symbols $Symbol* While context-free start symbols are defined as: context-free start-symbols $Symbol* SDF3 also supports kernel start-symbols: start-symbols $Symbol* In contrast to lexical and kernel start-symbols, context-free start symbols can be surrounded by optional layout. A lexical start-symbol should have been defined by a production in the lexical syntax; a context-free symbol should have been defined in the context-free syntax. Both symbols can also be defined in kernel syntax using the suffix -LEX or -CF .","title":"Start Symbols"},{"location":"references/syntax/start-symbols/#start-symbols","text":"The lexical or context-free start symbols sections explicitly define the symbols which will serve as start symbols when parsing terms. If no start symbols are defined it is not possible to recognize terms. This has the effect that input sentences corresponding to these symbols can be parsed. So, if we want to recognize boolean terms we have to define explicitly the sort Boolean as a start symbol in the module Booleans . Any symbol and also lists, optionals, etc., can serve as a start-symbol. A definition of lexical start symbols looks like: lexical start-symbols $Symbol* While context-free start symbols are defined as: context-free start-symbols $Symbol* SDF3 also supports kernel start-symbols: start-symbols $Symbol* In contrast to lexical and kernel start-symbols, context-free start symbols can be surrounded by optional layout. A lexical start-symbol should have been defined by a production in the lexical syntax; a context-free symbol should have been defined in the context-free syntax. Both symbols can also be defined in kernel syntax using the suffix -LEX or -CF .","title":"Start Symbols"},{"location":"references/syntax/symbols/","text":"Symbols \u00b6 The building block of SDF3 productions is a symbol. SDF3 symbols can be compared to terminals and non-terminals in other grammar formalisms. The elementary symbols are character classes, literals, and sorts. Intrinsically, only character classes are real terminal symbols. All other symbols represent non-terminals. SDF3 also support symbols that capture BNF-like notation such as lists, optionals, alternatives, and sequences. Note that these symbols are also non-terminals, and are just shorthands for common structures present in context-free grammars. Character classes \u00b6 Character classes occur only in lexical syntax and are enclosed by [ and ] . A character class consists of a list of zero or more characters (which stand for themselves) such as [x] to represent the character x , or character ranges, as an abbreviation for all the characters in the range such as [0-9] representing 0 , 1 , ..., 9 . A valid range consists of [c1-c2] , where the character c2 has a higher ASCII code than c1 . Note that nested character classes can also be concatenated within the same character class symbol, for example [c1c2-c3c4-c5] includes the characters c1 and the ranges c2-c3 , c4-c5 . In this case, the nested character classes do not need to be ordered, as SDF3 orders them when performing a normalization step. Escaping \u00b6 SDF3 uses a backslash ( \\ ) as a escape for the quotingof special characters. One should use \\c whenever c is not a digit or a letter in a character class. Unicode \u00b6 Arbitrary Unicode code points can be included in a character class by writing an escaped integer, which is particularly useful for representing characters outside the printable ASCII range. The integer can be a binary, octal, decimal, or hexadecimal number, for example: \\0b101010 , \\052 , \\42 , and \\0x2A all represent the code point 42, or the '*' character. Special ASCII Characters \u00b6 Additionally, special ASCII characters are represented by: \\t : horizontal tabulation \\n : newline character \\v : vertical tabulation \\f : form feed \\r : carriage return Operators \u00b6 SDF3 provides the following operators for character classes: (complement) ~ : Accepts all the characters that are not in the original class. (difference) / : Accepts all the characters in the first class unless they are in a second class. (union) \\/ : Accepts all the characters in either character classes. (intersection) /\\ : Accepts all the characters that are accepted by both character classes. Note that the first operator is unary and the other ones are left associative binary operators. Furthermore, such operators are not applicable to other symbols in general. Literals \u00b6 A literal symbol defines a fixed length word. This usually corresponds to a terminal symbol in ordinary context-free grammars, for example \"true\" or \"+\" . Literals must always be quoted and consist of (possibly escaped) ASCII characters. As literals are also regular non-terminals, SDF3 automatically generates productions for them in terms of terminal symbols. \"definition\" = [d][e][f][i][n][i][t][i][o][n] Note that the production above defines a case-sensitive implementation of the defined literal. Case-insensitive literals are defined using single-quoted strings as in 'true' or 'else' . SDF3 generates a different production for case-insensitive literals as 'definition' = [dD][eE][fF][iI][nN][iI][tT][iI][oO][nN] The literal above accepts case-insensitive inputs such as definition , DEFINITION , DeFiNiTiOn or defINITION . Sorts \u00b6 A sort corresponds to a plain non-terminal, e.g. Statement or Exp . Sort names start with a capital letter and may be followed by letters, digits, hyphens, or underscores. Note that unlike SDF2, SDF3 does not support parameterized sorts (yet!). Sorts are declared by listing their name in the appropriate sorts section, which have the following forms. For context-free sorts: context-free sorts $Sort* For lexical sorts: lexical sorts $Sort* SDF3 also supports kernel sorts: sorts $Sort* Note Kernel sorts should be suffixed with -CF or -LEX , depending on whether they are context-free sorts or lexical sorts. When a sort in a sorts block does not have a suffix, it is treated as a context-free sort. Writing a sort in these sections only indicates that a sort has been declared, even if it does not have any explicit production visible. Optionals \u00b6 SDF3 provides a shorthand for describing zero or exactly one occurrence of a sort by appending the sort with ? . For example, the sort Extends? can be parsed as Extends or without consuming any input. Internally, SDF3 generates the following productions after normalizing the grammar:: Extends?.None = Extends?.Some = Extends Note that using ? adds the constructors None and Some to the final abstract syntax tree. Lists \u00b6 Lists symbols as the name says, indicate that a symbol should occur several times. In this way, it is also possible to construct flat structures to represent them. SDF3 provides support for two types of lists, with and without separators. Furthermore, it is also possible to indicate whether a list can be empty ( * ) or should have at least one element ( + ). For example, a list Statement* indicates zero or more Statement , whereas a list with separator {ID \",\"}+ indicates one or more ID separated by , . Note that SDF3 only supports literal symbols as separators. Again, SDF3 generates the following productions to represent lists, when normalizing the grammar: Statement* = Statement* = Statement+ Statement+ = Statement+ Statement Statement+ = Statement {ID \",\"}* = {ID \",\"}* = {ID \",\"}+ {ID \",\"}+ = {ID \",\"}+ \",\" {ID \",\"} {ID \",\"}+ = {ID \",\"} When parsing a context-free list, SDF3 produces a flattened list as an AST node such as [Statement, ..., Statement] or [ID, ..., ID] . Note that because the separator is a literal, it does not appear in the AST. Alternative \u00b6 Alternative symbols express the choice between two symbols, for example, ID | INT . That is, the symbol ID | INT can be parsed as either ID or INT . For that reason, SDF3 normalizes alternatives by generating the following productions: ID | INT = ID ID | INT = INT Note that SDF3 only allow alternative symbols to occur in lexical syntax. Furthermore, note that the alternative operator is right associative and binds stronger than any operator. That is, ID \",\" | ID \";\" expresses ID (\",\" | ID) \";\" . To express (ID \",\") | (ID \";\") , we can use a sequence symbol. Sequence \u00b6 A sequence operator allows grouping of two or more symbols. Sequences are useful when combined with other symbols such, lists or optionals, for example (\"e\" [0-9]+)? . Like alternative symbols, sequences can only occur in lexical syntax. A sequence symbol is normalized as: (\"e\" [0-9]+) = \"e\" [0-9]+ Labeled symbols \u00b6 SDF3 supports decorating symbols with labels, such as myList:{elem:Stmt \";\"}* . The labels can be used for layout constraints/declarations or by other tools that use SDF3 grammars as input. LAYOUT \u00b6 The LAYOUT symbol is a reserved sort name. It is used to indicate the whitespace that can appear in between context-free symbols. The user must define the symbol LAYOUT such as: LAYOUT = [\\ \\t\\n] Note that the production above should be defined in the lexical syntax.","title":"Symbols"},{"location":"references/syntax/symbols/#symbols","text":"The building block of SDF3 productions is a symbol. SDF3 symbols can be compared to terminals and non-terminals in other grammar formalisms. The elementary symbols are character classes, literals, and sorts. Intrinsically, only character classes are real terminal symbols. All other symbols represent non-terminals. SDF3 also support symbols that capture BNF-like notation such as lists, optionals, alternatives, and sequences. Note that these symbols are also non-terminals, and are just shorthands for common structures present in context-free grammars.","title":"Symbols"},{"location":"references/syntax/symbols/#character-classes","text":"Character classes occur only in lexical syntax and are enclosed by [ and ] . A character class consists of a list of zero or more characters (which stand for themselves) such as [x] to represent the character x , or character ranges, as an abbreviation for all the characters in the range such as [0-9] representing 0 , 1 , ..., 9 . A valid range consists of [c1-c2] , where the character c2 has a higher ASCII code than c1 . Note that nested character classes can also be concatenated within the same character class symbol, for example [c1c2-c3c4-c5] includes the characters c1 and the ranges c2-c3 , c4-c5 . In this case, the nested character classes do not need to be ordered, as SDF3 orders them when performing a normalization step.","title":"Character classes"},{"location":"references/syntax/symbols/#escaping","text":"SDF3 uses a backslash ( \\ ) as a escape for the quotingof special characters. One should use \\c whenever c is not a digit or a letter in a character class.","title":"Escaping"},{"location":"references/syntax/symbols/#unicode","text":"Arbitrary Unicode code points can be included in a character class by writing an escaped integer, which is particularly useful for representing characters outside the printable ASCII range. The integer can be a binary, octal, decimal, or hexadecimal number, for example: \\0b101010 , \\052 , \\42 , and \\0x2A all represent the code point 42, or the '*' character.","title":"Unicode"},{"location":"references/syntax/symbols/#special-ascii-characters","text":"Additionally, special ASCII characters are represented by: \\t : horizontal tabulation \\n : newline character \\v : vertical tabulation \\f : form feed \\r : carriage return","title":"Special ASCII Characters"},{"location":"references/syntax/symbols/#operators","text":"SDF3 provides the following operators for character classes: (complement) ~ : Accepts all the characters that are not in the original class. (difference) / : Accepts all the characters in the first class unless they are in a second class. (union) \\/ : Accepts all the characters in either character classes. (intersection) /\\ : Accepts all the characters that are accepted by both character classes. Note that the first operator is unary and the other ones are left associative binary operators. Furthermore, such operators are not applicable to other symbols in general.","title":"Operators"},{"location":"references/syntax/symbols/#literals","text":"A literal symbol defines a fixed length word. This usually corresponds to a terminal symbol in ordinary context-free grammars, for example \"true\" or \"+\" . Literals must always be quoted and consist of (possibly escaped) ASCII characters. As literals are also regular non-terminals, SDF3 automatically generates productions for them in terms of terminal symbols. \"definition\" = [d][e][f][i][n][i][t][i][o][n] Note that the production above defines a case-sensitive implementation of the defined literal. Case-insensitive literals are defined using single-quoted strings as in 'true' or 'else' . SDF3 generates a different production for case-insensitive literals as 'definition' = [dD][eE][fF][iI][nN][iI][tT][iI][oO][nN] The literal above accepts case-insensitive inputs such as definition , DEFINITION , DeFiNiTiOn or defINITION .","title":"Literals"},{"location":"references/syntax/symbols/#sorts","text":"A sort corresponds to a plain non-terminal, e.g. Statement or Exp . Sort names start with a capital letter and may be followed by letters, digits, hyphens, or underscores. Note that unlike SDF2, SDF3 does not support parameterized sorts (yet!). Sorts are declared by listing their name in the appropriate sorts section, which have the following forms. For context-free sorts: context-free sorts $Sort* For lexical sorts: lexical sorts $Sort* SDF3 also supports kernel sorts: sorts $Sort* Note Kernel sorts should be suffixed with -CF or -LEX , depending on whether they are context-free sorts or lexical sorts. When a sort in a sorts block does not have a suffix, it is treated as a context-free sort. Writing a sort in these sections only indicates that a sort has been declared, even if it does not have any explicit production visible.","title":"Sorts"},{"location":"references/syntax/symbols/#optionals","text":"SDF3 provides a shorthand for describing zero or exactly one occurrence of a sort by appending the sort with ? . For example, the sort Extends? can be parsed as Extends or without consuming any input. Internally, SDF3 generates the following productions after normalizing the grammar:: Extends?.None = Extends?.Some = Extends Note that using ? adds the constructors None and Some to the final abstract syntax tree.","title":"Optionals"},{"location":"references/syntax/symbols/#lists","text":"Lists symbols as the name says, indicate that a symbol should occur several times. In this way, it is also possible to construct flat structures to represent them. SDF3 provides support for two types of lists, with and without separators. Furthermore, it is also possible to indicate whether a list can be empty ( * ) or should have at least one element ( + ). For example, a list Statement* indicates zero or more Statement , whereas a list with separator {ID \",\"}+ indicates one or more ID separated by , . Note that SDF3 only supports literal symbols as separators. Again, SDF3 generates the following productions to represent lists, when normalizing the grammar: Statement* = Statement* = Statement+ Statement+ = Statement+ Statement Statement+ = Statement {ID \",\"}* = {ID \",\"}* = {ID \",\"}+ {ID \",\"}+ = {ID \",\"}+ \",\" {ID \",\"} {ID \",\"}+ = {ID \",\"} When parsing a context-free list, SDF3 produces a flattened list as an AST node such as [Statement, ..., Statement] or [ID, ..., ID] . Note that because the separator is a literal, it does not appear in the AST.","title":"Lists"},{"location":"references/syntax/symbols/#alternative","text":"Alternative symbols express the choice between two symbols, for example, ID | INT . That is, the symbol ID | INT can be parsed as either ID or INT . For that reason, SDF3 normalizes alternatives by generating the following productions: ID | INT = ID ID | INT = INT Note that SDF3 only allow alternative symbols to occur in lexical syntax. Furthermore, note that the alternative operator is right associative and binds stronger than any operator. That is, ID \",\" | ID \";\" expresses ID (\",\" | ID) \";\" . To express (ID \",\") | (ID \";\") , we can use a sequence symbol.","title":"Alternative"},{"location":"references/syntax/symbols/#sequence","text":"A sequence operator allows grouping of two or more symbols. Sequences are useful when combined with other symbols such, lists or optionals, for example (\"e\" [0-9]+)? . Like alternative symbols, sequences can only occur in lexical syntax. A sequence symbol is normalized as: (\"e\" [0-9]+) = \"e\" [0-9]+","title":"Sequence"},{"location":"references/syntax/symbols/#labeled-symbols","text":"SDF3 supports decorating symbols with labels, such as myList:{elem:Stmt \";\"}* . The labels can be used for layout constraints/declarations or by other tools that use SDF3 grammars as input.","title":"Labeled symbols"},{"location":"references/syntax/symbols/#layout","text":"The LAYOUT symbol is a reserved sort name. It is used to indicate the whitespace that can appear in between context-free symbols. The user must define the symbol LAYOUT such as: LAYOUT = [\\ \\t\\n] Note that the production above should be defined in the lexical syntax.","title":"LAYOUT"},{"location":"references/syntax/templates/","text":"Templates \u00b6 Templates are a major change in SDF3 when comparing to SDF2. They are essential when aiming to generate a nice pretty printer or generate proper syntactic code completion templates. When generating such artifacts, a general production simply introduces a whitespace in between symbols. For example, when writing a grammar rule Statement.If = \"if\" \"(\" Exp \")\" Exp \"else\" Exp and pretty printing a valid program, we would get the text in a single line separated by spaces, as: Furthermore, code completion would consider the same indentation when inserting code snippets. However, when using template productions such as Statement.If = < if ($Exp) $Exp else $Exp We would get the following program: Again, code completion would also consider this indentation for proposals. That is, in template productions, the surrounding layout is used to nicely pretty print programs and its code completion suggestions. Template Productions \u00b6 Template productions are an alternative way of defining productions. Similarly, they consist of a left-hand side and a right-hand side separated by = . The left-hand side is the same as for productive rules. The right-hand side is a template delimited by < and > . The template can contain zero or more symbols: $Sort = < $Symbol* > $Sort.$Constructor = < $Symbol* > Alternatively, square brackets can be used to delimit a template: $Sort = [ $Symbol* ] $Sort.$Constructor = [ $Symbol* ] The symbols in a template can either be placeholders or literal strings. It is worth noting that: placeholders need to be enclosed within the same delimiters (either <...> or [...] ) as the template; literal strings need not not be enclosed within quotation marks; literal strings are tokenized on space characters (whitespace, tab); additionally, literal strings are tokenized on boundaries between characters from the set given by the tokenize option, see the tokenize template option ; placeholders translate literally. If a separator containing any layout characters is given, the placeholder maps to a list with separator that strips the layout. An example of a template rule: Exp.Addition = < $Exp + $Exp > Here, the + symbol is a literal string and $Exp is a placeholder for sort Exp . Placeholders are of the form: $Sort? : optional placeholder $Sort* : repetition (0...n) $Sort+ : repetition (1...n) <{$Sort \",\"}*> : repetition with separator Case-insensitive Literals \u00b6 As we showed before, SDF3 allows defining case-insensitive literals as single-quoted strings in regular productions. For example: Exp.If = 'if' \"(\" Exp \")\" Exp 'else' Exp accepts case-insensitive keywords for if and else such as if , IF , If , else , ELSE or ELsE . However, to generate case-insensitive literals from template productions, it is necessary to add annotate these productions as case-insensitive. For example, a template production: Exp.If = < if($Exp) $Exp else $Exp > {case-insensitive} accepts the same input as the regular production mentioned before. Moreover, lexical symbols can also be annotated as case-insensitive to parse as such. The constructed abstract syntax tree contains lower-case symbols, but the original term is preserved via origin-tracking. For example: ID = [a-zA-z][a-zA-Z0-9]* {case-insensitive} can parse foo , Foo , FOo , fOo , foO , fOO or FOO . Whichever option generates a node \"foo\" in the abstract syntax tree. By consulting the origin information on this node, it is possible to know which term was used as input to the parser. Template options \u00b6 Template options are options that are applied to the current file. A template options section is structured as follows: template options $TemplateOption* Multiple template option sections are not supported. If multiple template option sections are specified, the last one is used. There are three kinds of template options. keyword \u00b6 Convenient way for setting up lexical follow restrictions for keywords. See the section on follow restrictions for more information. The structure of the keyword option is as follows: keyword -/- $Pattern This will add a follow restriction on the pattern for each keyword in the language. Keywords are automatically detected, any terminal that ends with an alphanumeric character is considered a keyword. Multiple keyword options are not supported. If multiple keyword options are specified, the last one is used. Note that this only sets up follow restrictions, rejection of keywords as identifiers still needs to be written manually. tokenize \u00b6 Specifies which characters may have layout around them. The structure of a tokenize option is as follows: tokenize : \"$Character*\" Consider the following grammar specification: template options tokenize : \"(\" context-free syntax Exp.Call = <<ID>();> Because layout is allowed around the ( and ) characters, there may be layout between () and ; in the template rule. If no tokenize option is specified, it defaults to the default value of () . Multiple tokenize options are not supported. If multiple tokenize options are specified, the last one is used. reject \u00b6 Convenient way for setting up reject rules for keywords. See the section on rejections for more information. The structure of the reject option is as follows: Symbol = keyword {attrs} where Symbol is the symbol to generate the rules for. Note that attrs can be include any attribute, but by using reject , reject rules such as ID = \"true\" {reject} are generated for all keywords that appear in the templates. Multiple reject template options are not supported. If multiple reject template options are specified, the last one is used.","title":"Templates"},{"location":"references/syntax/templates/#templates","text":"Templates are a major change in SDF3 when comparing to SDF2. They are essential when aiming to generate a nice pretty printer or generate proper syntactic code completion templates. When generating such artifacts, a general production simply introduces a whitespace in between symbols. For example, when writing a grammar rule Statement.If = \"if\" \"(\" Exp \")\" Exp \"else\" Exp and pretty printing a valid program, we would get the text in a single line separated by spaces, as: Furthermore, code completion would consider the same indentation when inserting code snippets. However, when using template productions such as Statement.If = < if ($Exp) $Exp else $Exp We would get the following program: Again, code completion would also consider this indentation for proposals. That is, in template productions, the surrounding layout is used to nicely pretty print programs and its code completion suggestions.","title":"Templates"},{"location":"references/syntax/templates/#template-productions","text":"Template productions are an alternative way of defining productions. Similarly, they consist of a left-hand side and a right-hand side separated by = . The left-hand side is the same as for productive rules. The right-hand side is a template delimited by < and > . The template can contain zero or more symbols: $Sort = < $Symbol* > $Sort.$Constructor = < $Symbol* > Alternatively, square brackets can be used to delimit a template: $Sort = [ $Symbol* ] $Sort.$Constructor = [ $Symbol* ] The symbols in a template can either be placeholders or literal strings. It is worth noting that: placeholders need to be enclosed within the same delimiters (either <...> or [...] ) as the template; literal strings need not not be enclosed within quotation marks; literal strings are tokenized on space characters (whitespace, tab); additionally, literal strings are tokenized on boundaries between characters from the set given by the tokenize option, see the tokenize template option ; placeholders translate literally. If a separator containing any layout characters is given, the placeholder maps to a list with separator that strips the layout. An example of a template rule: Exp.Addition = < $Exp + $Exp > Here, the + symbol is a literal string and $Exp is a placeholder for sort Exp . Placeholders are of the form: $Sort? : optional placeholder $Sort* : repetition (0...n) $Sort+ : repetition (1...n) <{$Sort \",\"}*> : repetition with separator","title":"Template Productions"},{"location":"references/syntax/templates/#case-insensitive-literals","text":"As we showed before, SDF3 allows defining case-insensitive literals as single-quoted strings in regular productions. For example: Exp.If = 'if' \"(\" Exp \")\" Exp 'else' Exp accepts case-insensitive keywords for if and else such as if , IF , If , else , ELSE or ELsE . However, to generate case-insensitive literals from template productions, it is necessary to add annotate these productions as case-insensitive. For example, a template production: Exp.If = < if($Exp) $Exp else $Exp > {case-insensitive} accepts the same input as the regular production mentioned before. Moreover, lexical symbols can also be annotated as case-insensitive to parse as such. The constructed abstract syntax tree contains lower-case symbols, but the original term is preserved via origin-tracking. For example: ID = [a-zA-z][a-zA-Z0-9]* {case-insensitive} can parse foo , Foo , FOo , fOo , foO , fOO or FOO . Whichever option generates a node \"foo\" in the abstract syntax tree. By consulting the origin information on this node, it is possible to know which term was used as input to the parser.","title":"Case-insensitive Literals"},{"location":"references/syntax/templates/#template-options","text":"Template options are options that are applied to the current file. A template options section is structured as follows: template options $TemplateOption* Multiple template option sections are not supported. If multiple template option sections are specified, the last one is used. There are three kinds of template options.","title":"Template options"},{"location":"references/syntax/templates/#keyword","text":"Convenient way for setting up lexical follow restrictions for keywords. See the section on follow restrictions for more information. The structure of the keyword option is as follows: keyword -/- $Pattern This will add a follow restriction on the pattern for each keyword in the language. Keywords are automatically detected, any terminal that ends with an alphanumeric character is considered a keyword. Multiple keyword options are not supported. If multiple keyword options are specified, the last one is used. Note that this only sets up follow restrictions, rejection of keywords as identifiers still needs to be written manually.","title":"keyword"},{"location":"references/syntax/templates/#tokenize","text":"Specifies which characters may have layout around them. The structure of a tokenize option is as follows: tokenize : \"$Character*\" Consider the following grammar specification: template options tokenize : \"(\" context-free syntax Exp.Call = <<ID>();> Because layout is allowed around the ( and ) characters, there may be layout between () and ; in the template rule. If no tokenize option is specified, it defaults to the default value of () . Multiple tokenize options are not supported. If multiple tokenize options are specified, the last one is used.","title":"tokenize"},{"location":"references/syntax/templates/#reject","text":"Convenient way for setting up reject rules for keywords. See the section on rejections for more information. The structure of the reject option is as follows: Symbol = keyword {attrs} where Symbol is the symbol to generate the rules for. Note that attrs can be include any attribute, but by using reject , reject rules such as ID = \"true\" {reject} are generated for all keywords that appear in the templates. Multiple reject template options are not supported. If multiple reject template options are specified, the last one is used.","title":"reject"},{"location":"references/testing/","text":"SPT: Spoofax Testing Language \u00b6 The SPoofax Testing language (SPT) allows language developers to test their language in a declarative way. It offers a language to express test cases for any textual language that you want to test, and a framework for executing those tests on language implementations created with Spoofax. We will first describe the syntax and semantics of the SPT language. Then, we will discuss how you can execute your SPT test cases, and finally we conclude with an overview of the architecture of the SPT framework. In this section we will describe the syntax and semantics of SPT. If you want to write your own tests you can follow along as the different concepts are explained. We suggest using the Spoofax Eclipse plugins, as they contain an editor for SPT files. In an Eclipse with Spoofax installed, simply create a new file with the extension .spt and follow along to create your first SPT test suite.","title":"SPT: Spoofax Testing Language"},{"location":"references/testing/#spt-spoofax-testing-language","text":"The SPoofax Testing language (SPT) allows language developers to test their language in a declarative way. It offers a language to express test cases for any textual language that you want to test, and a framework for executing those tests on language implementations created with Spoofax. We will first describe the syntax and semantics of the SPT language. Then, we will discuss how you can execute your SPT test cases, and finally we conclude with an overview of the architecture of the SPT framework. In this section we will describe the syntax and semantics of SPT. If you want to write your own tests you can follow along as the different concepts are explained. We suggest using the Spoofax Eclipse plugins, as they contain an editor for SPT files. In an Eclipse with Spoofax installed, simply create a new file with the extension .spt and follow along to create your first SPT test suite.","title":"SPT: Spoofax Testing Language"},{"location":"references/testing/running-tests/","text":"Running SPT Tests \u00b6 SPT tests can be run different ways. Each one corresponds to a different use case of SPT. In the end, they all use the common core SPT Java API for extracting and executing tests. We will now briefly discuss the different ways to run an SPT test suite based on the use case. If you are new to SPT, the Interactive Test Design use case will be the one for you. Interactive Test Design \u00b6 SPT was originally designed for this use case. Its goal was to lower the threshold of writing tests for your language, by allowing you to concisely declare test inputs and outputs, offering editor services for the fragments that you write, and providing you with real time in-editor feedback on whether your test fails or passes. For this use case you would be using Eclipse with the Spoofax plugin. When you open a test suite in the Eclipse editor, all failing test cases will have error markers on them. By turning the test results into message markers inside the editor, we can provide you with a detailed location on where it went wrong. Especially for parsing or analysis errors. However, to keep the error message readable, they can not contain full stack traces, which you might need to debug transformation or Stratego run tests. It is also impractical to check all your test suites this way if you have many of them in your project. To solve this, we have created a JUnit style test runner in Eclipse. It is available through the Spoofax (meta) menu bar entry, and offers two ways to run tests. The first one is called Run tests of SPT files . If you click this, it will check if you currently have an SPT file that is open and, if so, launch the test runner to run all tests in this file. This mode can be useful if one of your tests is failing and you would like to see a more detailed error message. The second entry is called Run all selected tests . It will check what you selected in the package or project explorer. If you selected any SPT files, directories, or projects, it will scan all of the selected locations and run all of the SPT files it found within those selections. This method is useful for running regression tests. The user interface of the test runner consists of 3 parts. The first part is the progress bar, which is followed by two numbers that indicate the progress of your current test runs. This part is displayed at the top of the runner. The second part is the overview of all the test suites and their test cases which are part of this test run. This part is displayed in the bottom left. The final part is a console window, which contains more detailed error messages about the test suite or test case you selected in the second part. This part is displayed in the bottom right: The test runner will start displaying any test suites and test cases within those test suites as soon as it discovers them. Then, after they are all loaded, they will be executed one by one, and the progress bar at the top will increase. As long as the progress bar remains green, no tests failed yet. As soon as a single test fails, the progress bar will turn red to indicate so. The numbers next to the progress bar also indicate the progress. For example, 5 / 7 means 5 tests passed already out of a total of 7 tests. This can mean that either 2 tests failed, or some tests have not been executed yet. Which case applies can be determined by looking at the progress bar. Any SPT files that fail can't be parsed or from which we can't extract test cases for some other reason, will be included in the list on the bottom left side, along with the test suites that did manage to get extracted. The ones that did not extract properly will be displayed in red, as opposed to the default black color for test suites. By selecting a red test suite, the extraction errors will be displayed in the console on the bottom right. Any test suite can be double clicked to open the corresponding file in Eclipse. Test suites that got extracted succesfully can be expanded if they contained any test cases. This will show all the test cases of that suite as child elements of the test suite in the bottom left view. Test cases are displayed in the default black color if they have not been executed yet. Test cases that have finished will have their duration appended to their name. Failed test cases are displayed in red, and passing test cases are displayed in green. A red test case can be selected, doing so will show the messages about the test failure, including the exceptions that caused them (e.g. a StrategoRuntimeException with a stacktrace) in the console on the bottom right. Double clicking a test case will open the corresponding SPT file and jump to the location of the test case. When a test case fails, the test suite that contained the failing test case will be appended with the number of failed tests in that test suite so far. Run using the Command Line Runner \u00b6 At https://github.com/metaborg/spt/tree/master/org.metaborg.spt.cmd there is a project that creates an executable jar with which you can run all the test suites in a given directory. It is more of a proof of concept and an example of how to use our core SPT Java API than a full fledged test runner. For those interested in giving it a try: Obtaining the test runner jar: bash $ git clone https://github.com/metaborg/spt.git $ cd spt/org.metaborg.spt.cmd $ mvn package $ ls target/org.metaborg.spt.cmd* target/org.metaborg.spt.cmd-2.0.0-SNAPSHOT.jar This jar is the executable jar that contains the test runner. Next up, we want to run the tests for our language. To do so, we need: the directory with tests to run (e.g., path/to/test/project ) the language under test (e.g. path/to/MiniJava/project ) the SPT language, to be able to extract the tests from the specification (Optionally) the Stratego language if we want to be able to execute the run or transform expectations You should already have your tests and your language project, so next up is the SPT language. This is in the same repo as the command line runner: cd spt/org.metaborg.meta.lang.spt mvn verify If you want to use the run and transform expectations, you also need the Stratego language: $ git clone https://github.com/metaborg/stratego.git $ cd stratego/org.metaborg.meta.lang.stratego $ mvn verify Now we can run the tests: $ java -jar spt/org.metaborg.spt.cmd/target/org.metaborg.spt.cmd-2.0.0-SNAPSHOT.jar -h Usage: <main class> [options] Options: --help, -h Shows usage help Default: false --lang, -ol Location of any other language that should be loaded Default: [] * --lut, -l Location of the language under test * --spt, -s Location of the SPT language --start-symbol, -start Start Symbol for these tests * --tests, -t Location of test files $ java -jar spt/org.metaborg.spt.cmd/target/org.metaborg.spt.cmd-2.0.0-SNAPSHOT.jar --lut /path/to/MiniJava/project --tests /path/to/test/project --spt spt/org.metaborg.meta.lang.spt --lang stratego/org.metaborg.meta.lang.stratego Run using the SPT Framework \u00b6 The SPT framework at https://github.com/metaborg/spt offers a Java API to run SPT test suites. The framework is split between the generic part ( org.metaborg.mbt.core - MetaBorg Testing (MBT)) and the Spoofax specific part ( org.metaborg.spt.core SPoofax Testing (SPT)). The first step in running tests is to extract them from an SPT test suite. org.metaborg.mbt.core provides a Java object model to represent SPT test cases. To extract test cases from a test suite to the Java model, you can use the ITestCaseExtractor . You can either implement this for your own version of the SPT language, or use our SPT language ( org.metaborg.meta.lang.spt ) and our extractor ( ISpoofaxTestCaseExtractor ). Now that you have the tests in Java objects, you can execute them with the ITestCaseRunner . If the language you are testing is not integrated with Metaborg Core, you will either have to do so and subclass the TestCaseRunner , or make your own implementation for the ITestCaseRunner . If your language under test is integrated with Metaborg Core (this is the case for all languages created with Spoofax), you can use our ISpoofaxTestCaseRunner . For an example on how to use dependency injection to obtain the correct classes and extract and run SPT tests using the Java API, see the TestRunner class at ( https://github.com/metaborg/spt/tree/master/org.metaborg.spt.core ). Run using Maven \u00b6 For regression testing and continuous integration, it can be useful to be able to execute tests from a maven build. To do so, create a pom.xml file in your test project with the following content: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" > <modelVersion> 4.0.0 </modelVersion> <groupId> your.group.id </groupId> <artifactId> your.test.project.name </artifactId> <version> 0.0.1-SNAPSHOT </version> <packaging> spoofax-test </packaging> <parent> <groupId> org.metaborg </groupId> <artifactId> parent.language </artifactId> <version> 2.1.0-SNAPSHOT </version> </parent> <dependencies> <dependency> <groupId> your.group.id </groupId> <artifactId> your.language.under.test.id </artifactId> <version> 1.0.0-SNAPSHOT </version> <type> spoofax-language </type> </dependency> <dependency> <groupId> org.metaborg </groupId> <artifactId> org.metaborg.meta.lang.spt </artifactId> <version> ${metaborg-version} </version> <type> spoofax-language </type> <scope> test </scope> </dependency> </dependencies> <build> <plugins> <plugin> <groupId> org.metaborg </groupId> <artifactId> spoofax-maven-plugin </artifactId> <version> ${metaborg-version} </version> <configuration> <languageUnderTest> your.group.id:your.language.under.test.id:1.0.0-SNAPSHOT </languageUnderTest> </configuration> </plugin> </plugins> </build> </project> You should now be able to execute the tests with mvn verify .","title":"Running SPT Tests"},{"location":"references/testing/running-tests/#running-spt-tests","text":"SPT tests can be run different ways. Each one corresponds to a different use case of SPT. In the end, they all use the common core SPT Java API for extracting and executing tests. We will now briefly discuss the different ways to run an SPT test suite based on the use case. If you are new to SPT, the Interactive Test Design use case will be the one for you.","title":"Running SPT Tests"},{"location":"references/testing/running-tests/#interactive-test-design","text":"SPT was originally designed for this use case. Its goal was to lower the threshold of writing tests for your language, by allowing you to concisely declare test inputs and outputs, offering editor services for the fragments that you write, and providing you with real time in-editor feedback on whether your test fails or passes. For this use case you would be using Eclipse with the Spoofax plugin. When you open a test suite in the Eclipse editor, all failing test cases will have error markers on them. By turning the test results into message markers inside the editor, we can provide you with a detailed location on where it went wrong. Especially for parsing or analysis errors. However, to keep the error message readable, they can not contain full stack traces, which you might need to debug transformation or Stratego run tests. It is also impractical to check all your test suites this way if you have many of them in your project. To solve this, we have created a JUnit style test runner in Eclipse. It is available through the Spoofax (meta) menu bar entry, and offers two ways to run tests. The first one is called Run tests of SPT files . If you click this, it will check if you currently have an SPT file that is open and, if so, launch the test runner to run all tests in this file. This mode can be useful if one of your tests is failing and you would like to see a more detailed error message. The second entry is called Run all selected tests . It will check what you selected in the package or project explorer. If you selected any SPT files, directories, or projects, it will scan all of the selected locations and run all of the SPT files it found within those selections. This method is useful for running regression tests. The user interface of the test runner consists of 3 parts. The first part is the progress bar, which is followed by two numbers that indicate the progress of your current test runs. This part is displayed at the top of the runner. The second part is the overview of all the test suites and their test cases which are part of this test run. This part is displayed in the bottom left. The final part is a console window, which contains more detailed error messages about the test suite or test case you selected in the second part. This part is displayed in the bottom right: The test runner will start displaying any test suites and test cases within those test suites as soon as it discovers them. Then, after they are all loaded, they will be executed one by one, and the progress bar at the top will increase. As long as the progress bar remains green, no tests failed yet. As soon as a single test fails, the progress bar will turn red to indicate so. The numbers next to the progress bar also indicate the progress. For example, 5 / 7 means 5 tests passed already out of a total of 7 tests. This can mean that either 2 tests failed, or some tests have not been executed yet. Which case applies can be determined by looking at the progress bar. Any SPT files that fail can't be parsed or from which we can't extract test cases for some other reason, will be included in the list on the bottom left side, along with the test suites that did manage to get extracted. The ones that did not extract properly will be displayed in red, as opposed to the default black color for test suites. By selecting a red test suite, the extraction errors will be displayed in the console on the bottom right. Any test suite can be double clicked to open the corresponding file in Eclipse. Test suites that got extracted succesfully can be expanded if they contained any test cases. This will show all the test cases of that suite as child elements of the test suite in the bottom left view. Test cases are displayed in the default black color if they have not been executed yet. Test cases that have finished will have their duration appended to their name. Failed test cases are displayed in red, and passing test cases are displayed in green. A red test case can be selected, doing so will show the messages about the test failure, including the exceptions that caused them (e.g. a StrategoRuntimeException with a stacktrace) in the console on the bottom right. Double clicking a test case will open the corresponding SPT file and jump to the location of the test case. When a test case fails, the test suite that contained the failing test case will be appended with the number of failed tests in that test suite so far.","title":"Interactive Test Design"},{"location":"references/testing/running-tests/#run-using-the-command-line-runner","text":"At https://github.com/metaborg/spt/tree/master/org.metaborg.spt.cmd there is a project that creates an executable jar with which you can run all the test suites in a given directory. It is more of a proof of concept and an example of how to use our core SPT Java API than a full fledged test runner. For those interested in giving it a try: Obtaining the test runner jar: bash $ git clone https://github.com/metaborg/spt.git $ cd spt/org.metaborg.spt.cmd $ mvn package $ ls target/org.metaborg.spt.cmd* target/org.metaborg.spt.cmd-2.0.0-SNAPSHOT.jar This jar is the executable jar that contains the test runner. Next up, we want to run the tests for our language. To do so, we need: the directory with tests to run (e.g., path/to/test/project ) the language under test (e.g. path/to/MiniJava/project ) the SPT language, to be able to extract the tests from the specification (Optionally) the Stratego language if we want to be able to execute the run or transform expectations You should already have your tests and your language project, so next up is the SPT language. This is in the same repo as the command line runner: cd spt/org.metaborg.meta.lang.spt mvn verify If you want to use the run and transform expectations, you also need the Stratego language: $ git clone https://github.com/metaborg/stratego.git $ cd stratego/org.metaborg.meta.lang.stratego $ mvn verify Now we can run the tests: $ java -jar spt/org.metaborg.spt.cmd/target/org.metaborg.spt.cmd-2.0.0-SNAPSHOT.jar -h Usage: <main class> [options] Options: --help, -h Shows usage help Default: false --lang, -ol Location of any other language that should be loaded Default: [] * --lut, -l Location of the language under test * --spt, -s Location of the SPT language --start-symbol, -start Start Symbol for these tests * --tests, -t Location of test files $ java -jar spt/org.metaborg.spt.cmd/target/org.metaborg.spt.cmd-2.0.0-SNAPSHOT.jar --lut /path/to/MiniJava/project --tests /path/to/test/project --spt spt/org.metaborg.meta.lang.spt --lang stratego/org.metaborg.meta.lang.stratego","title":"Run using the Command Line Runner"},{"location":"references/testing/running-tests/#run-using-the-spt-framework","text":"The SPT framework at https://github.com/metaborg/spt offers a Java API to run SPT test suites. The framework is split between the generic part ( org.metaborg.mbt.core - MetaBorg Testing (MBT)) and the Spoofax specific part ( org.metaborg.spt.core SPoofax Testing (SPT)). The first step in running tests is to extract them from an SPT test suite. org.metaborg.mbt.core provides a Java object model to represent SPT test cases. To extract test cases from a test suite to the Java model, you can use the ITestCaseExtractor . You can either implement this for your own version of the SPT language, or use our SPT language ( org.metaborg.meta.lang.spt ) and our extractor ( ISpoofaxTestCaseExtractor ). Now that you have the tests in Java objects, you can execute them with the ITestCaseRunner . If the language you are testing is not integrated with Metaborg Core, you will either have to do so and subclass the TestCaseRunner , or make your own implementation for the ITestCaseRunner . If your language under test is integrated with Metaborg Core (this is the case for all languages created with Spoofax), you can use our ISpoofaxTestCaseRunner . For an example on how to use dependency injection to obtain the correct classes and extract and run SPT tests using the Java API, see the TestRunner class at ( https://github.com/metaborg/spt/tree/master/org.metaborg.spt.core ).","title":"Run using the SPT Framework"},{"location":"references/testing/running-tests/#run-using-maven","text":"For regression testing and continuous integration, it can be useful to be able to execute tests from a maven build. To do so, create a pom.xml file in your test project with the following content: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" > <modelVersion> 4.0.0 </modelVersion> <groupId> your.group.id </groupId> <artifactId> your.test.project.name </artifactId> <version> 0.0.1-SNAPSHOT </version> <packaging> spoofax-test </packaging> <parent> <groupId> org.metaborg </groupId> <artifactId> parent.language </artifactId> <version> 2.1.0-SNAPSHOT </version> </parent> <dependencies> <dependency> <groupId> your.group.id </groupId> <artifactId> your.language.under.test.id </artifactId> <version> 1.0.0-SNAPSHOT </version> <type> spoofax-language </type> </dependency> <dependency> <groupId> org.metaborg </groupId> <artifactId> org.metaborg.meta.lang.spt </artifactId> <version> ${metaborg-version} </version> <type> spoofax-language </type> <scope> test </scope> </dependency> </dependencies> <build> <plugins> <plugin> <groupId> org.metaborg </groupId> <artifactId> spoofax-maven-plugin </artifactId> <version> ${metaborg-version} </version> <configuration> <languageUnderTest> your.group.id:your.language.under.test.id:1.0.0-SNAPSHOT </languageUnderTest> </configuration> </plugin> </plugins> </build> </project> You should now be able to execute the tests with mvn verify .","title":"Run using Maven"},{"location":"references/testing/test-expectations/","text":"Test Expectations \u00b6 Test expectations allow you to specify which component of the language under test should be tested, and what the expected output of the test will be. We have already seen the parse succeeds expectation in action, and briefly mentioned that a test case without any test expectations is the same as a test case with a parse succeeds expectation. We will now list the syntax and semantics of all the currently supported test expectations: Parse Expectations \u00b6 Parse expectations can be used to test the syntax of your language. They indicate that the input fragment of the test case should be parsed and allow you to specify what you expect the result to be. As parsing is the preliminary step to all other language components (e.g., analysis and transformations) they are treated differently from other expectations. If no parse expectation is present on a test case, even if another expectation (e.g. an analysis expectation) is present, a parse succeeds expectation will be added to the test case. Expectation.ParseSucceeds = <parse succeeds> Parse the fragment and expect the parsing to succeed, with no parse errors and no ambiguities. Expectation.ParseFails = <parse fails> Parse the fragment and expect the parsing to fail, with at least one parse error. Expectation.ParseAmbiguous = <parse ambiguous> Parse the fragment and expect the parsing to succeed with one or more ambiguities. Expectation.ParseToAterm = <parse to <ATerm>> Parse the fragment, expect parsing to succeed, and compare it to the given ATerm AST. When using test fixtures, the ATerm should only be the AST of the fragment of the test, not of the entire test fixture. Please note that if you want to specify a List in the ATerm , the square brackets of the list may interfere with the markers of a fragment. Therefore, to specify a list as the expected output, prepend it with the keyword !ATerm . For example: parse to !ATerm [\"5\"] Expectation.ParseToFragment = <parse to <Language?> <OpenMarker> <Fragment> <CloseMarker>]]> Parse the fragment, expect parsing to succeed, and compare it to the result of parsing the given Fragment with the given Language . When the Language is omitted the language under test will be used to parse the given fragment. When using test fixtures, only the test's input fragment will be combined with the test fixture. The fragment in this expectation (i.e., the output fragment) will not be combined with it, even if the language under test is used to parse it. To counteract this, the entire AST (including the nodes from the fixture) will be compared to the expectation's fragment's AST. This expectation can be useful to test disambiguations, or to test your language against a reference implementation. An example test case for disambiguation in MiniJava would be: module disambiguation language MiniJava start symbol Exp test plus is left associative [[ 1 + 2 + 3 ]] parse to [[ (1 + 2) + 3 ]] Analysis Expectations \u00b6 Analysis expectations specify that the analyzer should be tested. We will first discuss the most generic analysis expectations: the message expectations. These expectations can be used to test the entire static semantic analysis process. Then we will look at test expectations that are more specific: the name analysis expectations. Analysis Message Expectations \u00b6 Analysis message expectations will cause the input fragment to be parsed and analyzed (e.g., name and type analysis and static error checking). Finally, the resulting messages (i.e. errors, warnings, or notes) will be compared to the expectation. Note that messages of the expected type are not allowed to appear in the test fixture , if one is present. This is to prevent test from succeeding, when a message that would make it fail appears in an unexpected location. Only the messages within the test's fragment will be compared to the expectation. Expectation.MessageExpectation = <<Operator?> <INT> <Severity>> These expectations will check if the number of messages of the given severity generated by the analysis matches the given number. Any other messages (of different severity or located in the test fixture) will be ignored by this expectation. The optional operator can be used to losen the strictness of the check. For example the expectation > 2 errors allows any number of errors bigger than 2. If no operator is specified, it will default to = . The allowed operators are = , > , >= , < , <= . The allowed message severities are: error or errors for error messages, warning or warnings for warning messages, and note or notes for note messages. Please note that error messages in this expectation refer only to analysis errors (not parse errors). Also when you are testing for warnings, for example, any analysis messages of different severity (including errors) will not cause the test to fail. For example, the following test will succeed, regardless of the blatant type error, because we only test for warnings: module test-for-warnings language MiniJava fixture [[ class Main { public static void main(String[] args) { System.out.println([[...]]); } } ]] test no warnings [[1 + new A()]] 0 warnings These analysis message expectations may be followed by the at keyword and a list of one or more comma separated references to selections of the test's fragment: #<INT>, #<INT>, #<INT>, ... . As this is the first time we encounter references to selections, let's look at an example: module error-locations language MiniJava test duplicate classes [[ class Main { public static void main(String[] args) { System.out.println(42); } } class [[A]]{} class [[A]]{} ]] 2 errors at #1, #2 This test will cause SPT to check if the specified messages appeared at the location of the given selection references. The selections are the classnames A that are selected by wrapping them in an open and close marker. Selections are referenced by the order in which they appear, starting at 1, from left to right and top to bottom. It is allowed to give less selection references than the number of expected messages. In this case SPT assumes you don't care about the location of the other messages. If the same selection is referenced more than once, multiple messages will be expected at that location. mFor example 3 errors at #1,#1 expects 3 errors, 2 of which should be at the location of selection number 1. The other error may be anywhere within the test fragment. Expectation.MessageContent = <<Severity> like <STRING>> This expectation specifies that there should be at least 1 message of the given severity that contains the given String. For example error like \"duplicate class name\" expects there to be at least 1 error in the fragment whose message contains duplicate class name . This expectation can also be followed by the at keyword, and a single selection reference, to indicate where you expect the message with the given content. Expectation.AnalysisSucceeds = <analysis succeeds> This expectation is syntactic sugar for 0 errors . Expectation.AnalysisFails = <analysis fails> This expectation is syntactic sugar for > 0 errors . Name Analysis Expectations \u00b6 Name analysis expectations will check if use sites can be resolved and, if required, if they resolve to the correct definition. The fragment will be parsed and analyzed, but any number and severity of analysis messages are allowed. Expectation.Resolve = <resolve #<INT>> Try to resolve the AST node at the given selection. Expect it to successfully resolve to any definition site. Expectation.ResolveTo = <resolve #<INT> to #<INT>> Try to resolve the AST node at the first given selection. Expect it to successfully resolve to the location marked by the second given selection. Note that selections can only occur in the test's fragment , not in the test fixture . So name analysis can only be tested within a test's fragment. Transformation Expectations \u00b6 A transformation transforms an AST to another AST. The idea within Spoofax is that a transformation has a name, and can be nested within a structure of menu's. Furthermore, it can have additional information about whether it transforms the raw AST (i.e. the parse result) or the analyzed AST (i.e. the result of desugaring and analysis). In languages created with Spoofax, transformations are Stratego strategies that are registered in the Menus.esv file. Transformation expectations will first look up a given transformation using the name under which it was registered. Note that, for Spoofax languages, this is not necessarily the name of the Stratego strategy, but the name under which it is registered in the Menus.esv file. If this name is not unique, the menu structure can be used to look up the proper transformation. Once the transformation is found, SPT will determine if it requires the raw AST, or the analyzed AST. If the raw AST is required, it will only parse the fragment. If the analyzed AST is required, it will also analyze the parse result. However, analysis is allowed to produce any number and severity of messages. Then, SPT will run the transformation on the entire AST, including the nodes from the test fixture, if there was one. Expectation.Transform = <transform <STRING>> The STRING should be delimited by double quotes and contain the name of the transformation. If the name is not unique, the menu structure can be included as well, seperated by -> . For example: transform \"Menu name -> transformation name\" to Some(Result()) . As long as the transformation returns a result, this expectation passes. Expectation.TransformToAterm = <transform <STRING> to <ATerm>> Same as Transform , but the result of the transformation is compared to the given AST. Expectation.TransformToFragment = <transform <STRING> to <Language?> <OpenMarker> <Fragment> <CloseMarker>> Does the same as TransformToAterm , but compares the result of the transformation to the AST of the given fragment. If the applied transformation required the raw AST, the given fragment will only be parsed with the given language. If no language is given, the language under test will be used. If the applied transformation required an analyzed AST, the given fragment will be parsed and analyzed. Run Stratego Expectations \u00b6 These test expectations are really only applicable to languages that use Stratego strategies in their implementation. They will parse and analyze the fragment and run a given Stratego strategy (with no arguments) and compare its output to the expectation. Expectation.Run = <run <STRATEGY>> This expectation will lookup the given strategy name and run it on the AST node in the test's fragment. If the fragment contains multiple nodes (e.g., it's a list of Statements but some Statements were in the test fixture) the strategy will be run on each of these nodes. Either until it completes successfully, or until it failed on all these nodes. Note that it wil not be executed on the nodes in the test fixture, if there was one. Expectation.RunWithArgs = <run <STRATEGY>(|<TermArgs>)> This expectation will run a strategy that expects term arguments. String literals, integer literals and selection references are permitted as term arguments.: test rename variable without type [[ let var msg := \"Hello World\" in print([[msg]]) end ]] run rename(|#1, \"message\", 0) to [[ let var message := \"Hello World\" in print(message) end ]] Expectation.RunFails = <run <STRATEGY> fails> This expectation checks if the given strategy fails. Expectation.RunOn = <run <STRATEGY> on #<INT>> This expectation does the same as Run , except it runs the strategy on the nodes at the given selection instead of the nodes of the test's fragment. Expectation.RunToAterm = <run <STRATEGY> to <ATerm>> shorty Expectation.RunToAtermOn = <run <STRATEGY> on #<INT> to <ATerm>> These expectations are similar to the first two, but they require the result of running the strategy to match the given AST. Expectation.RunToFragment = <run <STRATEGY> to <Language?> <OpenMarker> <Fragment> <CloseMarker>> Expectation.RunToFragmentOn = <run <STRATEGY> on #<INT> to <Language?> <OpenMarker> <Fragment> <CloseMarker>> These expectations are similar to the first two, but they require the result of running the strategy to match the result of analyzing the given fragment with the given language. If no language is given, the language under test is used. Origin Location Expectations \u00b6 Expectation.HasOrigins = <has origin locations> This expectation parses and analyzes the fragment. It then checks if all AST nodes in the test's fragment (except for Lists in Spoofax) have a source region (an origin) associated with them. It does not check the AST nodes in the test fixture. When using Spoofax, there are some strategies that will break the origin information when used. This can lead to desugarings that create AST nodes without origin information, which can cause problems when trying to create messages at their location and with other services. This expectation can be used to check that your analysis is origin preserving.","title":"Test Expectations"},{"location":"references/testing/test-expectations/#test-expectations","text":"Test expectations allow you to specify which component of the language under test should be tested, and what the expected output of the test will be. We have already seen the parse succeeds expectation in action, and briefly mentioned that a test case without any test expectations is the same as a test case with a parse succeeds expectation. We will now list the syntax and semantics of all the currently supported test expectations:","title":"Test Expectations"},{"location":"references/testing/test-expectations/#parse-expectations","text":"Parse expectations can be used to test the syntax of your language. They indicate that the input fragment of the test case should be parsed and allow you to specify what you expect the result to be. As parsing is the preliminary step to all other language components (e.g., analysis and transformations) they are treated differently from other expectations. If no parse expectation is present on a test case, even if another expectation (e.g. an analysis expectation) is present, a parse succeeds expectation will be added to the test case. Expectation.ParseSucceeds = <parse succeeds> Parse the fragment and expect the parsing to succeed, with no parse errors and no ambiguities. Expectation.ParseFails = <parse fails> Parse the fragment and expect the parsing to fail, with at least one parse error. Expectation.ParseAmbiguous = <parse ambiguous> Parse the fragment and expect the parsing to succeed with one or more ambiguities. Expectation.ParseToAterm = <parse to <ATerm>> Parse the fragment, expect parsing to succeed, and compare it to the given ATerm AST. When using test fixtures, the ATerm should only be the AST of the fragment of the test, not of the entire test fixture. Please note that if you want to specify a List in the ATerm , the square brackets of the list may interfere with the markers of a fragment. Therefore, to specify a list as the expected output, prepend it with the keyword !ATerm . For example: parse to !ATerm [\"5\"] Expectation.ParseToFragment = <parse to <Language?> <OpenMarker> <Fragment> <CloseMarker>]]> Parse the fragment, expect parsing to succeed, and compare it to the result of parsing the given Fragment with the given Language . When the Language is omitted the language under test will be used to parse the given fragment. When using test fixtures, only the test's input fragment will be combined with the test fixture. The fragment in this expectation (i.e., the output fragment) will not be combined with it, even if the language under test is used to parse it. To counteract this, the entire AST (including the nodes from the fixture) will be compared to the expectation's fragment's AST. This expectation can be useful to test disambiguations, or to test your language against a reference implementation. An example test case for disambiguation in MiniJava would be: module disambiguation language MiniJava start symbol Exp test plus is left associative [[ 1 + 2 + 3 ]] parse to [[ (1 + 2) + 3 ]]","title":"Parse Expectations"},{"location":"references/testing/test-expectations/#analysis-expectations","text":"Analysis expectations specify that the analyzer should be tested. We will first discuss the most generic analysis expectations: the message expectations. These expectations can be used to test the entire static semantic analysis process. Then we will look at test expectations that are more specific: the name analysis expectations.","title":"Analysis Expectations"},{"location":"references/testing/test-expectations/#analysis-message-expectations","text":"Analysis message expectations will cause the input fragment to be parsed and analyzed (e.g., name and type analysis and static error checking). Finally, the resulting messages (i.e. errors, warnings, or notes) will be compared to the expectation. Note that messages of the expected type are not allowed to appear in the test fixture , if one is present. This is to prevent test from succeeding, when a message that would make it fail appears in an unexpected location. Only the messages within the test's fragment will be compared to the expectation. Expectation.MessageExpectation = <<Operator?> <INT> <Severity>> These expectations will check if the number of messages of the given severity generated by the analysis matches the given number. Any other messages (of different severity or located in the test fixture) will be ignored by this expectation. The optional operator can be used to losen the strictness of the check. For example the expectation > 2 errors allows any number of errors bigger than 2. If no operator is specified, it will default to = . The allowed operators are = , > , >= , < , <= . The allowed message severities are: error or errors for error messages, warning or warnings for warning messages, and note or notes for note messages. Please note that error messages in this expectation refer only to analysis errors (not parse errors). Also when you are testing for warnings, for example, any analysis messages of different severity (including errors) will not cause the test to fail. For example, the following test will succeed, regardless of the blatant type error, because we only test for warnings: module test-for-warnings language MiniJava fixture [[ class Main { public static void main(String[] args) { System.out.println([[...]]); } } ]] test no warnings [[1 + new A()]] 0 warnings These analysis message expectations may be followed by the at keyword and a list of one or more comma separated references to selections of the test's fragment: #<INT>, #<INT>, #<INT>, ... . As this is the first time we encounter references to selections, let's look at an example: module error-locations language MiniJava test duplicate classes [[ class Main { public static void main(String[] args) { System.out.println(42); } } class [[A]]{} class [[A]]{} ]] 2 errors at #1, #2 This test will cause SPT to check if the specified messages appeared at the location of the given selection references. The selections are the classnames A that are selected by wrapping them in an open and close marker. Selections are referenced by the order in which they appear, starting at 1, from left to right and top to bottom. It is allowed to give less selection references than the number of expected messages. In this case SPT assumes you don't care about the location of the other messages. If the same selection is referenced more than once, multiple messages will be expected at that location. mFor example 3 errors at #1,#1 expects 3 errors, 2 of which should be at the location of selection number 1. The other error may be anywhere within the test fragment. Expectation.MessageContent = <<Severity> like <STRING>> This expectation specifies that there should be at least 1 message of the given severity that contains the given String. For example error like \"duplicate class name\" expects there to be at least 1 error in the fragment whose message contains duplicate class name . This expectation can also be followed by the at keyword, and a single selection reference, to indicate where you expect the message with the given content. Expectation.AnalysisSucceeds = <analysis succeeds> This expectation is syntactic sugar for 0 errors . Expectation.AnalysisFails = <analysis fails> This expectation is syntactic sugar for > 0 errors .","title":"Analysis Message Expectations"},{"location":"references/testing/test-expectations/#name-analysis-expectations","text":"Name analysis expectations will check if use sites can be resolved and, if required, if they resolve to the correct definition. The fragment will be parsed and analyzed, but any number and severity of analysis messages are allowed. Expectation.Resolve = <resolve #<INT>> Try to resolve the AST node at the given selection. Expect it to successfully resolve to any definition site. Expectation.ResolveTo = <resolve #<INT> to #<INT>> Try to resolve the AST node at the first given selection. Expect it to successfully resolve to the location marked by the second given selection. Note that selections can only occur in the test's fragment , not in the test fixture . So name analysis can only be tested within a test's fragment.","title":"Name Analysis Expectations"},{"location":"references/testing/test-expectations/#transformation-expectations","text":"A transformation transforms an AST to another AST. The idea within Spoofax is that a transformation has a name, and can be nested within a structure of menu's. Furthermore, it can have additional information about whether it transforms the raw AST (i.e. the parse result) or the analyzed AST (i.e. the result of desugaring and analysis). In languages created with Spoofax, transformations are Stratego strategies that are registered in the Menus.esv file. Transformation expectations will first look up a given transformation using the name under which it was registered. Note that, for Spoofax languages, this is not necessarily the name of the Stratego strategy, but the name under which it is registered in the Menus.esv file. If this name is not unique, the menu structure can be used to look up the proper transformation. Once the transformation is found, SPT will determine if it requires the raw AST, or the analyzed AST. If the raw AST is required, it will only parse the fragment. If the analyzed AST is required, it will also analyze the parse result. However, analysis is allowed to produce any number and severity of messages. Then, SPT will run the transformation on the entire AST, including the nodes from the test fixture, if there was one. Expectation.Transform = <transform <STRING>> The STRING should be delimited by double quotes and contain the name of the transformation. If the name is not unique, the menu structure can be included as well, seperated by -> . For example: transform \"Menu name -> transformation name\" to Some(Result()) . As long as the transformation returns a result, this expectation passes. Expectation.TransformToAterm = <transform <STRING> to <ATerm>> Same as Transform , but the result of the transformation is compared to the given AST. Expectation.TransformToFragment = <transform <STRING> to <Language?> <OpenMarker> <Fragment> <CloseMarker>> Does the same as TransformToAterm , but compares the result of the transformation to the AST of the given fragment. If the applied transformation required the raw AST, the given fragment will only be parsed with the given language. If no language is given, the language under test will be used. If the applied transformation required an analyzed AST, the given fragment will be parsed and analyzed.","title":"Transformation Expectations"},{"location":"references/testing/test-expectations/#run-stratego-expectations","text":"These test expectations are really only applicable to languages that use Stratego strategies in their implementation. They will parse and analyze the fragment and run a given Stratego strategy (with no arguments) and compare its output to the expectation. Expectation.Run = <run <STRATEGY>> This expectation will lookup the given strategy name and run it on the AST node in the test's fragment. If the fragment contains multiple nodes (e.g., it's a list of Statements but some Statements were in the test fixture) the strategy will be run on each of these nodes. Either until it completes successfully, or until it failed on all these nodes. Note that it wil not be executed on the nodes in the test fixture, if there was one. Expectation.RunWithArgs = <run <STRATEGY>(|<TermArgs>)> This expectation will run a strategy that expects term arguments. String literals, integer literals and selection references are permitted as term arguments.: test rename variable without type [[ let var msg := \"Hello World\" in print([[msg]]) end ]] run rename(|#1, \"message\", 0) to [[ let var message := \"Hello World\" in print(message) end ]] Expectation.RunFails = <run <STRATEGY> fails> This expectation checks if the given strategy fails. Expectation.RunOn = <run <STRATEGY> on #<INT>> This expectation does the same as Run , except it runs the strategy on the nodes at the given selection instead of the nodes of the test's fragment. Expectation.RunToAterm = <run <STRATEGY> to <ATerm>> shorty Expectation.RunToAtermOn = <run <STRATEGY> on #<INT> to <ATerm>> These expectations are similar to the first two, but they require the result of running the strategy to match the given AST. Expectation.RunToFragment = <run <STRATEGY> to <Language?> <OpenMarker> <Fragment> <CloseMarker>> Expectation.RunToFragmentOn = <run <STRATEGY> on #<INT> to <Language?> <OpenMarker> <Fragment> <CloseMarker>> These expectations are similar to the first two, but they require the result of running the strategy to match the result of analyzing the given fragment with the given language. If no language is given, the language under test is used.","title":"Run Stratego Expectations"},{"location":"references/testing/test-expectations/#origin-location-expectations","text":"Expectation.HasOrigins = <has origin locations> This expectation parses and analyzes the fragment. It then checks if all AST nodes in the test's fragment (except for Lists in Spoofax) have a source region (an origin) associated with them. It does not check the AST nodes in the test fixture. When using Spoofax, there are some strategies that will break the origin information when used. This can lead to desugarings that create AST nodes without origin information, which can cause problems when trying to create messages at their location and with other services. This expectation can be used to check that your analysis is origin preserving.","title":"Origin Location Expectations"},{"location":"references/testing/test-suites/","text":"Test suites \u00b6 Test cases in SPT are grouped in test suites (or modules). Each SPT file contains exactly 1 test suite. Test suites allow you to group together the test cases that test similar aspects of your language, making it easier to organize your tests. They also allow you to reduce the size of your test cases by using configuration options like headers and test fixtures that apply to all test cases in the test suite. We will describe those later on. The syntax for a test suite is as follows: TestSuite = < module < MODULENAME > < Header ? > < TestFixture ? > < TestCase * > > The modulename must start with a letter and can be followed by any number of letters, digits, underscores, and dashes. If you are following along, you can create your first SPT test suite. Just create a file with the extension .spt and paste the following: module my-first-test-suite language MyLanguageName Be sure to replace MyLanguageName with the name of the language you want to test. Also, feel free to use a more descriptive module name if you like. Headers \u00b6 Headers are a configuration option for all test cases in a test suite. The most important header is the language header. If you are following along with the example, you have already used it to specify which language you wanted to test. All test cases in the test suite will be ran against the language you specify with the language header. This language will be called the language under test (LUT): Header.Language = <language <LANGNAME>> For now, there is only one other header: the start symbol header. This optional header allows you to specify a nonterminal from your grammar from which the test cases will be parsed. Don't worry if that doesn't make any sense yet. We will look at an example later: Header.StartSymbol = <start symbol <ID>> Where the ID must start with a letter, and can be followed by any number of letters or digits. As our example test suite already contains the language header, we will move on to writing our first test. Test Cases \u00b6 Test cases are the most important parts of SPT. Each test case is a behaviorial test, or black-box test, for your language. A behavioral test consists of a component that should be tested, an initial state for that component, input for that component, and the expected output after running the component on the input. First, let's look at the input of a test case. As we are testing languages, the input of a test case is always a program written in the language under test. Such a program written in the language under test is called a fragment , and it is embedded in the SPT test suite. In the future, we want to offer all editor services of your language under test (e.g., syntax highlighting) while you are writing such a fragment. However, for now this is not yet supported. The component that should be tested and the expected output are captured in test expectations . We will discuss those in a later section. Finally, the initial state of the test case can be specified in a test fixture , analogous to the JUnit setUp and tearDown methods. These fixtures will also be discussed in a later section. The syntax of test case is as follows: TestCase = < test <Description> <OpenMarker> <Fragment> <CloseMarker> <Expectation*> > Where Description is the name of the test and can contain any character that is not a newline or [ . The OpenMarker marks the start of the fragment and can be one of [[ , [[[ , or [[[[ . The CloseMarker marks the end of the fragment and should be the counter part of the OpenMarker , so either ]] , ]]] , or ]]]] . The Fragment is a piece of text written in the language under test, about which we want to reason in our test case. It may contain any sequence of characters that are not open- or closing markers. For example, if the [[[ and ]]] markers are used, the fragment may contain at most 2 consecutive open or closing square brackets. If the [[[[ and ]]]] markers are used, the fragment may contain at most 3 consecutive open or closing square brackets. Parts of the fragment may be selected, by surrounding the text with the same open- and closing markers that were used to delimit the fragment. These selections can later be referred to in the test expectations to allow more precise reasoning about the fragment. We will discuss selections and expectations later, but note that not supplying any test expectation is equivalent to supplying only a parse succeeds expectation, indicating that the fragment is expected to be valid syntax and parsing it with the language under test is expected to succeed without errors. If you are following along with your own test suite, let's create our first test case. I will be using a simplified version of Java (called MiniJava) as my language under test, but it shouldn't be too hard to write a simple test for your own language. We will be writing a MiniJava program with valid syntax and test that it does indeed parse. The input for our test case will simply be a main class in MiniJava. The component that we will be testing is the parser, and the expected output is a successful parse result: module simple-parse-tests language MiniJava test check if this simple program parses sucessfully [[ class Main { public static void main(String[] args) { System.out.println(42); } } ]] parse succeeds Change the language header to refer to the name of your language and change the fragment to be a valid specification in your language, and you should have your first working test case. Try messing up the syntax of your fragment and an error message should be displayed to indicate that the test failed, as the fragment failed to be parsed correctly. These error messages can be displayed directly inside the fragment you wrote, to make it easier for you to spot why the test failed. This is the power of SPT fragments! Now that we know the basic structure of a test, we can already see how the start symbol header can be used to decrease the size of our test: module statement-parse-tests language MiniJava start symbol Statement test check if a printline is a valid statement [[ System.out.println(42); ]] Note how the fragment is now no longer a proper MiniJava program. The test still passes, as the fragment is now parsed starting from the Statement nonterminal. Note that this only works if Statement is exported as a start symbol in the MiniJava language. These start symbols are a way of indicating what the initial state of the component under test should be. In this case, it influences the state of the parser and only allows it to successfully parse statements. Before we move on to discuss the set of all supported test expectations, we will first look at another way to influence the initial state and reduce the size of our test cases: test fixtures. Test Fixtures \u00b6 A test fixture offers a template for all test cases in the test suite. Using test fixtures, you can factor out common boilerplate from your tests and write it only once. The syntax for a test fixture is as follows: TestFixture = < fixture <OpenMarker> <StringPart> <OpenMarker> ... <CloseMarker> <StringPart> <CloseMarker> > Where the OpenMarker is one of [[ , [[[ , or [[[[ , and the CloseMarker is one of ]] , ]]] , or ]]]] . The StringPart can contain any sequence of characters that is not an open- or closing marker, just like a fragment from a test. However, unlike a fragment of a test, it can not contain selections. For each test case, the fragment of the test will be inserted into the fixture at the marked location ( <OpenMarker> ... <CloseMarker> ), before the test expectations will be evaluated. We can now use a test fixture to test the syntax of statements in MiniJava without the use of the start symbol header: module statements-parse-test language MiniJava fixture [[ class Main { public static void main(String[] args) { [[...]] } } ]] test check if printing an integer is allowed [[ System.out.println(42); ]] test check if printing a String is allowed [[ System.out.println(\"42\"); ]] Note that test fixtures offer a fully language implementation agnostic way of factoring out boiler plate code, whereas the start symbol header requires knowledge of the non terminals of the language implementation.","title":"Test suites"},{"location":"references/testing/test-suites/#test-suites","text":"Test cases in SPT are grouped in test suites (or modules). Each SPT file contains exactly 1 test suite. Test suites allow you to group together the test cases that test similar aspects of your language, making it easier to organize your tests. They also allow you to reduce the size of your test cases by using configuration options like headers and test fixtures that apply to all test cases in the test suite. We will describe those later on. The syntax for a test suite is as follows: TestSuite = < module < MODULENAME > < Header ? > < TestFixture ? > < TestCase * > > The modulename must start with a letter and can be followed by any number of letters, digits, underscores, and dashes. If you are following along, you can create your first SPT test suite. Just create a file with the extension .spt and paste the following: module my-first-test-suite language MyLanguageName Be sure to replace MyLanguageName with the name of the language you want to test. Also, feel free to use a more descriptive module name if you like.","title":"Test suites"},{"location":"references/testing/test-suites/#headers","text":"Headers are a configuration option for all test cases in a test suite. The most important header is the language header. If you are following along with the example, you have already used it to specify which language you wanted to test. All test cases in the test suite will be ran against the language you specify with the language header. This language will be called the language under test (LUT): Header.Language = <language <LANGNAME>> For now, there is only one other header: the start symbol header. This optional header allows you to specify a nonterminal from your grammar from which the test cases will be parsed. Don't worry if that doesn't make any sense yet. We will look at an example later: Header.StartSymbol = <start symbol <ID>> Where the ID must start with a letter, and can be followed by any number of letters or digits. As our example test suite already contains the language header, we will move on to writing our first test.","title":"Headers"},{"location":"references/testing/test-suites/#test-cases","text":"Test cases are the most important parts of SPT. Each test case is a behaviorial test, or black-box test, for your language. A behavioral test consists of a component that should be tested, an initial state for that component, input for that component, and the expected output after running the component on the input. First, let's look at the input of a test case. As we are testing languages, the input of a test case is always a program written in the language under test. Such a program written in the language under test is called a fragment , and it is embedded in the SPT test suite. In the future, we want to offer all editor services of your language under test (e.g., syntax highlighting) while you are writing such a fragment. However, for now this is not yet supported. The component that should be tested and the expected output are captured in test expectations . We will discuss those in a later section. Finally, the initial state of the test case can be specified in a test fixture , analogous to the JUnit setUp and tearDown methods. These fixtures will also be discussed in a later section. The syntax of test case is as follows: TestCase = < test <Description> <OpenMarker> <Fragment> <CloseMarker> <Expectation*> > Where Description is the name of the test and can contain any character that is not a newline or [ . The OpenMarker marks the start of the fragment and can be one of [[ , [[[ , or [[[[ . The CloseMarker marks the end of the fragment and should be the counter part of the OpenMarker , so either ]] , ]]] , or ]]]] . The Fragment is a piece of text written in the language under test, about which we want to reason in our test case. It may contain any sequence of characters that are not open- or closing markers. For example, if the [[[ and ]]] markers are used, the fragment may contain at most 2 consecutive open or closing square brackets. If the [[[[ and ]]]] markers are used, the fragment may contain at most 3 consecutive open or closing square brackets. Parts of the fragment may be selected, by surrounding the text with the same open- and closing markers that were used to delimit the fragment. These selections can later be referred to in the test expectations to allow more precise reasoning about the fragment. We will discuss selections and expectations later, but note that not supplying any test expectation is equivalent to supplying only a parse succeeds expectation, indicating that the fragment is expected to be valid syntax and parsing it with the language under test is expected to succeed without errors. If you are following along with your own test suite, let's create our first test case. I will be using a simplified version of Java (called MiniJava) as my language under test, but it shouldn't be too hard to write a simple test for your own language. We will be writing a MiniJava program with valid syntax and test that it does indeed parse. The input for our test case will simply be a main class in MiniJava. The component that we will be testing is the parser, and the expected output is a successful parse result: module simple-parse-tests language MiniJava test check if this simple program parses sucessfully [[ class Main { public static void main(String[] args) { System.out.println(42); } } ]] parse succeeds Change the language header to refer to the name of your language and change the fragment to be a valid specification in your language, and you should have your first working test case. Try messing up the syntax of your fragment and an error message should be displayed to indicate that the test failed, as the fragment failed to be parsed correctly. These error messages can be displayed directly inside the fragment you wrote, to make it easier for you to spot why the test failed. This is the power of SPT fragments! Now that we know the basic structure of a test, we can already see how the start symbol header can be used to decrease the size of our test: module statement-parse-tests language MiniJava start symbol Statement test check if a printline is a valid statement [[ System.out.println(42); ]] Note how the fragment is now no longer a proper MiniJava program. The test still passes, as the fragment is now parsed starting from the Statement nonterminal. Note that this only works if Statement is exported as a start symbol in the MiniJava language. These start symbols are a way of indicating what the initial state of the component under test should be. In this case, it influences the state of the parser and only allows it to successfully parse statements. Before we move on to discuss the set of all supported test expectations, we will first look at another way to influence the initial state and reduce the size of our test cases: test fixtures.","title":"Test Cases"},{"location":"references/testing/test-suites/#test-fixtures","text":"A test fixture offers a template for all test cases in the test suite. Using test fixtures, you can factor out common boilerplate from your tests and write it only once. The syntax for a test fixture is as follows: TestFixture = < fixture <OpenMarker> <StringPart> <OpenMarker> ... <CloseMarker> <StringPart> <CloseMarker> > Where the OpenMarker is one of [[ , [[[ , or [[[[ , and the CloseMarker is one of ]] , ]]] , or ]]]] . The StringPart can contain any sequence of characters that is not an open- or closing marker, just like a fragment from a test. However, unlike a fragment of a test, it can not contain selections. For each test case, the fragment of the test will be inserted into the fixture at the marked location ( <OpenMarker> ... <CloseMarker> ), before the test expectations will be evaluated. We can now use a test fixture to test the syntax of statements in MiniJava without the use of the start symbol header: module statements-parse-test language MiniJava fixture [[ class Main { public static void main(String[] args) { [[...]] } } ]] test check if printing an integer is allowed [[ System.out.println(42); ]] test check if printing a String is allowed [[ System.out.println(\"42\"); ]] Note that test fixtures offer a fully language implementation agnostic way of factoring out boiler plate code, whereas the start symbol header requires knowledge of the non terminals of the language implementation.","title":"Test Fixtures"},{"location":"release/develop/","text":"Development Releases \u00b6 Use the development releases of Spoofax only if you want to be on the cutting-edge of Spoofax development. Choose the Eclipse Bundle installation (recommended), the Eclipse Plugin installation, or the From Source installation: Eclipse Bundle with JRE (recommended) Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Installation instructions . Eclipse Bundle Download an Eclipse instance (without JRE) and the latest Spoofax plugin pre-installed for your platform: macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: https://buildfarm.metaborg.org/job/metaborg/job/spoofax-releng/job/master/lastSuccessfulBuild/artifact/dist/spoofax/eclipse/site/ Installation instructions . From Source Use Git to clone the Spoofax Github repository : HTTPS git clone https://github.com/metaborg/spoofax-releng.git HTTPS git clone git@github.com:metaborg/spoofax-releng.git GitHub CLI gh repo clone metaborg/spoofax-releng Installation instructions .","title":"Development Releases"},{"location":"release/develop/#development-releases","text":"Use the development releases of Spoofax only if you want to be on the cutting-edge of Spoofax development. Choose the Eclipse Bundle installation (recommended), the Eclipse Plugin installation, or the From Source installation: Eclipse Bundle with JRE (recommended) Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Installation instructions . Eclipse Bundle Download an Eclipse instance (without JRE) and the latest Spoofax plugin pre-installed for your platform: macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: https://buildfarm.metaborg.org/job/metaborg/job/spoofax-releng/job/master/lastSuccessfulBuild/artifact/dist/spoofax/eclipse/site/ Installation instructions . From Source Use Git to clone the Spoofax Github repository : HTTPS git clone https://github.com/metaborg/spoofax-releng.git HTTPS git clone git@github.com:metaborg/spoofax-releng.git GitHub CLI gh repo clone metaborg/spoofax-releng Installation instructions .","title":"Development Releases"},{"location":"release/stable/","text":"Stable Releases \u00b6 This page lists the latest stable release of Spoofax: version 2.5.16 released on 04-06-2021 . Choose the Eclipse Bundle installation (recommended), the Eclipse Plugin installation, or the Homebrew installation ( macOS only): Eclipse Bundle with JRE (recommended) Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Installation instructions . Eclipse Bundle Download an Eclipse instance (without JRE) and the latest Spoofax plugin pre-installed for your platform: macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ Installation instructions . Homebrew On macOS Spoofax can be installed easily using Homebrew . Install the latest release of Spoofax Eclipse as follows: brew tap metaborg/metaborg brew install --cask spoofax The optional command-line tools are installed with: brew install strategoxt Upgrading the Spoofax cask is not recommended Upgrading the Spoofax cask using brew cask upgrade --greedy will lose all manually installed plugins. It is recommended to use Eclipse update sites to keep Spoofax up-to-date.","title":"Stable Releases"},{"location":"release/stable/#stable-releases","text":"This page lists the latest stable release of Spoofax: version 2.5.16 released on 04-06-2021 . Choose the Eclipse Bundle installation (recommended), the Eclipse Plugin installation, or the Homebrew installation ( macOS only): Eclipse Bundle with JRE (recommended) Download an Eclipse instance with an embedded Java Runtime Environment (JRE) and the latest Spoofax plugin pre-installed for your platform: + macOS Intel (64-bit) + Linux x64 (64-bit) + Windows x64 (64-bit) + Windows x86 (32-bit) Installation instructions . Eclipse Bundle Download an Eclipse instance (without JRE) and the latest Spoofax plugin pre-installed for your platform: macOS Intel (64-bit) Linux x64 (64-bit) Windows x64 (64-bit) Windows x86 (32-bit) Installation instructions . Eclipse Plugin Perform a manual installation of the Spoofax plugin in Eclipse 3.5 or newer through the update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ Installation instructions . Homebrew On macOS Spoofax can be installed easily using Homebrew . Install the latest release of Spoofax Eclipse as follows: brew tap metaborg/metaborg brew install --cask spoofax The optional command-line tools are installed with: brew install strategoxt Upgrading the Spoofax cask is not recommended Upgrading the Spoofax cask using brew cask upgrade --greedy will lose all manually installed plugins. It is recommended to use Eclipse update sites to keep Spoofax up-to-date.","title":"Stable Releases"},{"location":"release/migrate/2.0.0/","text":"Spoofax 2.0.0 Migration Guide \u00b6 This migration guide describes the differences between Spoofax 1.5 and 2.0 and describes the steps to convert a Spoofax 1.5 project to Spoofax 2.0 project. To gather the required knowledge for migrating a language project, go through the documentation in the following order: Spoofax 2.0 Release Notes , for a general overview of the changes in Spoofax 2.0. This document, for the concrete differences and steps to convert your Spoofax project. Differences \u00b6 Concepts \u00b6 Spoofax 2.0 introduces several new concepts and terminology. A language specification is the specification of a language using meta-languages. A language specification project specifies a language component . When the specification is compiled, the result is a component which can be loaded into Spoofax. A language component has specifications and implementations for parts of a language, such as its parser, pretty-printer, analysis, transformations, editor services, etc. A component contributes these parts to a language implementation . Multiple components can contribute to the same language implementation, and components can contribute to multiple language implementations. In the most simple case, a single component contributes all parts of the language to a single implementation. Language components can depend on other language components to depend on parts of a language. Currently, there are two kinds of dependencies: compile and source dependencies. A compile dependency on a language component is used to compile source files of that language component. For example, a compile dependency on NaBL will ensure that all .nab files are compiled into .str files. A source dependency is used to depend on source files of a language component. Source dependencies are used to depend on libraries, for example to depend on a Stratego library for name and type analysis. They are also used to compose multiple language components into a single language component, for example to do language extension. A language is the more abstract notion of a language, which has multiple language implementations. For example, the Java language has the JDK7 and JDK8 implementations , which each have front-end and back-end components . An end-user project is a project with programs of an end-user language, in contrast to a language specification project which has programs of meta-languages. For example, a Java project is a Java end-user project, whereas the JDK project is a language specification project. Project structure \u00b6 The project structure of language specification projects has significantly changed. The biggest change is that these projects are no longer Eclipse (plugin) projects, so that they can be used outside of the Eclipse platform as well. Ant build files have also been removed since we do not use Ant to build projects any more. Many ESV files have been deprecated, and all generated ESV files in the editor directory have been removed. The following files and directories are no longer part of the project structure: Ant build: .externalToolBuilders , build.generated.xml , build.main.xml Eclipse plugin: plugin.xml , META-INF Eclipse project: .settings , .classpath , .project , build.properties Refactoring: lib/refactor-common.generated.str Deprecated ESV files: editor/langname-Completions.esv , editor/langname-Folding.esv , editor/langname-Outliner.str , editor/langname-Refactorings.esv Generated ESV files: editor/langname-*.generated.esv , editor/langname-Outliner.generated.str The RTG and signatures files are no longer generated for SDF3 projects, since SDF3 generates its own signatures. The generated box pp files are no longer generated, and box pp files are no longer converted into pp.af files. The following files and directories have been moved: ESV Main ESV file must be at editor/Main.esv . If it does not exist, no packed ESV file will be generated. Packed ESV file: target/metaborg/editor.esv.af SDF Definition: src-gen/syntax/[LanguageName].def Permissive definition: src-gen/syntax/[LanguageName]-permissive.def Parenthesizer: src-gen/pp/[LanguageName]-parenthesize.str Parse table: target/metaborg/sdf.tbl Stratego 'editor-common.generated' file: src-gen/stratego/metaborg.str Ctree: target/metaborg/stratego.ctree Generated Java files: src-gen/stratego-java JAR: target/metaborg/stratego.jar Java strategies: src/main/strategies Java strategies JAR: target/metaborg/stratego-javastrat.jar Build cache: target/stratego-cache DynSem Manual Java: src/main/ds Generated Java: src-gen/ds-java The following generated files and directories still exist, but should not be published to source control any more: lib/editor-common.generated.str or stratego/metaborg.str src-gen When importing a language specification project into Eclipse or IntelliJ, several platform-specific files will be generated. These files should not be published to source control to keep projects as platform-agnostic as possible. Eclipse \u00b6 Importing \u00b6 To import a language specification project into Eclipse, use Import... \u2023 Maven \u2023 Existing Maven Projects . We use Maven to set up the correct Java dependencies, which is why there is no special 'Existing Spoofax Projects' importer. Builds \u00b6 Eclipse has the concept of incremental project builders, which incrementally parse, analyze, and compile files inside a project. An example of such a project builder is the Eclipse JDT builder which incrementally builds Java files. Spoofax 1.5 did not use this functionality, but the new Eclipse plugin in Spoofax 2.0 does. The project builder for Spoofax parses, analyses, executes transformations, and shows all error markers, for all language files (Stratego files, SDF3 files, files of your language, etc.) in the project. If the project is opened for the first time, a full build will occur, building all language files. When changes occur in the project, an incremental build occurs, building only changed files. The commands under the Project menu in Eclipse now also affect Spoofax projects. Executing Project \u2023 Build... (or pressing Ctrl/Cmd+Alt+B ) will build the project. Executing Project \u2023 Clean... will delete the .cache directory, reset the index and task engine, remove all error markers, and reanalyze and rebuild all files in the project. This makes the Reset and Reanalyze builder unnecessary, since this is now properly integrated with Eclipse. Automatic building can also be turned off by disabling Project \u2023 Build Automatically . Builds will then only occur if Project \u2023 Build Project is executed or if Ctrl/Cmd+Alt+B is pressed. Furthermore, the language specification build is no longer written in Ant, but in Java using the Pluto incremental build system. Natures \u00b6 The Spoofax language specification project builder is not enabled by default, to enable it a 'Spoofax meta nature' must be added to the project. A nature in Eclipse is a project tag which enables functionality for that project. To add the Spoofax nature to a project, right click the project, and choose Spoofax (meta) \u2023 Add Spoofax meta nature . When importing a language specification, this nature is automatically added. For end-user projects, right click the project, and choose Spoofax \u2023 Add Spoofax nature to add a nature for end-user projects. Editors will parse and analyze files regardless of there being a Spoofax nature, but the on-save handler will not be called. Builders \u00b6 Builders for the open editor are now located in the Spoofax main menu instead of buttons on the tool bar. Builders wait for the analyzed AST if needed, so the issue where builders are sometimes not executed on the analyzed AST should be solved now. Builders can also be executed on a set of files by selecting the files in the project or package explorer, right clicking the files, selecting the language name from the menu, and then selecting a builder. Cancellation \u00b6 Editor updates can now be cancelled by clicking the red stop button in the progress view. If the progress view is not visible, you can open it by choosing Window \u2023 Show View \u2023 Progress . If the editor update is not responsive (it is looping for example), the thread running the editor update will be killed after a while. Killing a thread during analysis may leave the index and task engine in an inconsistent state. If this happens, clean the project using Project \u2023 Clean... to force a full build, which makes the state consistent again. Killing a thread is not very well supported in Java and may break Eclipse or even the JVM, which then requires a restart. Project builds and transformations can also be cancelled in the progress view. Console logging \u00b6 Console logging in the new plugin is more prominent so that we can diagnose problems more easily. If the console is not visible, you can open it by choosing Window \u2023 Show View \u2023 Console . The console does not automatically pop-up when there is a message any more, so it can also be hidden by just closing it. All warning and error messages are also sent to Eclipse's error log. The error log can sometimes contain more information about exceptions and stack traces in errors. If the error log is not visible, you can open it by choosing Window \u2023 Show View \u2023 Error Log . Manually loading/unloading a language \u00b6 A language can be manually loaded or reloaded by right clicking a project and choosing Spoofax (meta) \u2023 Load language , and unloaded with Spoofax (meta) \u2023 Unload language . External dependencies \u00b6 The new plugin does not depend on a modified version of IMP, making it possible to install the Rascal plugin alongside the Spoofax plugin. All other external dependencies are limited to the Spoofax plugin, which should prevent conflicts with other Eclipse plugins. Converting a project \u00b6 If your project is simple (e.g. it only has syntax and a few transformations), the easiest way to convert your project is to create a new Spoofax language specification project, and to copy your files into that project. Otherwise, Spoofax 2.0 supports converting an old Spoofax project into a new Spoofax project, but some conversions need to be done manually. Warning Converting a Spoofax project is a destructive operation, some files will be deleted, replaced, or changed. Make a backup of your projects before doing any conversions! Automatic conversion \u00b6 First, import your existing Spoofax project into Eclipse using File \u2023 Import... \u2023 Existing Projects into Workspace . Right click the project, and choose Spoofax (meta) \u2023 Upgrade language project . A wizard screen will pop up where you have to fill in some details about your language. If a packed.esv file was found, Spoofax will try to fill in some fields for you. If not, all fields need to be filled in manually. The id and name of your language can be found in the main ESV file. For group id , use a Maven identifier for your organization (e.g. org.metaborg), and as version : 1.0.0-SNAPSHOT. Warning Make sure that the id and name fields match exactly with the fields in your ESV file, otherwise the conversion will go wrong. Press finish to convert the language project. Manual conversion \u00b6 Unfortunately, not all required conversions can be done automatically. Do the following conversions manually. Project configuration \u00b6 Most of the project configuration is now in the metaborg.yaml file. The manual page on configuration lists all configuration options. Add/remove compile and source dependencies as needed. Add build configuration, such as Stratego compiler arguments, SDF compiler arguments, external def files, and external JAR files. Imports \u00b6 In Stratego, SDF2, SDF3, NaBL, and TS files: Remove src-gen , lib , and trans , from module names and imports. These paths are now on the source path of the SDF and Stratego compilers. In Stratego, NaBL, and TS files: Instead of importing lib/editor-common.generated , import stratego/metaborg . Instead of importing include/<langname>-parenthesize , import pp/<langname>-parenthesize . If you're using SDF3: Instead of importing the signatures from include/<langname> , import them from signatures/<langname>-sig . These signatures are spread over multiple files, import all the required files to fix errors, since the Stratego editor does not handle transitive imports. You can also use the wildcard import signatures/- to import all signature files, if your syntax definition is not spread over multiple directories. If you're using SDF2 or an external definition file: Instead of importing the signatures from include/<langname> , import th em from signatures/<langname> . SDF \u00b6 If you are still using SDF2 instead of SDF3, add the following setting to the metaborg.yaml file: language : sdf : version : sdf2 NaBL and TS \u00b6 If you\u2019re using a NaBL/TS based analysis, perform the following changes: NaBL files are now generated into src-gen/names , fix imports to NaBL files, delete old generated NaBL files. TS files are now generated into src-gen/types , fix imports to TS files, delete old generated TS files. The editor-analyze calls have been changed. Remove analysis-single-default-interface , analysis-multiple-default-interface , and editor-analyze . Replace it with: editor-analyze = analyze-all ( pre-analysis , post-analysis , pp-message |< language >) with the pre-analysis , post-analysis , and pp-message arguments that you were using before. Also make sure the observer (in your esv) has the (multifile) property. The editor-save call to analysis-save-default(|<language>) has been removed, remove that call. You can remove editor-save entirely if you don\u2019t do any generation, also remove the on save strategy from ESV if you do. If you do generation but do not return a (file, text) tuple from editor-save, be sure to return a !None() to tell Spoofax that you\u2019re returning nothing. The index-setup and task-setup strategies have been removed, Spoofax Core does this for you now. Remove all calls to these strategies. Remove the path argument to analysis-resolve in editor-resolve . Remove the path argument to analysis-propose-completions in editor-complete . Remove the debug-reanalyze strategy, and remove it from your menu in ESV. You can reanalyze by cleaning the project. ESV \u00b6 The following ESV files are now deprecated, delete and remove any imports to these files: editor/langname-Completions.esv editor/langname-Folding.esv editor/langname-Refactorings.esv Previously generated ESV files in the editor directory are not generated any more. Delete the generated files and remove the imports to generated files. The colorer ESV file is now generated to src-gen/editor/Colorer.generated.esv , import it with imports editor/Colorer.generated in an ESV file. The generated syntax ESV file is no longer generated. If you were using the defaults from the generated file, add them to an ESV file: language line comment : \"//\" block comment : \"/*\" * \"*/\" fences : [ ] ( ) { } The outliner ( editor/langname-Outliner.str ) must be moved to the trans directory. Rename it to trans/outline.str , change its module to outline , and fix imports of the outliner. Change the file name of the main ESV file to Main.esv , and change its module to Main . In the main ESV file: Change the parse table: table : target/metaborg/sdf . tbl Change the Stratego providers For ctree projects: provider : target/metaborg/stratego . ctree For jar projects: provider : target/metaborg/stratego . jar For projects with Java strategies: provider : target/metaborg/stratego . jar provider : target/metaborg/stratego-javastrat . jar Java strategies \u00b6 If your project has Java strategies: Create the src/main/strategies directory. Move Java strategies from editor/java into the src/main/strategies directory. Be sure to preserve the existing Java package structure. Perform a Maven update by right clicking the project and choosing Maven \u2023 Update Project... , to update the Java source directories of the project. DynSem \u00b6 If your project has manual DynSem Java files: Create the src/main/ds directory. Move manual DynSem Java files from editor/java into the src/main/ds directory. Be sure to preserve the existing Java package structure. Perform a Maven update by right clicking the project and choosing Maven \u2023 Update Project... , to update the Java source directories of the project. Ant build customization \u00b6 Language specification builds do not use Ant any more, so any customizations to the build.main.xml are lost. To perform an Ant task before and after the build, add the following configuration option to your metaborg.yaml file: build : ant : - phase : preCompile file : ${path:root}/ant.xml target : generate-xml - phase : postCompile file : ${path:root}/ant.xml target : package-xml See the manual page on configuration for more information about configuring Ant build steps. Eclipse plugin \u00b6 Language specification projects are not Eclipse plugins anymore. Git \u00b6 If you're using Git, the .gitignore file is replaced with a new one, add entries that you need again. All generated files that were previously not ignored, are ignored now. To delete all ignored files from the Git index (the files will remain on the filesystem but Git will forget about them), make sure all your useful changes are committed and pushed, then run the following commands: git rm -r --cached . git add . git commit -am \"Remove ignored files\" Building \u00b6 When you are done with converting the project, build it with Cmd+Shift+B or Project \u2023 Build . If the build does not work, try cleaning the project first with Project \u2023 Clean , and then building again. Also make sure that Project \u2023 Build Automatically is turned on.","title":"Spoofax 2.0.0 Migration Guide"},{"location":"release/migrate/2.0.0/#spoofax-200-migration-guide","text":"This migration guide describes the differences between Spoofax 1.5 and 2.0 and describes the steps to convert a Spoofax 1.5 project to Spoofax 2.0 project. To gather the required knowledge for migrating a language project, go through the documentation in the following order: Spoofax 2.0 Release Notes , for a general overview of the changes in Spoofax 2.0. This document, for the concrete differences and steps to convert your Spoofax project.","title":"Spoofax 2.0.0 Migration Guide"},{"location":"release/migrate/2.0.0/#differences","text":"","title":"Differences"},{"location":"release/migrate/2.0.0/#concepts","text":"Spoofax 2.0 introduces several new concepts and terminology. A language specification is the specification of a language using meta-languages. A language specification project specifies a language component . When the specification is compiled, the result is a component which can be loaded into Spoofax. A language component has specifications and implementations for parts of a language, such as its parser, pretty-printer, analysis, transformations, editor services, etc. A component contributes these parts to a language implementation . Multiple components can contribute to the same language implementation, and components can contribute to multiple language implementations. In the most simple case, a single component contributes all parts of the language to a single implementation. Language components can depend on other language components to depend on parts of a language. Currently, there are two kinds of dependencies: compile and source dependencies. A compile dependency on a language component is used to compile source files of that language component. For example, a compile dependency on NaBL will ensure that all .nab files are compiled into .str files. A source dependency is used to depend on source files of a language component. Source dependencies are used to depend on libraries, for example to depend on a Stratego library for name and type analysis. They are also used to compose multiple language components into a single language component, for example to do language extension. A language is the more abstract notion of a language, which has multiple language implementations. For example, the Java language has the JDK7 and JDK8 implementations , which each have front-end and back-end components . An end-user project is a project with programs of an end-user language, in contrast to a language specification project which has programs of meta-languages. For example, a Java project is a Java end-user project, whereas the JDK project is a language specification project.","title":"Concepts"},{"location":"release/migrate/2.0.0/#project-structure","text":"The project structure of language specification projects has significantly changed. The biggest change is that these projects are no longer Eclipse (plugin) projects, so that they can be used outside of the Eclipse platform as well. Ant build files have also been removed since we do not use Ant to build projects any more. Many ESV files have been deprecated, and all generated ESV files in the editor directory have been removed. The following files and directories are no longer part of the project structure: Ant build: .externalToolBuilders , build.generated.xml , build.main.xml Eclipse plugin: plugin.xml , META-INF Eclipse project: .settings , .classpath , .project , build.properties Refactoring: lib/refactor-common.generated.str Deprecated ESV files: editor/langname-Completions.esv , editor/langname-Folding.esv , editor/langname-Outliner.str , editor/langname-Refactorings.esv Generated ESV files: editor/langname-*.generated.esv , editor/langname-Outliner.generated.str The RTG and signatures files are no longer generated for SDF3 projects, since SDF3 generates its own signatures. The generated box pp files are no longer generated, and box pp files are no longer converted into pp.af files. The following files and directories have been moved: ESV Main ESV file must be at editor/Main.esv . If it does not exist, no packed ESV file will be generated. Packed ESV file: target/metaborg/editor.esv.af SDF Definition: src-gen/syntax/[LanguageName].def Permissive definition: src-gen/syntax/[LanguageName]-permissive.def Parenthesizer: src-gen/pp/[LanguageName]-parenthesize.str Parse table: target/metaborg/sdf.tbl Stratego 'editor-common.generated' file: src-gen/stratego/metaborg.str Ctree: target/metaborg/stratego.ctree Generated Java files: src-gen/stratego-java JAR: target/metaborg/stratego.jar Java strategies: src/main/strategies Java strategies JAR: target/metaborg/stratego-javastrat.jar Build cache: target/stratego-cache DynSem Manual Java: src/main/ds Generated Java: src-gen/ds-java The following generated files and directories still exist, but should not be published to source control any more: lib/editor-common.generated.str or stratego/metaborg.str src-gen When importing a language specification project into Eclipse or IntelliJ, several platform-specific files will be generated. These files should not be published to source control to keep projects as platform-agnostic as possible.","title":"Project structure"},{"location":"release/migrate/2.0.0/#eclipse","text":"","title":"Eclipse"},{"location":"release/migrate/2.0.0/#importing","text":"To import a language specification project into Eclipse, use Import... \u2023 Maven \u2023 Existing Maven Projects . We use Maven to set up the correct Java dependencies, which is why there is no special 'Existing Spoofax Projects' importer.","title":"Importing"},{"location":"release/migrate/2.0.0/#builds","text":"Eclipse has the concept of incremental project builders, which incrementally parse, analyze, and compile files inside a project. An example of such a project builder is the Eclipse JDT builder which incrementally builds Java files. Spoofax 1.5 did not use this functionality, but the new Eclipse plugin in Spoofax 2.0 does. The project builder for Spoofax parses, analyses, executes transformations, and shows all error markers, for all language files (Stratego files, SDF3 files, files of your language, etc.) in the project. If the project is opened for the first time, a full build will occur, building all language files. When changes occur in the project, an incremental build occurs, building only changed files. The commands under the Project menu in Eclipse now also affect Spoofax projects. Executing Project \u2023 Build... (or pressing Ctrl/Cmd+Alt+B ) will build the project. Executing Project \u2023 Clean... will delete the .cache directory, reset the index and task engine, remove all error markers, and reanalyze and rebuild all files in the project. This makes the Reset and Reanalyze builder unnecessary, since this is now properly integrated with Eclipse. Automatic building can also be turned off by disabling Project \u2023 Build Automatically . Builds will then only occur if Project \u2023 Build Project is executed or if Ctrl/Cmd+Alt+B is pressed. Furthermore, the language specification build is no longer written in Ant, but in Java using the Pluto incremental build system.","title":"Builds"},{"location":"release/migrate/2.0.0/#natures","text":"The Spoofax language specification project builder is not enabled by default, to enable it a 'Spoofax meta nature' must be added to the project. A nature in Eclipse is a project tag which enables functionality for that project. To add the Spoofax nature to a project, right click the project, and choose Spoofax (meta) \u2023 Add Spoofax meta nature . When importing a language specification, this nature is automatically added. For end-user projects, right click the project, and choose Spoofax \u2023 Add Spoofax nature to add a nature for end-user projects. Editors will parse and analyze files regardless of there being a Spoofax nature, but the on-save handler will not be called.","title":"Natures"},{"location":"release/migrate/2.0.0/#builders","text":"Builders for the open editor are now located in the Spoofax main menu instead of buttons on the tool bar. Builders wait for the analyzed AST if needed, so the issue where builders are sometimes not executed on the analyzed AST should be solved now. Builders can also be executed on a set of files by selecting the files in the project or package explorer, right clicking the files, selecting the language name from the menu, and then selecting a builder.","title":"Builders"},{"location":"release/migrate/2.0.0/#cancellation","text":"Editor updates can now be cancelled by clicking the red stop button in the progress view. If the progress view is not visible, you can open it by choosing Window \u2023 Show View \u2023 Progress . If the editor update is not responsive (it is looping for example), the thread running the editor update will be killed after a while. Killing a thread during analysis may leave the index and task engine in an inconsistent state. If this happens, clean the project using Project \u2023 Clean... to force a full build, which makes the state consistent again. Killing a thread is not very well supported in Java and may break Eclipse or even the JVM, which then requires a restart. Project builds and transformations can also be cancelled in the progress view.","title":"Cancellation"},{"location":"release/migrate/2.0.0/#console-logging","text":"Console logging in the new plugin is more prominent so that we can diagnose problems more easily. If the console is not visible, you can open it by choosing Window \u2023 Show View \u2023 Console . The console does not automatically pop-up when there is a message any more, so it can also be hidden by just closing it. All warning and error messages are also sent to Eclipse's error log. The error log can sometimes contain more information about exceptions and stack traces in errors. If the error log is not visible, you can open it by choosing Window \u2023 Show View \u2023 Error Log .","title":"Console logging"},{"location":"release/migrate/2.0.0/#manually-loadingunloading-a-language","text":"A language can be manually loaded or reloaded by right clicking a project and choosing Spoofax (meta) \u2023 Load language , and unloaded with Spoofax (meta) \u2023 Unload language .","title":"Manually loading/unloading a language"},{"location":"release/migrate/2.0.0/#external-dependencies","text":"The new plugin does not depend on a modified version of IMP, making it possible to install the Rascal plugin alongside the Spoofax plugin. All other external dependencies are limited to the Spoofax plugin, which should prevent conflicts with other Eclipse plugins.","title":"External dependencies"},{"location":"release/migrate/2.0.0/#converting-a-project","text":"If your project is simple (e.g. it only has syntax and a few transformations), the easiest way to convert your project is to create a new Spoofax language specification project, and to copy your files into that project. Otherwise, Spoofax 2.0 supports converting an old Spoofax project into a new Spoofax project, but some conversions need to be done manually. Warning Converting a Spoofax project is a destructive operation, some files will be deleted, replaced, or changed. Make a backup of your projects before doing any conversions!","title":"Converting a project"},{"location":"release/migrate/2.0.0/#automatic-conversion","text":"First, import your existing Spoofax project into Eclipse using File \u2023 Import... \u2023 Existing Projects into Workspace . Right click the project, and choose Spoofax (meta) \u2023 Upgrade language project . A wizard screen will pop up where you have to fill in some details about your language. If a packed.esv file was found, Spoofax will try to fill in some fields for you. If not, all fields need to be filled in manually. The id and name of your language can be found in the main ESV file. For group id , use a Maven identifier for your organization (e.g. org.metaborg), and as version : 1.0.0-SNAPSHOT. Warning Make sure that the id and name fields match exactly with the fields in your ESV file, otherwise the conversion will go wrong. Press finish to convert the language project.","title":"Automatic conversion"},{"location":"release/migrate/2.0.0/#manual-conversion","text":"Unfortunately, not all required conversions can be done automatically. Do the following conversions manually.","title":"Manual conversion"},{"location":"release/migrate/2.0.0/#project-configuration","text":"Most of the project configuration is now in the metaborg.yaml file. The manual page on configuration lists all configuration options. Add/remove compile and source dependencies as needed. Add build configuration, such as Stratego compiler arguments, SDF compiler arguments, external def files, and external JAR files.","title":"Project configuration"},{"location":"release/migrate/2.0.0/#imports","text":"In Stratego, SDF2, SDF3, NaBL, and TS files: Remove src-gen , lib , and trans , from module names and imports. These paths are now on the source path of the SDF and Stratego compilers. In Stratego, NaBL, and TS files: Instead of importing lib/editor-common.generated , import stratego/metaborg . Instead of importing include/<langname>-parenthesize , import pp/<langname>-parenthesize . If you're using SDF3: Instead of importing the signatures from include/<langname> , import them from signatures/<langname>-sig . These signatures are spread over multiple files, import all the required files to fix errors, since the Stratego editor does not handle transitive imports. You can also use the wildcard import signatures/- to import all signature files, if your syntax definition is not spread over multiple directories. If you're using SDF2 or an external definition file: Instead of importing the signatures from include/<langname> , import th em from signatures/<langname> .","title":"Imports"},{"location":"release/migrate/2.0.0/#sdf","text":"If you are still using SDF2 instead of SDF3, add the following setting to the metaborg.yaml file: language : sdf : version : sdf2","title":"SDF"},{"location":"release/migrate/2.0.0/#nabl-and-ts","text":"If you\u2019re using a NaBL/TS based analysis, perform the following changes: NaBL files are now generated into src-gen/names , fix imports to NaBL files, delete old generated NaBL files. TS files are now generated into src-gen/types , fix imports to TS files, delete old generated TS files. The editor-analyze calls have been changed. Remove analysis-single-default-interface , analysis-multiple-default-interface , and editor-analyze . Replace it with: editor-analyze = analyze-all ( pre-analysis , post-analysis , pp-message |< language >) with the pre-analysis , post-analysis , and pp-message arguments that you were using before. Also make sure the observer (in your esv) has the (multifile) property. The editor-save call to analysis-save-default(|<language>) has been removed, remove that call. You can remove editor-save entirely if you don\u2019t do any generation, also remove the on save strategy from ESV if you do. If you do generation but do not return a (file, text) tuple from editor-save, be sure to return a !None() to tell Spoofax that you\u2019re returning nothing. The index-setup and task-setup strategies have been removed, Spoofax Core does this for you now. Remove all calls to these strategies. Remove the path argument to analysis-resolve in editor-resolve . Remove the path argument to analysis-propose-completions in editor-complete . Remove the debug-reanalyze strategy, and remove it from your menu in ESV. You can reanalyze by cleaning the project.","title":"NaBL and TS"},{"location":"release/migrate/2.0.0/#esv","text":"The following ESV files are now deprecated, delete and remove any imports to these files: editor/langname-Completions.esv editor/langname-Folding.esv editor/langname-Refactorings.esv Previously generated ESV files in the editor directory are not generated any more. Delete the generated files and remove the imports to generated files. The colorer ESV file is now generated to src-gen/editor/Colorer.generated.esv , import it with imports editor/Colorer.generated in an ESV file. The generated syntax ESV file is no longer generated. If you were using the defaults from the generated file, add them to an ESV file: language line comment : \"//\" block comment : \"/*\" * \"*/\" fences : [ ] ( ) { } The outliner ( editor/langname-Outliner.str ) must be moved to the trans directory. Rename it to trans/outline.str , change its module to outline , and fix imports of the outliner. Change the file name of the main ESV file to Main.esv , and change its module to Main . In the main ESV file: Change the parse table: table : target/metaborg/sdf . tbl Change the Stratego providers For ctree projects: provider : target/metaborg/stratego . ctree For jar projects: provider : target/metaborg/stratego . jar For projects with Java strategies: provider : target/metaborg/stratego . jar provider : target/metaborg/stratego-javastrat . jar","title":"ESV"},{"location":"release/migrate/2.0.0/#java-strategies","text":"If your project has Java strategies: Create the src/main/strategies directory. Move Java strategies from editor/java into the src/main/strategies directory. Be sure to preserve the existing Java package structure. Perform a Maven update by right clicking the project and choosing Maven \u2023 Update Project... , to update the Java source directories of the project.","title":"Java strategies"},{"location":"release/migrate/2.0.0/#dynsem","text":"If your project has manual DynSem Java files: Create the src/main/ds directory. Move manual DynSem Java files from editor/java into the src/main/ds directory. Be sure to preserve the existing Java package structure. Perform a Maven update by right clicking the project and choosing Maven \u2023 Update Project... , to update the Java source directories of the project.","title":"DynSem"},{"location":"release/migrate/2.0.0/#ant-build-customization","text":"Language specification builds do not use Ant any more, so any customizations to the build.main.xml are lost. To perform an Ant task before and after the build, add the following configuration option to your metaborg.yaml file: build : ant : - phase : preCompile file : ${path:root}/ant.xml target : generate-xml - phase : postCompile file : ${path:root}/ant.xml target : package-xml See the manual page on configuration for more information about configuring Ant build steps.","title":"Ant build customization"},{"location":"release/migrate/2.0.0/#eclipse-plugin","text":"Language specification projects are not Eclipse plugins anymore.","title":"Eclipse plugin"},{"location":"release/migrate/2.0.0/#git","text":"If you're using Git, the .gitignore file is replaced with a new one, add entries that you need again. All generated files that were previously not ignored, are ignored now. To delete all ignored files from the Git index (the files will remain on the filesystem but Git will forget about them), make sure all your useful changes are committed and pushed, then run the following commands: git rm -r --cached . git add . git commit -am \"Remove ignored files\"","title":"Git"},{"location":"release/migrate/2.0.0/#building","text":"When you are done with converting the project, build it with Cmd+Shift+B or Project \u2023 Build . If the build does not work, try cleaning the project first with Project \u2023 Clean , and then building again. Also make sure that Project \u2023 Build Automatically is turned on.","title":"Building"},{"location":"release/migrate/2.1.0/","text":"Spoofax 2.1.0 Migration Guide \u00b6 This migration guide describes the differences between Spoofax 2.0 and 2.1 and how to convert to 2.1. New Stratego library for Spoofax \u00b6 Historically, the org.metaborg.meta.lib.analysis library (also called the runtime libraries ) from this repo , was first used as a library to support NaBL, TS, and task engine based static analysis. However, a lot of other functionality such as completions, refactoring, origin tracking, and annotation handling, was also added to the library for convenience. Likewise, the src-gen/stratego/metaborg.str generated file also contains arbitrary functionality such as parsing and import path primitives, and the src-gen/editor/Colorer.generated generated file contains a default coloring scheme. Since this kind of functionality does not belong the analysis library and generated files, we have moved this into a new library, libspoofax , which can be found at this repo . Migration \u00b6 The org.metaborg.meta.lib.analysis library still contains the old arbitrary functionality, but is now deprecated , meaning we will not update that functionality any more, and that it will be removed in a future version. Any functionality pertaining NaBL, TS, and task engine based static analysis will of course be retained. Likewise, the src-gen/stratego/metaborg.str and src-gen/editor/Colorer.generated generated files are also deprecated , meaning that they will stop being generated in a future version. The new libspoofax library is now required for every Spoofax project. Add a source dependency to org.metaborg:meta.lib.spoofax:${metaborgVersion} in your metaborg.yaml file. For example, change the following dependencies: dependencies : compile : - org.metaborg:org.metaborg.meta.lang.esv:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.template:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.nabl:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.ts:${metaborgVersion} source : - org.metaborg:org.metaborg.meta.lib.analysis:${metaborgVersion} into: dependencies : compile : - org.metaborg:org.metaborg.meta.lang.esv:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.template:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.nabl:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.ts:${metaborgVersion} source : - org.metaborg:meta.lib.spoofax:${metaborgVersion} - org.metaborg:org.metaborg.meta.lib.analysis:${metaborgVersion} If you do not use NaBL, TS, and task engine based analysis any more, you can also delete the org.metaborg:org.metaborg.meta.lib.analysis:${metaborgVersion} source dependency. Some imports have to be changed to point to the new libspoofax library: In editor/Syntax.esv or your equivalent ESV file that handles coloring: Change import editor/Colorer.generated to libspoofax/color/default In trans/analysis.str for NaBL/TS projects: Add import libspoofax/core/language In trans/outline.str : Remove imports to the runtime libraries Add import libspoofax/editor/outline In trans/pp.str : Remove imports to the runtime libraries Add imports libspoofax/sdf/pp and libspoofax/editor/refactoring/- In other Stratego files: Remove all imports to the runtime libraries that do not pertain NaBL, TS, and task engine based static analysis, and replace them with libspoofax equivalents. Remove all imports to the stratego/metaborg generated file, and replace them with libspoofax equivalents. Here is a list of other imports and strategies that were moved: Imports: runtime/editor/origins -> libspoofax/term/origin runtime/editor/annotations -> libspoofax/term/annotation runtime/completion/- -> libspoofax/editor/completion/- Strategies: *-at-position : libspoofax/term/position parse-file : libspoofax/core/parse language : libspoofax/core/language project-path : libspoofax/resource/path open-import : libspoofax/resource/cache refresh-workspace-file : removed, not needed any more since all file system operations go though VFS, which routes them through Eclipse.","title":"Spoofax 2.1.0 Migration Guide"},{"location":"release/migrate/2.1.0/#spoofax-210-migration-guide","text":"This migration guide describes the differences between Spoofax 2.0 and 2.1 and how to convert to 2.1.","title":"Spoofax 2.1.0 Migration Guide"},{"location":"release/migrate/2.1.0/#new-stratego-library-for-spoofax","text":"Historically, the org.metaborg.meta.lib.analysis library (also called the runtime libraries ) from this repo , was first used as a library to support NaBL, TS, and task engine based static analysis. However, a lot of other functionality such as completions, refactoring, origin tracking, and annotation handling, was also added to the library for convenience. Likewise, the src-gen/stratego/metaborg.str generated file also contains arbitrary functionality such as parsing and import path primitives, and the src-gen/editor/Colorer.generated generated file contains a default coloring scheme. Since this kind of functionality does not belong the analysis library and generated files, we have moved this into a new library, libspoofax , which can be found at this repo .","title":"New Stratego library for Spoofax"},{"location":"release/migrate/2.1.0/#migration","text":"The org.metaborg.meta.lib.analysis library still contains the old arbitrary functionality, but is now deprecated , meaning we will not update that functionality any more, and that it will be removed in a future version. Any functionality pertaining NaBL, TS, and task engine based static analysis will of course be retained. Likewise, the src-gen/stratego/metaborg.str and src-gen/editor/Colorer.generated generated files are also deprecated , meaning that they will stop being generated in a future version. The new libspoofax library is now required for every Spoofax project. Add a source dependency to org.metaborg:meta.lib.spoofax:${metaborgVersion} in your metaborg.yaml file. For example, change the following dependencies: dependencies : compile : - org.metaborg:org.metaborg.meta.lang.esv:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.template:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.nabl:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.ts:${metaborgVersion} source : - org.metaborg:org.metaborg.meta.lib.analysis:${metaborgVersion} into: dependencies : compile : - org.metaborg:org.metaborg.meta.lang.esv:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.template:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.nabl:${metaborgVersion} - org.metaborg:org.metaborg.meta.lang.ts:${metaborgVersion} source : - org.metaborg:meta.lib.spoofax:${metaborgVersion} - org.metaborg:org.metaborg.meta.lib.analysis:${metaborgVersion} If you do not use NaBL, TS, and task engine based analysis any more, you can also delete the org.metaborg:org.metaborg.meta.lib.analysis:${metaborgVersion} source dependency. Some imports have to be changed to point to the new libspoofax library: In editor/Syntax.esv or your equivalent ESV file that handles coloring: Change import editor/Colorer.generated to libspoofax/color/default In trans/analysis.str for NaBL/TS projects: Add import libspoofax/core/language In trans/outline.str : Remove imports to the runtime libraries Add import libspoofax/editor/outline In trans/pp.str : Remove imports to the runtime libraries Add imports libspoofax/sdf/pp and libspoofax/editor/refactoring/- In other Stratego files: Remove all imports to the runtime libraries that do not pertain NaBL, TS, and task engine based static analysis, and replace them with libspoofax equivalents. Remove all imports to the stratego/metaborg generated file, and replace them with libspoofax equivalents. Here is a list of other imports and strategies that were moved: Imports: runtime/editor/origins -> libspoofax/term/origin runtime/editor/annotations -> libspoofax/term/annotation runtime/completion/- -> libspoofax/editor/completion/- Strategies: *-at-position : libspoofax/term/position parse-file : libspoofax/core/parse language : libspoofax/core/language project-path : libspoofax/resource/path open-import : libspoofax/resource/cache refresh-workspace-file : removed, not needed any more since all file system operations go though VFS, which routes them through Eclipse.","title":"Migration"},{"location":"release/migrate/2.5.10/","text":"Spoofax 2.5.10 Migration Guide \u00b6 A change in Statix need migration for users of the Stratego API. SDF3 \u00b6 In an upcoming version of Spoofax 2 it will be required to properly declare sorts in SDF3 syntax definitions. Sorts for which context-free rules are defined should be declared in a context-free sorts block: context-free sorts Stmt Expr Note : For backward compatibility, sorts declared in a plain sorts block are treated as context-free sorts. So this is equivalent and also fine: sorts Stmt Expr Sorts for which lexical rules are defined should be declared in a lexical sorts block: lexical sorts ID INT STRING Note : Lexical sorts are not supported in combination with sdf2table: c . Typesmart \u00b6 If your metaborg.yaml file still contains mention of Typesmart (e.g. debug: typesmart: false ), you can remove it. See the release notes for why Typesmart support was removed. Stratego \u00b6 Spoofax languages used to always generate target/metaborg/stratego-javastrat.jar which contains the compiled Java code from src/main/stratego . Conditional on your settings in the metaborg.yaml file, your Stratego code would be turned into target/metaborg/stratego.ctree or target/metaborg/stratego.jar depending on whether you chose compilation or interpretation. As of this release, there is no longer a separate stratego-javastrat.jar . Instead stratego.jar is always generated and always contains at least the compiled Java code from src/main/stratego . If you choose compilation for your Stratego code, the compiled Stratego code is added to the stratego.jar file as was already the case originally. What you need to do: Go to your editor/main.esv file and find the provider: ... lines (or search your other ESV files if it's not there). The line provider: target/metaborg/stratego-javastrat.jar should be replaced by provider: target/metaborg/stratego.jar . If you already have a provider: target/metaborg/stratego.jar , one is enough and you can remove the stratego-javastrat.jar provider directive entirely. Statix \u00b6 The AST property type is now a built-in property. Users of the Stratego API to get this property should change their API calls. Instead of stx-get-ast-property (| a , \"type\" ) one should now use: stx-get-ast-type (| a ) SPT \u00b6 In SPT, parse succeeds tests will now fail when the input parses ambiguously. If this is intended, use parse ambiguous instead.","title":"Spoofax 2.5.10 Migration Guide"},{"location":"release/migrate/2.5.10/#spoofax-2510-migration-guide","text":"A change in Statix need migration for users of the Stratego API.","title":"Spoofax 2.5.10 Migration Guide"},{"location":"release/migrate/2.5.10/#sdf3","text":"In an upcoming version of Spoofax 2 it will be required to properly declare sorts in SDF3 syntax definitions. Sorts for which context-free rules are defined should be declared in a context-free sorts block: context-free sorts Stmt Expr Note : For backward compatibility, sorts declared in a plain sorts block are treated as context-free sorts. So this is equivalent and also fine: sorts Stmt Expr Sorts for which lexical rules are defined should be declared in a lexical sorts block: lexical sorts ID INT STRING Note : Lexical sorts are not supported in combination with sdf2table: c .","title":"SDF3"},{"location":"release/migrate/2.5.10/#typesmart","text":"If your metaborg.yaml file still contains mention of Typesmart (e.g. debug: typesmart: false ), you can remove it. See the release notes for why Typesmart support was removed.","title":"Typesmart"},{"location":"release/migrate/2.5.10/#stratego","text":"Spoofax languages used to always generate target/metaborg/stratego-javastrat.jar which contains the compiled Java code from src/main/stratego . Conditional on your settings in the metaborg.yaml file, your Stratego code would be turned into target/metaborg/stratego.ctree or target/metaborg/stratego.jar depending on whether you chose compilation or interpretation. As of this release, there is no longer a separate stratego-javastrat.jar . Instead stratego.jar is always generated and always contains at least the compiled Java code from src/main/stratego . If you choose compilation for your Stratego code, the compiled Stratego code is added to the stratego.jar file as was already the case originally. What you need to do: Go to your editor/main.esv file and find the provider: ... lines (or search your other ESV files if it's not there). The line provider: target/metaborg/stratego-javastrat.jar should be replaced by provider: target/metaborg/stratego.jar . If you already have a provider: target/metaborg/stratego.jar , one is enough and you can remove the stratego-javastrat.jar provider directive entirely.","title":"Stratego"},{"location":"release/migrate/2.5.10/#statix","text":"The AST property type is now a built-in property. Users of the Stratego API to get this property should change their API calls. Instead of stx-get-ast-property (| a , \"type\" ) one should now use: stx-get-ast-type (| a )","title":"Statix"},{"location":"release/migrate/2.5.10/#spt","text":"In SPT, parse succeeds tests will now fail when the input parses ambiguously. If this is intended, use parse ambiguous instead.","title":"SPT"},{"location":"release/migrate/2.5.15/","text":"Spoofax 2.5.15 Migration Guide \u00b6 Statix Injection Explication \u00b6 There was an issue with Statix injection explication where the origin of the top-level term was lost and this caused SPT tests of Stratego strategies on analyzed ASTs to fail. Fix this by wrapping the bodies of the pre-analyze and post-analyze strategies in analyze.str with origin-track-forced , like this: imports libspoofax / term / origin rules pre-analyze = origin-track-forced ( explicate-injections-MyLang-Start ) post-analyze = origin-track-forced ( implicate-injections-MyLang-Start ) This is already fixed in new projects.","title":"Spoofax 2.5.15 Migration Guide"},{"location":"release/migrate/2.5.15/#spoofax-2515-migration-guide","text":"","title":"Spoofax 2.5.15 Migration Guide"},{"location":"release/migrate/2.5.15/#statix-injection-explication","text":"There was an issue with Statix injection explication where the origin of the top-level term was lost and this caused SPT tests of Stratego strategies on analyzed ASTs to fail. Fix this by wrapping the bodies of the pre-analyze and post-analyze strategies in analyze.str with origin-track-forced , like this: imports libspoofax / term / origin rules pre-analyze = origin-track-forced ( explicate-injections-MyLang-Start ) post-analyze = origin-track-forced ( implicate-injections-MyLang-Start ) This is already fixed in new projects.","title":"Statix Injection Explication"},{"location":"release/migrate/2.5.5/","text":"Spoofax 2.5.5 Migration Guide \u00b6 A few changes in Statix need migration for specs written for older versions. Statix \u00b6 A few features in Statix were not well-defined and removed until a better way to support them is found. Therefore, the following changes may require you to make small modifications to a specification: Functional constraints can only have a single output. Regular expression and label order must be direct parameters to queries. Namespace based query short-hands require a literal occurrence as argument, and an explicit resolution policy entry. Functional constraint outputs \u00b6 Functional constraints previously were allowed to have multiple output arguments, such as: f : T * T -> T * T ` This was sometimes indistinguishable from types with a single tuple output, such as: f : T * T -> ( T * T ) ` From now on, functional constraints can only have a single output. Constraints with multiple outputs need to be rewritten to return a tuple instead. Query parameters \u00b6 Queries would accept arbitrary predicates for the label regular expression and label order. For example, a query could be written as: query filter myWf and true min myOrd and false in s |-> _ with myWf ( ls ) : - pathMatch [ P *] ( ls ). myOrd ( l1 , l2 ) : - pathLt [$ < P ] ( l1 , l2 ). This is not possible anymore, instead the regular expression and label order must be direct parameters of the query. The query above should be directly written as: query filter P * and true min $ < P and false in s |-> _ The following syntax is still accepted, but deprecated: query filter pathMatch [ P *] and true min pathLt [$ < P ] and false in s |-> _ Namespace-based resolution shorthands \u00b6 The requirements for namespace-based resolution shorthands have become stricter. First, if they are used, ther emust be an explicit resolution policy for the namespace. For example, using a constraints such as: Var { x @ - } in s |-> _ type of Var { x @ - } in s |-> _ requires in the signature at least: signature name-resolution resolve Var A full resolution policy with regular expression and label order looks like this: signature name-resolution resolve Var filter P * min $ < P Second, the namespace-based constraints require an occurrence literal. The following does not work anymore: d == Var { x @ - }, d in s |-> _ The occurrence has to be repeated in the constraints, as: Var { x @ - } in s |-> _","title":"Spoofax 2.5.5 Migration Guide"},{"location":"release/migrate/2.5.5/#spoofax-255-migration-guide","text":"A few changes in Statix need migration for specs written for older versions.","title":"Spoofax 2.5.5 Migration Guide"},{"location":"release/migrate/2.5.5/#statix","text":"A few features in Statix were not well-defined and removed until a better way to support them is found. Therefore, the following changes may require you to make small modifications to a specification: Functional constraints can only have a single output. Regular expression and label order must be direct parameters to queries. Namespace based query short-hands require a literal occurrence as argument, and an explicit resolution policy entry.","title":"Statix"},{"location":"release/migrate/2.5.5/#functional-constraint-outputs","text":"Functional constraints previously were allowed to have multiple output arguments, such as: f : T * T -> T * T ` This was sometimes indistinguishable from types with a single tuple output, such as: f : T * T -> ( T * T ) ` From now on, functional constraints can only have a single output. Constraints with multiple outputs need to be rewritten to return a tuple instead.","title":"Functional constraint outputs"},{"location":"release/migrate/2.5.5/#query-parameters","text":"Queries would accept arbitrary predicates for the label regular expression and label order. For example, a query could be written as: query filter myWf and true min myOrd and false in s |-> _ with myWf ( ls ) : - pathMatch [ P *] ( ls ). myOrd ( l1 , l2 ) : - pathLt [$ < P ] ( l1 , l2 ). This is not possible anymore, instead the regular expression and label order must be direct parameters of the query. The query above should be directly written as: query filter P * and true min $ < P and false in s |-> _ The following syntax is still accepted, but deprecated: query filter pathMatch [ P *] and true min pathLt [$ < P ] and false in s |-> _","title":"Query parameters"},{"location":"release/migrate/2.5.5/#namespace-based-resolution-shorthands","text":"The requirements for namespace-based resolution shorthands have become stricter. First, if they are used, ther emust be an explicit resolution policy for the namespace. For example, using a constraints such as: Var { x @ - } in s |-> _ type of Var { x @ - } in s |-> _ requires in the signature at least: signature name-resolution resolve Var A full resolution policy with regular expression and label order looks like this: signature name-resolution resolve Var filter P * min $ < P Second, the namespace-based constraints require an occurrence literal. The following does not work anymore: d == Var { x @ - }, d in s |-> _ The occurrence has to be repeated in the constraints, as: Var { x @ - } in s |-> _","title":"Namespace-based resolution shorthands"},{"location":"release/migrate/march2016_project_structure/","text":"Directory structure migration (March 2016) \u00b6 To clean up the structure of a language specification project, we've made the following changes: ESV Main ESV file must be at editor/Main.esv . If it does not exist, no packed ESV file will be generated. Packed ESV file: target/metaborg/editor.esv.af SDF The RTG and signatures files are no longer generated for SDF3 projects, since SDF3 generates its own signatures. The generated box pp files are no longer generated, and box pp files are no longer converted into pp.af files. Definition: src-gen/syntax/[LanguageName].def Permissive definition: src-gen/syntax/[LanguageName]-permissive.def Parenthesizer: src-gen/pp/[LanguageName]-parenthesize.str Parse table: target/metaborg/sdf.tbl Stratego 'editor-common.generated' file: src-gen/stratego/metaborg.str Ctree: target/metaborg/stratego.ctree Generated Java files: src-gen/stratego-java JAR: target/metaborg/stratego.jar Java strategies: src/main/strategies Java strategies JAR: target/metaborg/stratego-javastrat.jar Build cache: target/stratego-cache DynSem Manual Java: src/main/ds Generated Java: src-gen/ds-java Other Pluto build cache: target/pluto To migrate your project, make the following changes: Change the file name of the main ESV file to Main.esv , and change its module to Main . In the main ESV file: Change the parse table: table : target/metaborg/sdf . tbl Change the Stratego providers For ctree projects: provider : target/metaborg/stratego . ctree For jar projects: provider : target/metaborg/stratego . jar For projects with Java strategies: esv provider : target/metaborg/stratego.jar provider : target/metaborg/stratego-javastrat.jar In all Stratego, NaBL, TS files Instead of importing lib/editor-common.generated , import stratego/metaborg . Instead of importing include/<langname>-parenthesize , import pp/<langname>-parenthesize . If you're using SDF3: Instead of importing the signatures from include/<langname> , import them from signatures/<langname>-sig . These signatures are spread over multiple files, import all the required files to fix errors, since the Stratego editor does not handle transitive imports. You can also use the wildcard import signatures/- to import all signature files, if your syntax definition is not spread over multiple directories. If you're using SDF2 or an external definition file: Instead of importing the signatures from include/<langname> , import them from signatures/<langname> . If your project has Java strategies: Create the src/main/strategies directory. Move Java strategies from editor/java into the src/main/strategies directory. Be sure to preserve the existing Java package structure. If your project has manual DynSem Java files: Create the src/main/ds directory. Move manual DynSem Java files from editor/java into the src/main/ds directory. Be sure to preserve the existing Java package structure. Perform a Maven update by right clicking the project and choosing Maven \u2023 Update Project... . Enable Force Update of Snapshots/Releases in the new window and press Ok . This updates the Java source directories of the project. If you are still using SDF2 instead of SDF3, add the following setting to the metaborg.yaml file: language : sdf : version : sdf2","title":"Directory structure migration (March 2016)"},{"location":"release/migrate/march2016_project_structure/#directory-structure-migration-march-2016","text":"To clean up the structure of a language specification project, we've made the following changes: ESV Main ESV file must be at editor/Main.esv . If it does not exist, no packed ESV file will be generated. Packed ESV file: target/metaborg/editor.esv.af SDF The RTG and signatures files are no longer generated for SDF3 projects, since SDF3 generates its own signatures. The generated box pp files are no longer generated, and box pp files are no longer converted into pp.af files. Definition: src-gen/syntax/[LanguageName].def Permissive definition: src-gen/syntax/[LanguageName]-permissive.def Parenthesizer: src-gen/pp/[LanguageName]-parenthesize.str Parse table: target/metaborg/sdf.tbl Stratego 'editor-common.generated' file: src-gen/stratego/metaborg.str Ctree: target/metaborg/stratego.ctree Generated Java files: src-gen/stratego-java JAR: target/metaborg/stratego.jar Java strategies: src/main/strategies Java strategies JAR: target/metaborg/stratego-javastrat.jar Build cache: target/stratego-cache DynSem Manual Java: src/main/ds Generated Java: src-gen/ds-java Other Pluto build cache: target/pluto To migrate your project, make the following changes: Change the file name of the main ESV file to Main.esv , and change its module to Main . In the main ESV file: Change the parse table: table : target/metaborg/sdf . tbl Change the Stratego providers For ctree projects: provider : target/metaborg/stratego . ctree For jar projects: provider : target/metaborg/stratego . jar For projects with Java strategies: esv provider : target/metaborg/stratego.jar provider : target/metaborg/stratego-javastrat.jar In all Stratego, NaBL, TS files Instead of importing lib/editor-common.generated , import stratego/metaborg . Instead of importing include/<langname>-parenthesize , import pp/<langname>-parenthesize . If you're using SDF3: Instead of importing the signatures from include/<langname> , import them from signatures/<langname>-sig . These signatures are spread over multiple files, import all the required files to fix errors, since the Stratego editor does not handle transitive imports. You can also use the wildcard import signatures/- to import all signature files, if your syntax definition is not spread over multiple directories. If you're using SDF2 or an external definition file: Instead of importing the signatures from include/<langname> , import them from signatures/<langname> . If your project has Java strategies: Create the src/main/strategies directory. Move Java strategies from editor/java into the src/main/strategies directory. Be sure to preserve the existing Java package structure. If your project has manual DynSem Java files: Create the src/main/ds directory. Move manual DynSem Java files from editor/java into the src/main/ds directory. Be sure to preserve the existing Java package structure. Perform a Maven update by right clicking the project and choosing Maven \u2023 Update Project... . Enable Force Update of Snapshots/Releases in the new window and press Ok . This updates the Java source directories of the project. If you are still using SDF2 instead of SDF3, add the following setting to the metaborg.yaml file: language : sdf : version : sdf2","title":"Directory structure migration (March 2016)"},{"location":"release/migrate/vnext/","text":"Spoofax vNext Migration Guide \u00b6 This is a stub for the migration guide of the upcoming version of Spoofax. Update the current documentation vnext.md file instead Until we have migrated to this new documentation, update the vnext.md in the current documentation repository .","title":"Spoofax vNext Migration Guide"},{"location":"release/migrate/vnext/#spoofax-vnext-migration-guide","text":"This is a stub for the migration guide of the upcoming version of Spoofax. Update the current documentation vnext.md file instead Until we have migrated to this new documentation, update the vnext.md in the current documentation repository .","title":"Spoofax vNext Migration Guide"},{"location":"release/note/1.0.0/","text":"Spoofax 1.0.0 (28-12-2011) \u00b6 We're pleased to announce the release of Spoofax 1.0.0. A number of significant new features have been added since the last stable release, a long list of bugs has been fixed, and various minor improvements were introduced. Highlights of the release include: Support for writing tests for language definitions Support for defining refactorings Major improvements to content completion: Spoofax/289 , Spoofax/357 Support for using rewrite rules to disambiguate syntax: Spoofax/328 In addition to these features, we're actively working on improving Spoofax with new features. In particular, we are now working on providing full support for debugging, on an interactive shell for Stratego and custom languages, and a new meta-language called SpoofaxLang to define languages in a more modular fashion. A full list of feature requests and issues addressed in the new version is provided at http://yellowgrass.org/tag/Spoofax/1.0 .","title":"Spoofax 1.0.0 (28-12-2011)"},{"location":"release/note/1.0.0/#spoofax-100-28-12-2011","text":"We're pleased to announce the release of Spoofax 1.0.0. A number of significant new features have been added since the last stable release, a long list of bugs has been fixed, and various minor improvements were introduced. Highlights of the release include: Support for writing tests for language definitions Support for defining refactorings Major improvements to content completion: Spoofax/289 , Spoofax/357 Support for using rewrite rules to disambiguate syntax: Spoofax/328 In addition to these features, we're actively working on improving Spoofax with new features. In particular, we are now working on providing full support for debugging, on an interactive shell for Stratego and custom languages, and a new meta-language called SpoofaxLang to define languages in a more modular fashion. A full list of feature requests and issues addressed in the new version is provided at http://yellowgrass.org/tag/Spoofax/1.0 .","title":"Spoofax 1.0.0 (28-12-2011)"},{"location":"release/note/1.0.2/","text":"Spoofax 1.0.2 (15-02-2012) \u00b6 Today we're releasing a minor maintenance release of Spoofax, version 1.0.2. This release fixes a memory leak that was introduced in the 1.0 release. There are no new features in this release, those will be included in the upcoming 1.1 release instead.","title":"Spoofax 1.0.2 (15-02-2012)"},{"location":"release/note/1.0.2/#spoofax-102-15-02-2012","text":"Today we're releasing a minor maintenance release of Spoofax, version 1.0.2. This release fixes a memory leak that was introduced in the 1.0 release. There are no new features in this release, those will be included in the upcoming 1.1 release instead.","title":"Spoofax 1.0.2 (15-02-2012)"},{"location":"release/note/1.1.0/","text":"Spoofax 1.1.0 (25-03-2013) \u00b6 We are happy to announce the release of Spoofax 1.1! This is the first major release since version 1.0.2 and includes major features and improvements. Spoofax 1.1 supports all current Eclipse versions, up to version 4.2.2. Changes \u00b6 NaBL \u00b6 One of the most important improvements in Spoofax 1.1 is the inclusion of NaBL, the Spoofax Name Binding Language. NaBL is used in all new projects created and significantly simplifies name binding analysis, as well as any editor services that depend on it (e.g., code completion, reference resolving) NaBL is documented at the following pages: Research paper Other \u00b6 Other highlights of the 1.1 release include: Improved build process: generated files can be deleted, building & loading are separated, projects can be cleaned ( http://yellowgrass.org/issue/Spoofax/577 , http://yellowgrass.org/issue/Spoofax/591 , http://yellowgrass.org/issue/Spoofax/578 ) Improved Stratego editor with multi-file reference resolving based on NaBL ( http://yellowgrass.org/issue/Spoofax/12 ) Extended support for customizing refactoring UI ( http://yellowgrass.org/issue/Spoofax/440 ) Automatic configuration of git/svn ignore settings ( http://yellowgrass.org/issue/Spoofax/573 ) Added support loading for Java-based plugin dependencies, in case your plugin depends on some other plugin such as EMF ( http://yellowgrass.org/issue/Spoofax/322 ) And there were a number of notable changes under the hood: Much improved completion engine ( http://yellowgrass.org/issue/Spoofax/360 ) We now show a nice warning if Eclipse is not configured with a proper stack and heap size ( http://yellowgrass.org/issue/Spoofax/86 ) Files are now queued for re-analysis even if their editor is not open ( http://yellowgrass.org/issue/Spoofax/224 ) A comprehensive list of changes can be viewed at http://yellowgrass.org/tag/Spoofax/1.1 .","title":"Spoofax 1.1.0 (25-03-2013)"},{"location":"release/note/1.1.0/#spoofax-110-25-03-2013","text":"We are happy to announce the release of Spoofax 1.1! This is the first major release since version 1.0.2 and includes major features and improvements. Spoofax 1.1 supports all current Eclipse versions, up to version 4.2.2.","title":"Spoofax 1.1.0 (25-03-2013)"},{"location":"release/note/1.1.0/#changes","text":"","title":"Changes"},{"location":"release/note/1.1.0/#nabl","text":"One of the most important improvements in Spoofax 1.1 is the inclusion of NaBL, the Spoofax Name Binding Language. NaBL is used in all new projects created and significantly simplifies name binding analysis, as well as any editor services that depend on it (e.g., code completion, reference resolving) NaBL is documented at the following pages: Research paper","title":"NaBL"},{"location":"release/note/1.1.0/#other","text":"Other highlights of the 1.1 release include: Improved build process: generated files can be deleted, building & loading are separated, projects can be cleaned ( http://yellowgrass.org/issue/Spoofax/577 , http://yellowgrass.org/issue/Spoofax/591 , http://yellowgrass.org/issue/Spoofax/578 ) Improved Stratego editor with multi-file reference resolving based on NaBL ( http://yellowgrass.org/issue/Spoofax/12 ) Extended support for customizing refactoring UI ( http://yellowgrass.org/issue/Spoofax/440 ) Automatic configuration of git/svn ignore settings ( http://yellowgrass.org/issue/Spoofax/573 ) Added support loading for Java-based plugin dependencies, in case your plugin depends on some other plugin such as EMF ( http://yellowgrass.org/issue/Spoofax/322 ) And there were a number of notable changes under the hood: Much improved completion engine ( http://yellowgrass.org/issue/Spoofax/360 ) We now show a nice warning if Eclipse is not configured with a proper stack and heap size ( http://yellowgrass.org/issue/Spoofax/86 ) Files are now queued for re-analysis even if their editor is not open ( http://yellowgrass.org/issue/Spoofax/224 ) A comprehensive list of changes can be viewed at http://yellowgrass.org/tag/Spoofax/1.1 .","title":"Other"},{"location":"release/note/1.2.0/","text":"Spoofax 1.2.0 (13-08-2014) \u00b6 We're happy to announce the release of Spoofax 1.2! This document describes the changes in Spoofax 1.2 since Spoofax 1.1. Changes \u00b6 Editor interface \u00b6 Several aspects of the editor interface for Spoofax languages have been improved. Menus \u00b6 Each language now has its own set of menus. These menus replace the Transform menu that was shared among all Spoofax-based languages. The new menus dynamically pop up in the Eclipse menus toolbar, based on the current active editor. There is now support for submenus, icons and menu separators. Contributors: Oskar van Rest Outline \u00b6 Editor outlines are now specified in Stratego instead of ESV, to allow for full customization. A library with reusable outline strategies is provided to allow you to quickly realize an outline. It is now possible to have icons in your outline. It is now possible to also base an outline on the current text selection instead of the complete text. Contributors: Oskar van Rest Properties view \u00b6 A property view has been added that shows properties for the selected AST node. By default, the new properties view integrates with NaBL and presents (NaBL) properties associated with the selected text in the editor. The properties view can be customized to show different kinds of properties, either to aid language development or to provide language users with additional information about their programs. Contributors: Daco Harkes, Oskar van Rest Meta-languages \u00b6 We have created a new version of SDF, improved NaBL and developed a DSL for describing type systems: TS. SDF3 \u00b6 SDF3 is the next iteration of SDF, replacing SDF2. The most important new features are: Productive productions can be used in context-free syntax sections, improving readability and consistency with main-stream grammar notation. Constructors are now formally integrated to the language. A productive production introduces the constructor directly after the left hand side non-terminal. By using constructors, productions can now be uniquely identified. Therefore, it is no longer necessary to repeat the entire production in the priorities section, but use its Sort.Constructor shorthand. Template productions are the major change from SDF2. They can be used to define what the concrete syntax of the language should look like. Syntactic completion and pretty-printer rules are automatically generated from templates. Documentation Contributors: Eduardo Amorim, Guido Wachsmuth NaBL \u00b6 NaBL has received many bug fixes and several new features. New features include: Filter clauses can be used to constrain valid resolution targets based on properties such as types. Disambiguation clauses can be used to disambiguate resolutions based on relations between properties, for example type hierarchies. It is now possible to specify non-transitive scopes, in which resolution ignores lexical parent scopes. Where clauses can include constraints and type calculations in TS syntax. We added new scope calculation constructs, which can be used to navigate the scope hierarchy. For example, it is possible to calculate the surrounding class of the current variable scope. For examples of name binding rules, see the Java front project Contributors: Guido Wachsmuth, Gabri\u00ebl Konat TS \u00b6 TS is a new meta-language for the specification of type analysis that is complementary to NaBL. Type rules in TS define constraints on abstract syntax tree nodes and may compute a type or other property. In addition, type rules can define subtyping predicates (relations on types) and type functions. For examples of type system rules, see the Java front project Contributors: Eelco Visser, Guido Wachsmuth, Gabri\u00ebl Konat Command-line integration \u00b6 Programs of your language can now be parsed, analyzed, and transformed from the command-line using Sunshine (in contrast with an Eclipse). Sunshine can also be used as a Java API to develop new language tooling. Contributors: Vlad Vergu Finer-grained incrementality \u00b6 Incrementality in the previous version of Spoofax was based on files. Any file that changed, and any dependent files would be reparsed and reanalysed completely. In the new version of Spoofax, there is more fine-grained dependency tracking which allows more fine-grained incrementality. If a file changes, that file is reparsed, but only affected computations are recomputed, and other files are never reparsed. Name and type computations which are described in NaBL and TS are incrementally executed. Incrementality is powered by a task engine, described in our paper . Contributors: Gabri\u00ebl Konat, Guido Wachsmuth Modelware \u00b6 Spoofax Modelware is a new Spoofax component that provides integration with the Eclipse Modeling Framework (EMF) and the Graphical Modeling Framework (GMF) to allow for real-time synchronized textual and graphical editors and/or views. It also allows you to use other EMF-based tooling in combination with Spoofax. Contributors: Oskar van Rest Documentation \u00b6 We have moved most of our documentation to the doc repository on GitHub. We're still in the process of moving over other documentation and writing more documentation. There are also two new tutorials available: Questionaire language tutorial : learn to create a questionaire language. This tutorial was given in a hands-on session at the Code Generation conference in 2014. Compiler Construction lab assignments : a more in-depth tutorial. These are the assignments from our Compiler Construction lab where we teach students to create MiniJava inside Spoofax. Contributors: Guido Wachsmuth and others Other \u00b6 To reduce maintenance effort, we have dropped support for Eclipse 3.7 (Indigo) and lower. We now support Eclipse 4.2 (Juno), 4.3 (Kepler), and 4.4 (Luna). We recommend you to download and install Eclipse 4.4 (Luna) for Java Developers . We have also dropped support for Java 5 and 6. Java 7 and 8 are supported, we recommend you to download and install Java 8 . Note that on OSX, Java 6 is installed by default which is not enough to run Spoofax, install Java 8 from the previous link. All source code has been moved to our organization on GitHub . Feel free to fork our code and send pull requests for patches or improvements! More changes, additions, and bug fixes can be found in the roadmap on our issue tracker: http://yellowgrass.org/tag/Spoofax/1.2 Contributors: Vlad Vergu, Gabri\u00ebl Konat","title":"Spoofax 1.2.0 (13-08-2014)"},{"location":"release/note/1.2.0/#spoofax-120-13-08-2014","text":"We're happy to announce the release of Spoofax 1.2! This document describes the changes in Spoofax 1.2 since Spoofax 1.1.","title":"Spoofax 1.2.0 (13-08-2014)"},{"location":"release/note/1.2.0/#changes","text":"","title":"Changes"},{"location":"release/note/1.2.0/#editor-interface","text":"Several aspects of the editor interface for Spoofax languages have been improved.","title":"Editor interface"},{"location":"release/note/1.2.0/#menus","text":"Each language now has its own set of menus. These menus replace the Transform menu that was shared among all Spoofax-based languages. The new menus dynamically pop up in the Eclipse menus toolbar, based on the current active editor. There is now support for submenus, icons and menu separators. Contributors: Oskar van Rest","title":"Menus"},{"location":"release/note/1.2.0/#outline","text":"Editor outlines are now specified in Stratego instead of ESV, to allow for full customization. A library with reusable outline strategies is provided to allow you to quickly realize an outline. It is now possible to have icons in your outline. It is now possible to also base an outline on the current text selection instead of the complete text. Contributors: Oskar van Rest","title":"Outline"},{"location":"release/note/1.2.0/#properties-view","text":"A property view has been added that shows properties for the selected AST node. By default, the new properties view integrates with NaBL and presents (NaBL) properties associated with the selected text in the editor. The properties view can be customized to show different kinds of properties, either to aid language development or to provide language users with additional information about their programs. Contributors: Daco Harkes, Oskar van Rest","title":"Properties view"},{"location":"release/note/1.2.0/#meta-languages","text":"We have created a new version of SDF, improved NaBL and developed a DSL for describing type systems: TS.","title":"Meta-languages"},{"location":"release/note/1.2.0/#sdf3","text":"SDF3 is the next iteration of SDF, replacing SDF2. The most important new features are: Productive productions can be used in context-free syntax sections, improving readability and consistency with main-stream grammar notation. Constructors are now formally integrated to the language. A productive production introduces the constructor directly after the left hand side non-terminal. By using constructors, productions can now be uniquely identified. Therefore, it is no longer necessary to repeat the entire production in the priorities section, but use its Sort.Constructor shorthand. Template productions are the major change from SDF2. They can be used to define what the concrete syntax of the language should look like. Syntactic completion and pretty-printer rules are automatically generated from templates. Documentation Contributors: Eduardo Amorim, Guido Wachsmuth","title":"SDF3"},{"location":"release/note/1.2.0/#nabl","text":"NaBL has received many bug fixes and several new features. New features include: Filter clauses can be used to constrain valid resolution targets based on properties such as types. Disambiguation clauses can be used to disambiguate resolutions based on relations between properties, for example type hierarchies. It is now possible to specify non-transitive scopes, in which resolution ignores lexical parent scopes. Where clauses can include constraints and type calculations in TS syntax. We added new scope calculation constructs, which can be used to navigate the scope hierarchy. For example, it is possible to calculate the surrounding class of the current variable scope. For examples of name binding rules, see the Java front project Contributors: Guido Wachsmuth, Gabri\u00ebl Konat","title":"NaBL"},{"location":"release/note/1.2.0/#ts","text":"TS is a new meta-language for the specification of type analysis that is complementary to NaBL. Type rules in TS define constraints on abstract syntax tree nodes and may compute a type or other property. In addition, type rules can define subtyping predicates (relations on types) and type functions. For examples of type system rules, see the Java front project Contributors: Eelco Visser, Guido Wachsmuth, Gabri\u00ebl Konat","title":"TS"},{"location":"release/note/1.2.0/#command-line-integration","text":"Programs of your language can now be parsed, analyzed, and transformed from the command-line using Sunshine (in contrast with an Eclipse). Sunshine can also be used as a Java API to develop new language tooling. Contributors: Vlad Vergu","title":"Command-line integration"},{"location":"release/note/1.2.0/#finer-grained-incrementality","text":"Incrementality in the previous version of Spoofax was based on files. Any file that changed, and any dependent files would be reparsed and reanalysed completely. In the new version of Spoofax, there is more fine-grained dependency tracking which allows more fine-grained incrementality. If a file changes, that file is reparsed, but only affected computations are recomputed, and other files are never reparsed. Name and type computations which are described in NaBL and TS are incrementally executed. Incrementality is powered by a task engine, described in our paper . Contributors: Gabri\u00ebl Konat, Guido Wachsmuth","title":"Finer-grained incrementality"},{"location":"release/note/1.2.0/#modelware","text":"Spoofax Modelware is a new Spoofax component that provides integration with the Eclipse Modeling Framework (EMF) and the Graphical Modeling Framework (GMF) to allow for real-time synchronized textual and graphical editors and/or views. It also allows you to use other EMF-based tooling in combination with Spoofax. Contributors: Oskar van Rest","title":"Modelware"},{"location":"release/note/1.2.0/#documentation","text":"We have moved most of our documentation to the doc repository on GitHub. We're still in the process of moving over other documentation and writing more documentation. There are also two new tutorials available: Questionaire language tutorial : learn to create a questionaire language. This tutorial was given in a hands-on session at the Code Generation conference in 2014. Compiler Construction lab assignments : a more in-depth tutorial. These are the assignments from our Compiler Construction lab where we teach students to create MiniJava inside Spoofax. Contributors: Guido Wachsmuth and others","title":"Documentation"},{"location":"release/note/1.2.0/#other","text":"To reduce maintenance effort, we have dropped support for Eclipse 3.7 (Indigo) and lower. We now support Eclipse 4.2 (Juno), 4.3 (Kepler), and 4.4 (Luna). We recommend you to download and install Eclipse 4.4 (Luna) for Java Developers . We have also dropped support for Java 5 and 6. Java 7 and 8 are supported, we recommend you to download and install Java 8 . Note that on OSX, Java 6 is installed by default which is not enough to run Spoofax, install Java 8 from the previous link. All source code has been moved to our organization on GitHub . Feel free to fork our code and send pull requests for patches or improvements! More changes, additions, and bug fixes can be found in the roadmap on our issue tracker: http://yellowgrass.org/tag/Spoofax/1.2 Contributors: Vlad Vergu, Gabri\u00ebl Konat","title":"Other"},{"location":"release/note/1.3.0/","text":"Spoofax 1.3.0 (12-11-2014) \u00b6 We're happy to announce the release of Spoofax 1.3, which improves SDF3, the build system for languages, and the build system for Spoofax itself. Porting \u00b6 Language projects \u00b6 The build for language projects has changed since the last Spoofax, so your language projects need to be migrated. To automatically migrate your project, follow these steps: Right click the project in Eclipse Choose Spoofax -> Upgrade Spoofax project Press Finish This will automatically upgrade your project to the latest format, after which you can build your project normally. The build might fail with: sdf2parenthesize: BUILD FAILED /Users/gohla/spoofax/workspace/runtime-Spoofax/Entity/build.generated.xml:266: The following error occurred while executing this line: Target \"sdf2parenthesize.helper\" does not exist in the project \"Entity\". If this is the case, build again, because some files are only upgraded after building once. Compiled Java files are now stored in the target/classes directory instead of the bin directory, so the bin directory can be deleted. SDF3 grammars \u00b6 SDF2 and old SDF3 grammars can be migrated. Some manual changes might still be necessary since a sort cannot have more than one constructor with the same name and arity, and priority rules may contain the entire production instead of priority shorthands. Changes \u00b6 SDF3 \u00b6 Several improvements have been made to increase the consistency and robustness of SDF3. Regular productive productions can be mixed with template productions in the context-free syntax section. Each production defines a non-unique sort and a unique constructor of that sort. References point to these definitions and errors are given for undefined elements in the grammar. Signatures are generated on-save following the sorts used in the right-hand side of a production and the sort and constructor that are being defined. All code generated from SDF3 grammars is organized in the src-gen directory of the project, which keeps Spoofax projects more clean and structured. Contributors: Eduardo Amorim Building language projects \u00b6 Projects are built using Ant Macros tailored to make the build system more incremental. Language projects can now also be built on the command-line using Maven. Contributors: Eduardo Amorim, Gabri\u00ebl Konat Building and developing Spoofax \u00b6 The build system for Spoofax itself has been refactored from a Nix build into a full Maven build. This enables local builds on any system that Maven supports, which is basically any system that supports Java. See the instructions on building Spoofax for more information follow the instructions on using MetaBorg Maven artifacts for more information. There is now also documentation on Setting up Eclipse for Spoofax development , which explains how to set up an environment for developing projects which are included in Spoofax. The nightly version of Spoofax is now built on our Jenkins server: http://buildfarm.metaborg.org/ . It publishes artifacts to our artifact server: http://artifacts.metaborg.org/ . To use these artifacts, read the instructions on the instructions on using MetaBorg Maven artifacts for more information. Contributors: Gabri\u00ebl Konat, Danny Groenewegen, Elmer van Chastelet Other \u00b6 More changes, additions, and bug fixes can be found in the roadmap on our issue tracker: http://yellowgrass.org/tag/Spoofax/1.3","title":"Spoofax 1.3.0 (12-11-2014)"},{"location":"release/note/1.3.0/#spoofax-130-12-11-2014","text":"We're happy to announce the release of Spoofax 1.3, which improves SDF3, the build system for languages, and the build system for Spoofax itself.","title":"Spoofax 1.3.0 (12-11-2014)"},{"location":"release/note/1.3.0/#porting","text":"","title":"Porting"},{"location":"release/note/1.3.0/#language-projects","text":"The build for language projects has changed since the last Spoofax, so your language projects need to be migrated. To automatically migrate your project, follow these steps: Right click the project in Eclipse Choose Spoofax -> Upgrade Spoofax project Press Finish This will automatically upgrade your project to the latest format, after which you can build your project normally. The build might fail with: sdf2parenthesize: BUILD FAILED /Users/gohla/spoofax/workspace/runtime-Spoofax/Entity/build.generated.xml:266: The following error occurred while executing this line: Target \"sdf2parenthesize.helper\" does not exist in the project \"Entity\". If this is the case, build again, because some files are only upgraded after building once. Compiled Java files are now stored in the target/classes directory instead of the bin directory, so the bin directory can be deleted.","title":"Language projects"},{"location":"release/note/1.3.0/#sdf3-grammars","text":"SDF2 and old SDF3 grammars can be migrated. Some manual changes might still be necessary since a sort cannot have more than one constructor with the same name and arity, and priority rules may contain the entire production instead of priority shorthands.","title":"SDF3 grammars"},{"location":"release/note/1.3.0/#changes","text":"","title":"Changes"},{"location":"release/note/1.3.0/#sdf3","text":"Several improvements have been made to increase the consistency and robustness of SDF3. Regular productive productions can be mixed with template productions in the context-free syntax section. Each production defines a non-unique sort and a unique constructor of that sort. References point to these definitions and errors are given for undefined elements in the grammar. Signatures are generated on-save following the sorts used in the right-hand side of a production and the sort and constructor that are being defined. All code generated from SDF3 grammars is organized in the src-gen directory of the project, which keeps Spoofax projects more clean and structured. Contributors: Eduardo Amorim","title":"SDF3"},{"location":"release/note/1.3.0/#building-language-projects","text":"Projects are built using Ant Macros tailored to make the build system more incremental. Language projects can now also be built on the command-line using Maven. Contributors: Eduardo Amorim, Gabri\u00ebl Konat","title":"Building language projects"},{"location":"release/note/1.3.0/#building-and-developing-spoofax","text":"The build system for Spoofax itself has been refactored from a Nix build into a full Maven build. This enables local builds on any system that Maven supports, which is basically any system that supports Java. See the instructions on building Spoofax for more information follow the instructions on using MetaBorg Maven artifacts for more information. There is now also documentation on Setting up Eclipse for Spoofax development , which explains how to set up an environment for developing projects which are included in Spoofax. The nightly version of Spoofax is now built on our Jenkins server: http://buildfarm.metaborg.org/ . It publishes artifacts to our artifact server: http://artifacts.metaborg.org/ . To use these artifacts, read the instructions on the instructions on using MetaBorg Maven artifacts for more information. Contributors: Gabri\u00ebl Konat, Danny Groenewegen, Elmer van Chastelet","title":"Building and developing Spoofax"},{"location":"release/note/1.3.0/#other","text":"More changes, additions, and bug fixes can be found in the roadmap on our issue tracker: http://yellowgrass.org/tag/Spoofax/1.3","title":"Other"},{"location":"release/note/1.3.1/","text":"Spoofax 1.3.1 (09-12-2014) \u00b6 We're happy to announce the release of Spoofax 1.3.1, a maintenance release which fixes several issues in SDF3, and compatibility with Eclipse 4.3 (Kepler). Changes \u00b6 Fix: Allowing sequences in SDF3 lexical syntax. Fix: Allowing specific arguments of productions in SDF3 priorities. Fix: SDF3 outliner not working. Fix: Unable to install in Eclipse 4.3. Contributors: Eduardo Amorim","title":"Spoofax 1.3.1 (09-12-2014)"},{"location":"release/note/1.3.1/#spoofax-131-09-12-2014","text":"We're happy to announce the release of Spoofax 1.3.1, a maintenance release which fixes several issues in SDF3, and compatibility with Eclipse 4.3 (Kepler).","title":"Spoofax 1.3.1 (09-12-2014)"},{"location":"release/note/1.3.1/#changes","text":"Fix: Allowing sequences in SDF3 lexical syntax. Fix: Allowing specific arguments of productions in SDF3 priorities. Fix: SDF3 outliner not working. Fix: Unable to install in Eclipse 4.3. Contributors: Eduardo Amorim","title":"Changes"},{"location":"release/note/1.4.0/","text":"Spoofax 1.4.0 (06-03-2015) \u00b6 We're happy to announce the release of Spoofax 1.4.0, a minor release with SDF3 fixes and improvements to language plugins. Changes \u00b6 SDF3 \u00b6 Fix: Supporting Windows line endings in SDF3. Fix: Providing warnings for literals that could be placeholders. Contributors: Eduardo Amorim Language plugins \u00b6 Reduced download size of deployed plugins \u00b6 Previously, when creating an Eclipse update site for your language (see tutorial ), the result was a ~90MB download that included meta-tools such as SDF3 and NaBL. We brought the download size down to ~60MB by removing dependencies to some of the meta-tools, since end-users of deployed languages don't need them. In the future, we plan to bring the size further down by removing more dependencies. Contributors: Oskar van Rest Setting Java VM options for your language \u00b6 For Spoofax and Spoofax-based languages to run smoothly, it is recommended to set Java's -server flag and to increase the stack size and memory allocation pool: -Xss<size> specifies the thread stack size -Xmx<size> specifies the maximum size, in bytes, of the memory allocation pool. -server selects server application runtime optimizations. The server VM will take longer to start and \u201cwarm up\u201d but will be more aggressively optimized. The -server option only affects 32-bit VMs and has influence on 64-bit VMs because these always use server optimizations. These options can be configured in eclipse.ini as described on the download page. The recommended settings for Spoofax are -server -Xss8m -Xmx1024m and a warning will pop-up if Eclipse is started with settings that are too low. Previously, the same settings were assumed for deployed plugins and were enforced by a similar pup-up warning. With Spoofax 1.4.0, language developers can choose their own Java VM settings, which are then recommended to end-users of their language. This can be configured in editor/yourlang.main.esv . The syntax is as follows: jvm opts: [-server | -X[ss|mx]<size>[g|G|m|M|k|K]]+ For example: jvm opts: -server -Xss8m -Xmx1024m . If multiple Spoofax-based languages are installed, the configuration warning will tell how eclipse.ini needs to be updated such that the requirements of all languages are satisfied. Contributors: Oskar van Rest","title":"Spoofax 1.4.0 (06-03-2015)"},{"location":"release/note/1.4.0/#spoofax-140-06-03-2015","text":"We're happy to announce the release of Spoofax 1.4.0, a minor release with SDF3 fixes and improvements to language plugins.","title":"Spoofax 1.4.0 (06-03-2015)"},{"location":"release/note/1.4.0/#changes","text":"","title":"Changes"},{"location":"release/note/1.4.0/#sdf3","text":"Fix: Supporting Windows line endings in SDF3. Fix: Providing warnings for literals that could be placeholders. Contributors: Eduardo Amorim","title":"SDF3"},{"location":"release/note/1.4.0/#language-plugins","text":"","title":"Language plugins"},{"location":"release/note/1.4.0/#reduced-download-size-of-deployed-plugins","text":"Previously, when creating an Eclipse update site for your language (see tutorial ), the result was a ~90MB download that included meta-tools such as SDF3 and NaBL. We brought the download size down to ~60MB by removing dependencies to some of the meta-tools, since end-users of deployed languages don't need them. In the future, we plan to bring the size further down by removing more dependencies. Contributors: Oskar van Rest","title":"Reduced download size of deployed plugins"},{"location":"release/note/1.4.0/#setting-java-vm-options-for-your-language","text":"For Spoofax and Spoofax-based languages to run smoothly, it is recommended to set Java's -server flag and to increase the stack size and memory allocation pool: -Xss<size> specifies the thread stack size -Xmx<size> specifies the maximum size, in bytes, of the memory allocation pool. -server selects server application runtime optimizations. The server VM will take longer to start and \u201cwarm up\u201d but will be more aggressively optimized. The -server option only affects 32-bit VMs and has influence on 64-bit VMs because these always use server optimizations. These options can be configured in eclipse.ini as described on the download page. The recommended settings for Spoofax are -server -Xss8m -Xmx1024m and a warning will pop-up if Eclipse is started with settings that are too low. Previously, the same settings were assumed for deployed plugins and were enforced by a similar pup-up warning. With Spoofax 1.4.0, language developers can choose their own Java VM settings, which are then recommended to end-users of their language. This can be configured in editor/yourlang.main.esv . The syntax is as follows: jvm opts: [-server | -X[ss|mx]<size>[g|G|m|M|k|K]]+ For example: jvm opts: -server -Xss8m -Xmx1024m . If multiple Spoofax-based languages are installed, the configuration warning will tell how eclipse.ini needs to be updated such that the requirements of all languages are satisfied. Contributors: Oskar van Rest","title":"Setting Java VM options for your language"},{"location":"release/note/1.5.0/","text":"Spoofax 1.5.0 (18-12-2015) \u00b6 We're happy to announce the release of Spoofax 1.5.0 with new SDF3 features and fixes, and support for Eclipse Mars. Changes \u00b6 SDF3 \u00b6 Feature: support for case insensitive keywords . All keywords in a template production are case insensitive if the production has the attribute case-insensitive . Feature: pretty-print ambiguous programs (by taking the first alternative). Feature: give an error if the filename does not match the module name. Fix: ESV generation with empty imports . Fix: disallow empty placeholders <> in template productions. Contributor: Eduardo Amorim Eclipse \u00b6 Feature: support for Eclipse Mars. Feature: generation of premade Eclipse installations with Spoofax installed. Contributor: Gabriel Konat Command-line tools \u00b6 Fix: Sunshine now pretty-prints ATerms before presenting them, mimicking the behavior in Eclipse. Contributor: Gabriel Konat","title":"Spoofax 1.5.0 (18-12-2015)"},{"location":"release/note/1.5.0/#spoofax-150-18-12-2015","text":"We're happy to announce the release of Spoofax 1.5.0 with new SDF3 features and fixes, and support for Eclipse Mars.","title":"Spoofax 1.5.0 (18-12-2015)"},{"location":"release/note/1.5.0/#changes","text":"","title":"Changes"},{"location":"release/note/1.5.0/#sdf3","text":"Feature: support for case insensitive keywords . All keywords in a template production are case insensitive if the production has the attribute case-insensitive . Feature: pretty-print ambiguous programs (by taking the first alternative). Feature: give an error if the filename does not match the module name. Fix: ESV generation with empty imports . Fix: disallow empty placeholders <> in template productions. Contributor: Eduardo Amorim","title":"SDF3"},{"location":"release/note/1.5.0/#eclipse","text":"Feature: support for Eclipse Mars. Feature: generation of premade Eclipse installations with Spoofax installed. Contributor: Gabriel Konat","title":"Eclipse"},{"location":"release/note/1.5.0/#command-line-tools","text":"Fix: Sunshine now pretty-prints ATerms before presenting them, mimicking the behavior in Eclipse. Contributor: Gabriel Konat","title":"Command-line tools"},{"location":"release/note/2.0.0-beta1/","text":"Spoofax 2.0.0-beta1 (07-04-2016) \u00b6 This is the first beta release of Spoofax 2.0. These notes provide the download links for the various artifacts. See the 2.0.0 release notes for more information about Spoofax 2.0. See the 2.0.0 migration guide for migrating a Spoofax 1.5 project to a Spoofax 2.0 project.","title":"Spoofax 2.0.0-beta1 (07-04-2016)"},{"location":"release/note/2.0.0-beta1/#spoofax-200-beta1-07-04-2016","text":"This is the first beta release of Spoofax 2.0. These notes provide the download links for the various artifacts. See the 2.0.0 release notes for more information about Spoofax 2.0. See the 2.0.0 migration guide for migrating a Spoofax 1.5 project to a Spoofax 2.0 project.","title":"Spoofax 2.0.0-beta1 (07-04-2016)"},{"location":"release/note/2.0.0/","text":"Spoofax 2.0.0 (08-07-2016) \u00b6 Spoofax 2.0 is a complete rewrite of Spoofax which improves the architecture by separating Spoofax into the Spoofax Core API and implementations on top of that API, massively improves the language development workflow, and properly supports language extension. See the corresponding migration guide for migrating from Spoofax 1.5 to Spoofax 2.0. Known Issues \u00b6 Stratego imports do not work. To work around this issue, add an explicit compile dependency to Stratego: dependencies: compile: - org.metaborg:org.metaborg.meta.lang.stratego:${metaborgVersion} Changes \u00b6 Architecture \u00b6 The biggest change in Spoofax 2.0 is the architecture. Previously, Spoofax was built on top of the Eclipse and IMP platform, meaning Spoofax was not usable outside of the Eclipse platform. In Spoofax 2.0, all platform-agnostic functionality such as language management, parsing, analysis, transformation, and editor services, are implemented in Spoofax Core, which is a portable Java library with an API. This means that the Spoofax language workbench, and any language implementations made with Spoofax, can now be used by any application, platform, or framework in the Java ecosystem. Integrations \u00b6 We have integrated Spoofax Core with Eclipse, IntelliJ, Maven, and the command-line. We support the Eclipse platform through a new plugin that integrates Spoofax Core as an Eclipse plugin. The new Eclipse plugin supports language development in Eclipse, and supports exporting languages made with Spoofax as an Eclipse plugin with full-fledged editor support. We have also performed a more faithful Eclipse integration than Spoofax 1.5 did. For example, we now use natures to enable Spoofax for a project, use the incremental builder framework to allow suspending automatic builds, and use Eclipse's menu system for builders instead of non-standard buttons. See the migration guide for a full list of changes to the Eclipse plugin. IntelliJ is an experimentally supported platform through the Eclipse IntelliJ plugin. Languages can be developed in IntelliJ, and exported as IntelliJ plugins with full-fledged editor support. The Spoofax Maven plugin supports command-line builds and continuous integration of language implementations in Maven. Language implementations can be exported as Maven artifacts which can be depended on and used to build programs of that language. Command-line use of language implementations is supported through Sunshine's integration with Spoofax Core. Sunshine's command-line interface has been simplified to improve ease of use, and now also supports a server mode to reduce the high cost of starting a new JVM and loading a language. Furthermore, anyone can make new integrations using the Core API. Language Development Workflow \u00b6 There are several improvements to the language development workflow in Spoofax 2.0. Almost all generated files are now generated to the src-gen directory of a language project. All required generated files are now (re)generated when building, so it is no longer necessary to commit generated files to source control. This results in much cleaner projects. Furthermore, the language build is now incremental, which speeds up the build in many cases. The bootstrapping process of meta-languages has been significantly improved by versioning languages. It is now possible to load multiple versions of the same language implementation into Spoofax. Meta-languages are bootstrapped by building them against baseline versions of the meta-languages. When a meta-language under development breaks, it is possible to revert back to a previous version to get things working again. Extension \u00b6 Spoofax 2.0 supports language extension on the source level, without the need to copy-paste files around. A dependency can be made from a language specification to another language specification, which then allows importing modules of the specification into the other. For example, language extensions can depend on a base language and extend its concepts. Those extensions can be composed together with the base language specification into a new language specification that contains the base and extensions. There is also limited support for dynamic extension, i.e. extension at the runtime level instead of the source level. A language implementation can be extended with new builders at runtime. This allows adding builders to existing language implementations, and supports separating the front-end and back-end of a language into multiple projects. License \u00b6 The license has been changed from LPGLv2.1 to the Apache 2.0 license, to improve adoption of Spoofax. Any contributions made to Spoofax must be licensed under the Apache 2.0 license as well. Missing Features \u00b6 A few features didn't make it to Spoofax 2.0, with the biggest one being semantic completions. Semantic completions were already very dodgy in Spoofax 1.5, only working in some specific cases. This is why we did not port the completion algorithm from Spoofax 1.5 to 2.0, and are instead working on a new completion algorithm that will be included in a future version. Refactorings were already broken in Spoofax 1.5, so we did not port refactorings to Spoofax 2.0. In the future we will revisit refactorings for Spoofax 2.0 with our new meta-languages. The Spoofax modelware component was not ported to Spoofax 2.0 since we do not have the knowledge to port this component. Folding, realtime builders, and the eclipse.ini check are minor features that are not implemented in 2.0, but may be implemented in the future. A missing integration in Spoofax 2.0 is a Spoofax Gradle plugin, we are working on that integration for inclusion in a future version.","title":"Spoofax 2.0.0 (08-07-2016)"},{"location":"release/note/2.0.0/#spoofax-200-08-07-2016","text":"Spoofax 2.0 is a complete rewrite of Spoofax which improves the architecture by separating Spoofax into the Spoofax Core API and implementations on top of that API, massively improves the language development workflow, and properly supports language extension. See the corresponding migration guide for migrating from Spoofax 1.5 to Spoofax 2.0.","title":"Spoofax 2.0.0 (08-07-2016)"},{"location":"release/note/2.0.0/#known-issues","text":"Stratego imports do not work. To work around this issue, add an explicit compile dependency to Stratego: dependencies: compile: - org.metaborg:org.metaborg.meta.lang.stratego:${metaborgVersion}","title":"Known Issues"},{"location":"release/note/2.0.0/#changes","text":"","title":"Changes"},{"location":"release/note/2.0.0/#architecture","text":"The biggest change in Spoofax 2.0 is the architecture. Previously, Spoofax was built on top of the Eclipse and IMP platform, meaning Spoofax was not usable outside of the Eclipse platform. In Spoofax 2.0, all platform-agnostic functionality such as language management, parsing, analysis, transformation, and editor services, are implemented in Spoofax Core, which is a portable Java library with an API. This means that the Spoofax language workbench, and any language implementations made with Spoofax, can now be used by any application, platform, or framework in the Java ecosystem.","title":"Architecture"},{"location":"release/note/2.0.0/#integrations","text":"We have integrated Spoofax Core with Eclipse, IntelliJ, Maven, and the command-line. We support the Eclipse platform through a new plugin that integrates Spoofax Core as an Eclipse plugin. The new Eclipse plugin supports language development in Eclipse, and supports exporting languages made with Spoofax as an Eclipse plugin with full-fledged editor support. We have also performed a more faithful Eclipse integration than Spoofax 1.5 did. For example, we now use natures to enable Spoofax for a project, use the incremental builder framework to allow suspending automatic builds, and use Eclipse's menu system for builders instead of non-standard buttons. See the migration guide for a full list of changes to the Eclipse plugin. IntelliJ is an experimentally supported platform through the Eclipse IntelliJ plugin. Languages can be developed in IntelliJ, and exported as IntelliJ plugins with full-fledged editor support. The Spoofax Maven plugin supports command-line builds and continuous integration of language implementations in Maven. Language implementations can be exported as Maven artifacts which can be depended on and used to build programs of that language. Command-line use of language implementations is supported through Sunshine's integration with Spoofax Core. Sunshine's command-line interface has been simplified to improve ease of use, and now also supports a server mode to reduce the high cost of starting a new JVM and loading a language. Furthermore, anyone can make new integrations using the Core API.","title":"Integrations"},{"location":"release/note/2.0.0/#language-development-workflow","text":"There are several improvements to the language development workflow in Spoofax 2.0. Almost all generated files are now generated to the src-gen directory of a language project. All required generated files are now (re)generated when building, so it is no longer necessary to commit generated files to source control. This results in much cleaner projects. Furthermore, the language build is now incremental, which speeds up the build in many cases. The bootstrapping process of meta-languages has been significantly improved by versioning languages. It is now possible to load multiple versions of the same language implementation into Spoofax. Meta-languages are bootstrapped by building them against baseline versions of the meta-languages. When a meta-language under development breaks, it is possible to revert back to a previous version to get things working again.","title":"Language Development Workflow"},{"location":"release/note/2.0.0/#extension","text":"Spoofax 2.0 supports language extension on the source level, without the need to copy-paste files around. A dependency can be made from a language specification to another language specification, which then allows importing modules of the specification into the other. For example, language extensions can depend on a base language and extend its concepts. Those extensions can be composed together with the base language specification into a new language specification that contains the base and extensions. There is also limited support for dynamic extension, i.e. extension at the runtime level instead of the source level. A language implementation can be extended with new builders at runtime. This allows adding builders to existing language implementations, and supports separating the front-end and back-end of a language into multiple projects.","title":"Extension"},{"location":"release/note/2.0.0/#license","text":"The license has been changed from LPGLv2.1 to the Apache 2.0 license, to improve adoption of Spoofax. Any contributions made to Spoofax must be licensed under the Apache 2.0 license as well.","title":"License"},{"location":"release/note/2.0.0/#missing-features","text":"A few features didn't make it to Spoofax 2.0, with the biggest one being semantic completions. Semantic completions were already very dodgy in Spoofax 1.5, only working in some specific cases. This is why we did not port the completion algorithm from Spoofax 1.5 to 2.0, and are instead working on a new completion algorithm that will be included in a future version. Refactorings were already broken in Spoofax 1.5, so we did not port refactorings to Spoofax 2.0. In the future we will revisit refactorings for Spoofax 2.0 with our new meta-languages. The Spoofax modelware component was not ported to Spoofax 2.0 since we do not have the knowledge to port this component. Folding, realtime builders, and the eclipse.ini check are minor features that are not implemented in 2.0, but may be implemented in the future. A missing integration in Spoofax 2.0 is a Spoofax Gradle plugin, we are working on that integration for inclusion in a future version.","title":"Missing Features"},{"location":"release/note/2.1.0/","text":"Spoofax 2.1.0 (10-01-2017) \u00b6 Spoofax 2.1 improves on Spoofax 2.0 with several bug fixes, an implementation of syntactic completions based on SDF3, and addition of the DynSem dynamic semantics specification meta-language. See the corresponding migration guide <2.1.0-migration-guide> for migrating from Spoofax 2.0 to Spoofax 2.1. Changes \u00b6 Syntactic Completions \u00b6 Spoofax now has support for syntactic completions. Syntactic completions are generated automatically from an SDF3 specification. New projects using SDF3 automatically support syntactic completions. Existing projects need to make a few changes, documented in the migration guide <new-completion-framework-migration-guide> {.interpreted-text role=\"ref\"}. DynSem \u00b6 DynSem is a DSL for concise and modular specification of dynamic semantics of programming languages. Fully functional interpreters are automatically derived from dynamic semantics specifications. For more information about DynSem, see the following sources: Paper Documentation <dynsem-index> Getting started tutorial <dynsem-getting-started> {.interpreted-text role=\"ref\"} Example language While DynSem was included in Spoofax 2.0.0, we did not advertise this as it was still under heavy development. Since 2.0.0, the following major improvements were made: Redesigned semantic component and explication subsystem Support for tuples Updated tutorial for SIMPL <dynsem-getting-started> {.interpreted-text role=\"ref\"} Added support for unit testing and continuous integration of generated interpreters <dynsem-ci> {.interpreted-text role=\"ref\"}","title":"Spoofax 2.1.0 (10-01-2017)"},{"location":"release/note/2.1.0/#spoofax-210-10-01-2017","text":"Spoofax 2.1 improves on Spoofax 2.0 with several bug fixes, an implementation of syntactic completions based on SDF3, and addition of the DynSem dynamic semantics specification meta-language. See the corresponding migration guide <2.1.0-migration-guide> for migrating from Spoofax 2.0 to Spoofax 2.1.","title":"Spoofax 2.1.0 (10-01-2017)"},{"location":"release/note/2.1.0/#changes","text":"","title":"Changes"},{"location":"release/note/2.1.0/#syntactic-completions","text":"Spoofax now has support for syntactic completions. Syntactic completions are generated automatically from an SDF3 specification. New projects using SDF3 automatically support syntactic completions. Existing projects need to make a few changes, documented in the migration guide <new-completion-framework-migration-guide> {.interpreted-text role=\"ref\"}.","title":"Syntactic Completions"},{"location":"release/note/2.1.0/#dynsem","text":"DynSem is a DSL for concise and modular specification of dynamic semantics of programming languages. Fully functional interpreters are automatically derived from dynamic semantics specifications. For more information about DynSem, see the following sources: Paper Documentation <dynsem-index> Getting started tutorial <dynsem-getting-started> {.interpreted-text role=\"ref\"} Example language While DynSem was included in Spoofax 2.0.0, we did not advertise this as it was still under heavy development. Since 2.0.0, the following major improvements were made: Redesigned semantic component and explication subsystem Support for tuples Updated tutorial for SIMPL <dynsem-getting-started> {.interpreted-text role=\"ref\"} Added support for unit testing and continuous integration of generated interpreters <dynsem-ci> {.interpreted-text role=\"ref\"}","title":"DynSem"},{"location":"release/note/2.2.0/","text":"Spoofax 2.2.0 (18-04-2017) \u00b6 Spoofax 2.2 improves on Spoofax 2.1 with a new NaBL2 constraint solver which is optimised for performance, improved progress reporting and cancellation in Eclipse, an experimental replacement for sdf2table which fixes several long-standing bugs, improvements to the core API, and several bug fixes. See the corresponding migration guide <2.2.0-migration-guide> for migrating from Spoofax 2.1 to Spoofax 2.2. Changes \u00b6 Overall \u00b6 The deprecated libraries and files from Spoofax 2.1.0 have been removed. If you have not done so yet, follow the Spoofax 2.1.0 migration guide <2.1.0-migration-guide> {.interpreted-text role=\"ref\"} to migrate your project to the new Spoofax library. Core API \u00b6 Improve: Spoofax/190 - Extend API for language discovery . This deprecates several methods in the language discovery API, see the migration guide <2.2.0-migration-guide> {.interpreted-text role=\"ref\"} on how to migrate your code. Improve: Spoofax/193 - Stratego warnings in Spoofax language projects with NaBL2 analysis . The excessive number of warnings from Stratego compilation are now filtered out. Improve: Parsing and analysis can report progress and be cancelled. Improve: Builds now report progress. Fix: Path and project path that are passed to the editor hover strategy are now consistent with paths passed to other strategies. Fix: Spoofax/187 - Provide simplified builder API . Fix: Spoofax/188 - Java type error in documented language processing code . Eclipse \u00b6 Upgrade: Eclipse Neon (4.6) is now required. Improve: Added several switches to the Spoofax (meta) menu for disabling analyses and builds, to improve usability in cases where these operations are very slow. Improve: Bind new progress reporting and cancellation in core to Eclipse progress monitors, enabling reporting of builds and cancellation of analysis. Fix: Fix cancellation not being propagated in SubMonitors, preventing cancellation from working in many places. SDF3 \u00b6 Feature: Re-implemented the parse table generator in Java, removing the dependency on a platform-specific sdf2table binary, and fixing several long-standing bugs. This implementation is still being tested, it is therefore only enabled after opt-in. To enable the new implementation, set the following option in your metaborg.yaml file: language : sdf : sdf2table : java Improve: Moved the placeholder and pretty-print options in the metaborg.yaml file to be under language.sdf , as in: language : sdf : placeholder : prefix : \"[[\" suffix : \"]]\" pretty-print : LangName NaBL2 \u00b6 Improve: Introduces a new solver implementation with improved performance. Improve: Introduces separate signature sections for constructors , relations , and functions . Deprecate: The types signature, which will be removed in the next release. SPT \u00b6 Fix: Several origin tracking issues related to section markers. DynSem \u00b6 Fix: Analysis crashes on empty rules sections ( #161 ) Improve: Support for abrupt termination: automatic expansion and propagation of read-write semantic components with default values Improve: Analysis performance improvements Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.2.0/org.metaborg.spoofax.eclipse.updatesite-2.2.0-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.2.0 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.2.0 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.2.0 .","title":"Spoofax 2.2.0 (18-04-2017)"},{"location":"release/note/2.2.0/#spoofax-220-18-04-2017","text":"Spoofax 2.2 improves on Spoofax 2.1 with a new NaBL2 constraint solver which is optimised for performance, improved progress reporting and cancellation in Eclipse, an experimental replacement for sdf2table which fixes several long-standing bugs, improvements to the core API, and several bug fixes. See the corresponding migration guide <2.2.0-migration-guide> for migrating from Spoofax 2.1 to Spoofax 2.2.","title":"Spoofax 2.2.0 (18-04-2017)"},{"location":"release/note/2.2.0/#changes","text":"","title":"Changes"},{"location":"release/note/2.2.0/#overall","text":"The deprecated libraries and files from Spoofax 2.1.0 have been removed. If you have not done so yet, follow the Spoofax 2.1.0 migration guide <2.1.0-migration-guide> {.interpreted-text role=\"ref\"} to migrate your project to the new Spoofax library.","title":"Overall"},{"location":"release/note/2.2.0/#core-api","text":"Improve: Spoofax/190 - Extend API for language discovery . This deprecates several methods in the language discovery API, see the migration guide <2.2.0-migration-guide> {.interpreted-text role=\"ref\"} on how to migrate your code. Improve: Spoofax/193 - Stratego warnings in Spoofax language projects with NaBL2 analysis . The excessive number of warnings from Stratego compilation are now filtered out. Improve: Parsing and analysis can report progress and be cancelled. Improve: Builds now report progress. Fix: Path and project path that are passed to the editor hover strategy are now consistent with paths passed to other strategies. Fix: Spoofax/187 - Provide simplified builder API . Fix: Spoofax/188 - Java type error in documented language processing code .","title":"Core API"},{"location":"release/note/2.2.0/#eclipse","text":"Upgrade: Eclipse Neon (4.6) is now required. Improve: Added several switches to the Spoofax (meta) menu for disabling analyses and builds, to improve usability in cases where these operations are very slow. Improve: Bind new progress reporting and cancellation in core to Eclipse progress monitors, enabling reporting of builds and cancellation of analysis. Fix: Fix cancellation not being propagated in SubMonitors, preventing cancellation from working in many places.","title":"Eclipse"},{"location":"release/note/2.2.0/#sdf3","text":"Feature: Re-implemented the parse table generator in Java, removing the dependency on a platform-specific sdf2table binary, and fixing several long-standing bugs. This implementation is still being tested, it is therefore only enabled after opt-in. To enable the new implementation, set the following option in your metaborg.yaml file: language : sdf : sdf2table : java Improve: Moved the placeholder and pretty-print options in the metaborg.yaml file to be under language.sdf , as in: language : sdf : placeholder : prefix : \"[[\" suffix : \"]]\" pretty-print : LangName","title":"SDF3"},{"location":"release/note/2.2.0/#nabl2","text":"Improve: Introduces a new solver implementation with improved performance. Improve: Introduces separate signature sections for constructors , relations , and functions . Deprecate: The types signature, which will be removed in the next release.","title":"NaBL2"},{"location":"release/note/2.2.0/#spt","text":"Fix: Several origin tracking issues related to section markers.","title":"SPT"},{"location":"release/note/2.2.0/#dynsem","text":"Fix: Analysis crashes on empty rules sections ( #161 ) Improve: Support for abrupt termination: automatic expansion and propagation of read-write semantic components with default values Improve: Analysis performance improvements","title":"DynSem"},{"location":"release/note/2.2.0/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.2.0/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.2.0/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.2.0/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.2.0/org.metaborg.spoofax.eclipse.updatesite-2.2.0-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.2.0/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.2.0 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.2.0/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.2.0/#core-api_1","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.2.0","title":"Core API"},{"location":"release/note/2.2.0/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.2.0/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.2.0 .","title":"Maven artifacts"},{"location":"release/note/2.2.1/","text":"Spoofax 2.2.1 (04-05-2017) \u00b6 Spoofax 2.2.1 is a minor bugfix release. Changes \u00b6 Overall \u00b6 Fix: error when generating projects with analysis enabled (which is enabled by default). Fix: possibly erroneous completions file in spoofax meta library. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.2.1/org.metaborg.spoofax.eclipse.updatesite-2.2.1-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.2.1 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.2.1 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.2.1 .","title":"Spoofax 2.2.1 (04-05-2017)"},{"location":"release/note/2.2.1/#spoofax-221-04-05-2017","text":"Spoofax 2.2.1 is a minor bugfix release.","title":"Spoofax 2.2.1 (04-05-2017)"},{"location":"release/note/2.2.1/#changes","text":"","title":"Changes"},{"location":"release/note/2.2.1/#overall","text":"Fix: error when generating projects with analysis enabled (which is enabled by default). Fix: possibly erroneous completions file in spoofax meta library.","title":"Overall"},{"location":"release/note/2.2.1/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.2.1/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.2.1/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.2.1/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.2.1/org.metaborg.spoofax.eclipse.updatesite-2.2.1-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.2.1/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.2.1 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.2.1/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.2.1/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.2.1","title":"Core API"},{"location":"release/note/2.2.1/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.2.1/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.2.1 .","title":"Maven artifacts"},{"location":"release/note/2.3.0/","text":"Spoofax 2.3.0 (29-09-2017) \u00b6 Spoofax 2.3 fixes several minor bugs, upgrades to the latest Eclipse and Java versions, includes improvements to SDF3 and NaBL2, and introduces experimental parse table generation and parsing features. Changes \u00b6 Overall \u00b6 Improvement: made NaBL2 the default static semantics language. Improvement: put deprecated markers on NaBL+TS and Stratego as static semantics languages, and SDF2 as syntax language. Improvement: allow configuration of source folders in metaborg.yaml. Improvement: allow multiple languages in source and export entries. Improvement: add dynsem as a compile dependency to newly generated languages. Language specification build \u00b6 Fix: occasional NPEs when the build failed. Fix: hidden dependency error when building Stratego concrete syntax extensions. Eclipse plugin \u00b6 Improvement: updated Eclipse instance generator to generate Eclipse Oxygen instances. Improvement: updated Eclipse instance generator to include JDK 8u144b01. Improvement: do not reanalyze already analyzed files when opening an editor. Improvement: use a default configuration if metaborg.yaml is not present. NaBL2 \u00b6 Improvement: extended Stratego API to query reference resolution. Improvement: add ? and + operators to regexp syntax for path well-formedness. Fix: regexp normalization was only one level deep. Fix: non-termination in name resolution in the cases of a direct cycle between a scope. Update: conform to latest DynSem version. Fix: support all Stratego constructor and sort names, by allowing dashes and single quotes in sort and constructor names. Fix: do not crash if dynsem properties file is missing. SDF3 \u00b6 Improvement: more stable SDF3 parser generator. Improvement: new parenthesizer that considers deep priority conflicts. Improvement: (experimental) support for lazy parse table generation, where the parse table is generated on-the-fly by the parser. Fix: bug in the SDF3 normalizer for groups of priorities outside of a chain. Fix: added support for generating the parse table from a \\\"permissive\\\" grammar Fix: not necessary to specify the parse table as sdf-new.tbl in the ESV file when using the new parse table generator. Parser \u00b6 Added the new (experimental) SGLR parser implementation JSGLR2. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.3.0/org.metaborg.spoofax.eclipse.updatesite-2.3.0-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.3.0 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.3.0 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.3.0 .","title":"Spoofax 2.3.0 (29-09-2017)"},{"location":"release/note/2.3.0/#spoofax-230-29-09-2017","text":"Spoofax 2.3 fixes several minor bugs, upgrades to the latest Eclipse and Java versions, includes improvements to SDF3 and NaBL2, and introduces experimental parse table generation and parsing features.","title":"Spoofax 2.3.0 (29-09-2017)"},{"location":"release/note/2.3.0/#changes","text":"","title":"Changes"},{"location":"release/note/2.3.0/#overall","text":"Improvement: made NaBL2 the default static semantics language. Improvement: put deprecated markers on NaBL+TS and Stratego as static semantics languages, and SDF2 as syntax language. Improvement: allow configuration of source folders in metaborg.yaml. Improvement: allow multiple languages in source and export entries. Improvement: add dynsem as a compile dependency to newly generated languages.","title":"Overall"},{"location":"release/note/2.3.0/#language-specification-build","text":"Fix: occasional NPEs when the build failed. Fix: hidden dependency error when building Stratego concrete syntax extensions.","title":"Language specification build"},{"location":"release/note/2.3.0/#eclipse-plugin","text":"Improvement: updated Eclipse instance generator to generate Eclipse Oxygen instances. Improvement: updated Eclipse instance generator to include JDK 8u144b01. Improvement: do not reanalyze already analyzed files when opening an editor. Improvement: use a default configuration if metaborg.yaml is not present.","title":"Eclipse plugin"},{"location":"release/note/2.3.0/#nabl2","text":"Improvement: extended Stratego API to query reference resolution. Improvement: add ? and + operators to regexp syntax for path well-formedness. Fix: regexp normalization was only one level deep. Fix: non-termination in name resolution in the cases of a direct cycle between a scope. Update: conform to latest DynSem version. Fix: support all Stratego constructor and sort names, by allowing dashes and single quotes in sort and constructor names. Fix: do not crash if dynsem properties file is missing.","title":"NaBL2"},{"location":"release/note/2.3.0/#sdf3","text":"Improvement: more stable SDF3 parser generator. Improvement: new parenthesizer that considers deep priority conflicts. Improvement: (experimental) support for lazy parse table generation, where the parse table is generated on-the-fly by the parser. Fix: bug in the SDF3 normalizer for groups of priorities outside of a chain. Fix: added support for generating the parse table from a \\\"permissive\\\" grammar Fix: not necessary to specify the parse table as sdf-new.tbl in the ESV file when using the new parse table generator.","title":"SDF3"},{"location":"release/note/2.3.0/#parser","text":"Added the new (experimental) SGLR parser implementation JSGLR2.","title":"Parser"},{"location":"release/note/2.3.0/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.3.0/#eclipse-plugin_1","text":"","title":"Eclipse plugin"},{"location":"release/note/2.3.0/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.3.0/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.3.0/org.metaborg.spoofax.eclipse.updatesite-2.3.0-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.3.0/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.3.0 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.3.0/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.3.0/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.3.0","title":"Core API"},{"location":"release/note/2.3.0/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.3.0/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.3.0 .","title":"Maven artifacts"},{"location":"release/note/2.4.0/","text":"Spoofax 2.4.0 (09-01-2018) \u00b6 Spoofax 2.4 fixes several bugs and includes a program generator. Changes \u00b6 Eclipse Plugin \u00b6 Fix: re-parse and re-analyze open editors if the language is reloaded. NaBL2 \u00b6 Fix: use deep equality instead of object equality to compare elements in set constraints. Fix: prevent clashes of variable names with known lower-case Stratego constructors. Improvement: add strategies to the Stratego API to query references and declaration associated with AST nodes. Fix: prevent exception traces when hovering over the editor. Fix: bug in Stratego generation when complex terms are used in occurrences. Fix: bug where editor resolution would only consider leaf nodes, but not parents if the leafs do not resolve. Fix: bug where sometimes error messages of files were lost. Parser \u00b6 Improvement: latest JSGLR2 performance optimizations. Fix: bug in JSGLR2 where non-default start symbols were not taken into account. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.4.0/org.metaborg.spoofax.eclipse.updatesite-2.4.0-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.4.0 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.4.0 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.4.0 .","title":"Spoofax 2.4.0 (09-01-2018)"},{"location":"release/note/2.4.0/#spoofax-240-09-01-2018","text":"Spoofax 2.4 fixes several bugs and includes a program generator.","title":"Spoofax 2.4.0 (09-01-2018)"},{"location":"release/note/2.4.0/#changes","text":"","title":"Changes"},{"location":"release/note/2.4.0/#eclipse-plugin","text":"Fix: re-parse and re-analyze open editors if the language is reloaded.","title":"Eclipse Plugin"},{"location":"release/note/2.4.0/#nabl2","text":"Fix: use deep equality instead of object equality to compare elements in set constraints. Fix: prevent clashes of variable names with known lower-case Stratego constructors. Improvement: add strategies to the Stratego API to query references and declaration associated with AST nodes. Fix: prevent exception traces when hovering over the editor. Fix: bug in Stratego generation when complex terms are used in occurrences. Fix: bug where editor resolution would only consider leaf nodes, but not parents if the leafs do not resolve. Fix: bug where sometimes error messages of files were lost.","title":"NaBL2"},{"location":"release/note/2.4.0/#parser","text":"Improvement: latest JSGLR2 performance optimizations. Fix: bug in JSGLR2 where non-default start symbols were not taken into account.","title":"Parser"},{"location":"release/note/2.4.0/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.4.0/#eclipse-plugin_1","text":"","title":"Eclipse plugin"},{"location":"release/note/2.4.0/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.4.0/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.4.0/org.metaborg.spoofax.eclipse.updatesite-2.4.0-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.4.0/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.4.0 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.4.0/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.4.0/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.4.0","title":"Core API"},{"location":"release/note/2.4.0/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.4.0/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.4.0 .","title":"Maven artifacts"},{"location":"release/note/2.4.1/","text":"Spoofax 2.4.1 (29-01-2018) \u00b6 Spoofax 2.4.1 is a minor bugfix release. Changes \u00b6 Fix: remove dependency on nativebundle from jsglr2 , preventing native binaries (with a cygwin vulnerability) from showing up in Spoofax Core. Update jackson-core , jackson-databind , jackson-annotations , jackson-dataformat-yaml dependencies to 2.9.3 to avoid a vulnerability in those libraries. Update commons-configuration2 to 2.2 , commons-configuration2-jackson to 0.7.0 , and snakeyaml to 1.18 , for compatibility with jackson version 2.9.3 . Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.4.1/org.metaborg.spoofax.eclipse.updatesite-2.4.1-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.4.1 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.4.1 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.4.1 .","title":"Spoofax 2.4.1 (29-01-2018)"},{"location":"release/note/2.4.1/#spoofax-241-29-01-2018","text":"Spoofax 2.4.1 is a minor bugfix release.","title":"Spoofax 2.4.1 (29-01-2018)"},{"location":"release/note/2.4.1/#changes","text":"Fix: remove dependency on nativebundle from jsglr2 , preventing native binaries (with a cygwin vulnerability) from showing up in Spoofax Core. Update jackson-core , jackson-databind , jackson-annotations , jackson-dataformat-yaml dependencies to 2.9.3 to avoid a vulnerability in those libraries. Update commons-configuration2 to 2.2 , commons-configuration2-jackson to 0.7.0 , and snakeyaml to 1.18 , for compatibility with jackson version 2.9.3 .","title":"Changes"},{"location":"release/note/2.4.1/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.4.1/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.4.1/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.4.1/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.4.1/org.metaborg.spoofax.eclipse.updatesite-2.4.1-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.4.1/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.4.1 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.4.1/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.4.1/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.4.1","title":"Core API"},{"location":"release/note/2.4.1/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.4.1/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.4.1 .","title":"Maven artifacts"},{"location":"release/note/2.5.0/","text":"Spoofax 2.5.0 (11-09-2018) \u00b6 Spoofax 2.5 introduces FlowSpec, a new meta-language for intra-procedural data-flow analysis; layout-sensitive parsing in SDF3; and has several small improvements and bug fixes. Changes \u00b6 Maven \u00b6 We updated the Guice version that Spoofax uses to 4.2.0. This has the cascading effect that we need Maven 3.5.4, since Spoofax is used in a maven plugin. Be sure to have this version of Maven installed, or you will run into MethodNotFoundException s for com.google.inject.internal.* . Core \u00b6 The constraint analyzer was generalized: The constraint analyzer is now independent of NaBL2, and can be used as a generic analysis mechanism from Stratego. The analysis cycle and the Stratego interface to it are defined ans documented in module libspoofax/analysis/constraint . Fixed a bug where ambiguity errors were not always correctly reported. FlowSpec \u00b6 This release introduces FlowSpec, a new meta-language for intra-procedural data-flow analysis. See the documentation of the language for more details. Stratego \u00b6 Stratego received some small patches to improve user experience. Stratego editor now gives warnings when using left , right and a couple of other variables names as these are also constructors in libstratego-sglr and interpreted as constructor match patterns. When the Stratego compiler generates names for code generation, these now start with the source code name if available or with a constant name related to the language feature (e.g. a where(s) is turned into ?where243;s;!where243 ). Since some generated names turn up in a stack trace from a Stratego, this should improve readability of the stack trace. Complex closures are still named lifted26 , as the compiler cannot replace humans in properly naming things. SPT \u00b6 Problems related to escaping in string terms of expectations are fixed: Double quotes ( \" ) in expectation require escaping using backslashes, but were not unescaped when comparing with actual parse results. This made correct tests containing double quotes in strings fail. This is fixed by unescaping the expectation terms. Formatting expectation terms as strings was different than default ATerms ( \" and \\ were not escaped), which was confusing when a test fails and the actual and expected terms were reported. This is fixed by aligning the SPT expectation terms formatting with default ATerm formatting. NaBL2 \u00b6 Small usability improvements: Empty parameter tuples in rules can be omitted. Accidentally writing a dot instead of a comma before a recursive rule invocation could make that constraint look like a rule without constraints. Layout is now used to give a warning when such a case is written. Fix import problems caused by nabl2.runtime exports. The exports are restricted such that layout syntax and DynSem signatures are not exported anymore. The sorts defined by the runtime are all prefixed with NaBL2 to prevent accidental merges with sorts from the importing language. Allow all Stratego identifiers to be used as constructor names. Solver changes: Adopt new naming convention, with packages named mb.nabl2.* , and artifacts named nabl2.* . Add classes for matching and subtitution of terms, independent of unification. Use the generalized constraint analyzer for the NaBL2 analysis strategy. SDF3 \u00b6 The experimental support for generating Scala case classes from an SDF3 specification was removed. It was incomplete, unmaintained and unused. Added support for Layout Declarations <layout-declarations> {.interpreted-text role=\"ref\"} for layout-sensitive parsing and pretty-printing. Eclipse \u00b6 Small fixes and improvements: Execute builders for languages which have no analysis defined. Previously builders would always wait until an analysis result was produced. Cancel running SPT test suites. It is now possible to cancel a running SPT test suite in the progress window. IntelliJ \u00b6 Small fixes and improvements: Can now be installed into any latest IntelliJ, not just the last version we tested By default runs in IntelliJ 2018.1.1 Simplified project structure Updated dependencies Changes to support Java 9 in the future Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.0/org.metaborg.spoofax.eclipse.updatesite-2.5.0-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.0 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.0 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.0 .","title":"Spoofax 2.5.0 (11-09-2018)"},{"location":"release/note/2.5.0/#spoofax-250-11-09-2018","text":"Spoofax 2.5 introduces FlowSpec, a new meta-language for intra-procedural data-flow analysis; layout-sensitive parsing in SDF3; and has several small improvements and bug fixes.","title":"Spoofax 2.5.0 (11-09-2018)"},{"location":"release/note/2.5.0/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.0/#maven","text":"We updated the Guice version that Spoofax uses to 4.2.0. This has the cascading effect that we need Maven 3.5.4, since Spoofax is used in a maven plugin. Be sure to have this version of Maven installed, or you will run into MethodNotFoundException s for com.google.inject.internal.* .","title":"Maven"},{"location":"release/note/2.5.0/#core","text":"The constraint analyzer was generalized: The constraint analyzer is now independent of NaBL2, and can be used as a generic analysis mechanism from Stratego. The analysis cycle and the Stratego interface to it are defined ans documented in module libspoofax/analysis/constraint . Fixed a bug where ambiguity errors were not always correctly reported.","title":"Core"},{"location":"release/note/2.5.0/#flowspec","text":"This release introduces FlowSpec, a new meta-language for intra-procedural data-flow analysis. See the documentation of the language for more details.","title":"FlowSpec"},{"location":"release/note/2.5.0/#stratego","text":"Stratego received some small patches to improve user experience. Stratego editor now gives warnings when using left , right and a couple of other variables names as these are also constructors in libstratego-sglr and interpreted as constructor match patterns. When the Stratego compiler generates names for code generation, these now start with the source code name if available or with a constant name related to the language feature (e.g. a where(s) is turned into ?where243;s;!where243 ). Since some generated names turn up in a stack trace from a Stratego, this should improve readability of the stack trace. Complex closures are still named lifted26 , as the compiler cannot replace humans in properly naming things.","title":"Stratego"},{"location":"release/note/2.5.0/#spt","text":"Problems related to escaping in string terms of expectations are fixed: Double quotes ( \" ) in expectation require escaping using backslashes, but were not unescaped when comparing with actual parse results. This made correct tests containing double quotes in strings fail. This is fixed by unescaping the expectation terms. Formatting expectation terms as strings was different than default ATerms ( \" and \\ were not escaped), which was confusing when a test fails and the actual and expected terms were reported. This is fixed by aligning the SPT expectation terms formatting with default ATerm formatting.","title":"SPT"},{"location":"release/note/2.5.0/#nabl2","text":"Small usability improvements: Empty parameter tuples in rules can be omitted. Accidentally writing a dot instead of a comma before a recursive rule invocation could make that constraint look like a rule without constraints. Layout is now used to give a warning when such a case is written. Fix import problems caused by nabl2.runtime exports. The exports are restricted such that layout syntax and DynSem signatures are not exported anymore. The sorts defined by the runtime are all prefixed with NaBL2 to prevent accidental merges with sorts from the importing language. Allow all Stratego identifiers to be used as constructor names. Solver changes: Adopt new naming convention, with packages named mb.nabl2.* , and artifacts named nabl2.* . Add classes for matching and subtitution of terms, independent of unification. Use the generalized constraint analyzer for the NaBL2 analysis strategy.","title":"NaBL2"},{"location":"release/note/2.5.0/#sdf3","text":"The experimental support for generating Scala case classes from an SDF3 specification was removed. It was incomplete, unmaintained and unused. Added support for Layout Declarations <layout-declarations> {.interpreted-text role=\"ref\"} for layout-sensitive parsing and pretty-printing.","title":"SDF3"},{"location":"release/note/2.5.0/#eclipse","text":"Small fixes and improvements: Execute builders for languages which have no analysis defined. Previously builders would always wait until an analysis result was produced. Cancel running SPT test suites. It is now possible to cancel a running SPT test suite in the progress window.","title":"Eclipse"},{"location":"release/note/2.5.0/#intellij","text":"Small fixes and improvements: Can now be installed into any latest IntelliJ, not just the last version we tested By default runs in IntelliJ 2018.1.1 Simplified project structure Updated dependencies Changes to support Java 9 in the future","title":"IntelliJ"},{"location":"release/note/2.5.0/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.0/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.0/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.0/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.0/org.metaborg.spoofax.eclipse.updatesite-2.5.0-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.0/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.0 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.0/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.0/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.0","title":"Core API"},{"location":"release/note/2.5.0/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.0/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.0 .","title":"Maven artifacts"},{"location":"release/note/2.5.1/","text":"Spoofax 2.5.1 (02-10-2018) \u00b6 Spoofax 2.5.1 is a minor bugfix release. Changes \u00b6 Core \u00b6 Fix: ( Spoofax/242 ) StrategoMix.def not found error, after incrementally building a language specification project with a Stratego mix grammar. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.1/org.metaborg.spoofax.eclipse.updatesite-2.5.1-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.1 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.1 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.1 .","title":"Spoofax 2.5.1 (02-10-2018)"},{"location":"release/note/2.5.1/#spoofax-251-02-10-2018","text":"Spoofax 2.5.1 is a minor bugfix release.","title":"Spoofax 2.5.1 (02-10-2018)"},{"location":"release/note/2.5.1/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.1/#core","text":"Fix: ( Spoofax/242 ) StrategoMix.def not found error, after incrementally building a language specification project with a Stratego mix grammar.","title":"Core"},{"location":"release/note/2.5.1/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.1/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.1/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.1/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.1/org.metaborg.spoofax.eclipse.updatesite-2.5.1-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.1/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.1 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.1/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.1/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.1","title":"Core API"},{"location":"release/note/2.5.1/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.1/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.1 .","title":"Maven artifacts"},{"location":"release/note/2.5.10/","text":"Spoofax 2.5.10 (07-07-2020) \u00b6 Spoofax 2.5.10 contains several smaller improvements. Changes \u00b6 Overall \u00b6 Update Apache VFS2 to 2.6.0 Support for TypeSmart was removed. We anticipate a more useable type analysis for Stratego in the form of a gradual type system. The metaborg.yaml file of a generated project used to contain a debug: typesmart: false . This was to turn off the TypeSmart dynamic analysis by default. This analysis would stop any Stratego code when it tried to construct a tree that did not conform to the grammar of the project. To our knowledge TypeSmart was not used in any active Spoofax project. It did, however, slow down the build time of all Spoofax projects, because extraction of the grammar into a TypeSmart readable format had to be done even if the analysis was off for that project. These two points, and the anticipation of a gradual type system for Stratego, were the reasons to drop TypeSmart support. SDF3 \u00b6 Lexical and context-free sort declarations: In SDF3 you can now explicitly declare your sorts. Declare lexical sorts in a lexical sorts block, and context-free sorts in a context-free sorts block. Sorts declared in a kernel sorts block default to declaring context-free sorts until a suffix such as -LEX is added. Note that you have to use sdf2table: java to support lexical sorts. Statix \u00b6 New project that use Statix automatically have the Statix signature generator enabled. For this to work properly, declare your lexical and context-free sorts in SDF3 explicitly. See the Statix signature generator <statix-signature-generator> {.interpreted-text role=\"ref\"} documentation for more information. Statix specifications are now compiled as much as possible, even if there are errors in some files. Errors in Statix files that are not actually imported, do not cause analysis to fail on an empty specification anymore. The AST property [type]{.title-ref} is now a built-in, which is automatically used in the default editor hover strategy. Stratego \u00b6 Combined compiled Stratego and helper code Compilation of Stratego and helper code written in Java (in src/main/strategies ) is now combined in a single jar file per Spoofax language instead of two. See the migration guide for more information on what to change in your Spoofax project. SPT \u00b6 SPT gains support for the parse ambiguous expectation, which succeeds when a fragment parses successfully but with ambiguities. Tests with the parse succeeds expectation will now fail when the input parses ambiguously. To write tests for ambiguous parses, use the parse ambiguous expectation instead. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.10/org.metaborg.spoofax.eclipse.updatesite-2.5.10-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.10 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.10 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.10 .","title":"Spoofax 2.5.10 (07-07-2020)"},{"location":"release/note/2.5.10/#spoofax-2510-07-07-2020","text":"Spoofax 2.5.10 contains several smaller improvements.","title":"Spoofax 2.5.10 (07-07-2020)"},{"location":"release/note/2.5.10/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.10/#overall","text":"Update Apache VFS2 to 2.6.0 Support for TypeSmart was removed. We anticipate a more useable type analysis for Stratego in the form of a gradual type system. The metaborg.yaml file of a generated project used to contain a debug: typesmart: false . This was to turn off the TypeSmart dynamic analysis by default. This analysis would stop any Stratego code when it tried to construct a tree that did not conform to the grammar of the project. To our knowledge TypeSmart was not used in any active Spoofax project. It did, however, slow down the build time of all Spoofax projects, because extraction of the grammar into a TypeSmart readable format had to be done even if the analysis was off for that project. These two points, and the anticipation of a gradual type system for Stratego, were the reasons to drop TypeSmart support.","title":"Overall"},{"location":"release/note/2.5.10/#sdf3","text":"Lexical and context-free sort declarations: In SDF3 you can now explicitly declare your sorts. Declare lexical sorts in a lexical sorts block, and context-free sorts in a context-free sorts block. Sorts declared in a kernel sorts block default to declaring context-free sorts until a suffix such as -LEX is added. Note that you have to use sdf2table: java to support lexical sorts.","title":"SDF3"},{"location":"release/note/2.5.10/#statix","text":"New project that use Statix automatically have the Statix signature generator enabled. For this to work properly, declare your lexical and context-free sorts in SDF3 explicitly. See the Statix signature generator <statix-signature-generator> {.interpreted-text role=\"ref\"} documentation for more information. Statix specifications are now compiled as much as possible, even if there are errors in some files. Errors in Statix files that are not actually imported, do not cause analysis to fail on an empty specification anymore. The AST property [type]{.title-ref} is now a built-in, which is automatically used in the default editor hover strategy.","title":"Statix"},{"location":"release/note/2.5.10/#stratego","text":"Combined compiled Stratego and helper code Compilation of Stratego and helper code written in Java (in src/main/strategies ) is now combined in a single jar file per Spoofax language instead of two. See the migration guide for more information on what to change in your Spoofax project.","title":"Stratego"},{"location":"release/note/2.5.10/#spt","text":"SPT gains support for the parse ambiguous expectation, which succeeds when a fragment parses successfully but with ambiguities. Tests with the parse succeeds expectation will now fail when the input parses ambiguously. To write tests for ambiguous parses, use the parse ambiguous expectation instead.","title":"SPT"},{"location":"release/note/2.5.10/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.10/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.10/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.10/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.10/org.metaborg.spoofax.eclipse.updatesite-2.5.10-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.10/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.10 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.10/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.10/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.10","title":"Core API"},{"location":"release/note/2.5.10/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.10/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.10 .","title":"Maven artifacts"},{"location":"release/note/2.5.11/","text":"Spoofax 2.5.11 (17-07-2020) \u00b6 Spoofax 2.5.11 contains a dependency bugfix. Changes \u00b6 Overall \u00b6 Exclude the hadoop-hdfs-client transitive dependency from Apache VFS2 Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.11/org.metaborg.spoofax.eclipse.updatesite-2.5.11-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.11 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.11 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.11 .","title":"Spoofax 2.5.11 (17-07-2020)"},{"location":"release/note/2.5.11/#spoofax-2511-17-07-2020","text":"Spoofax 2.5.11 contains a dependency bugfix.","title":"Spoofax 2.5.11 (17-07-2020)"},{"location":"release/note/2.5.11/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.11/#overall","text":"Exclude the hadoop-hdfs-client transitive dependency from Apache VFS2","title":"Overall"},{"location":"release/note/2.5.11/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.11/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.11/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.11/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.11/org.metaborg.spoofax.eclipse.updatesite-2.5.11-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.11/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.11 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.11/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.11/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.11","title":"Core API"},{"location":"release/note/2.5.11/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.11/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.11 .","title":"Maven artifacts"},{"location":"release/note/2.5.12/","text":"Spoofax 2.5.12 (08-10-2020) \u00b6 Spoofax 2.5.12 contains an experimental gradual type system for Stratego, performance improvements to NaBL2 and Statix, and updates to Eclipse installations and their embedded JREs. Changes \u00b6 Stratego \u00b6 Stratego has two new reserved words: cast and is . Local variables can be reserved words if they start with ' , so you can use 'cast and 'is . Under the Stratego language options in your metaborg.yaml file you can turn on the gradual type system, if you use the incremental compiler. This option is gradual: static , and only tests the types statically. The default is gradual: none right now, meaning the gradual type system is not on by default. The is dynamic type check is not yet supported at runtime, you may get Java compilation errors when attempting to compile Stratego code with that. Dynamic type casts inserted by the gradual type system are also forthcoming, runtime support for this is not yet ready. NaBL2 \u00b6 NaBL2 supports a new resolution algorithm based on fexid-point environment computation instead of graph search, which can be enabled by adding strategy environments to the name-resolution signature section. It has much better performance characteristics, especially when dealing with mutually importing scopes and transitive imports. Compared the the search-based, the environment-based algorithm can get stuck on scope graphs with cycles involving scopes importing references that can be resolved via that same scope. Note that the environment-based algorithm may increase memory usage. The default remains the search-based algorithm. If a file was already analyzed in the editor, it is not reanalyzed on save anoymore. Statix \u00b6 Analysis times of large, multi-file Statix specifications has improved significantly. If a file was already analyzed in the editor, it is not reanalyzed on save anoymore. Eclipse \u00b6 Premade Eclipse installations have been updated from Eclipse Photon to Eclipse 2020-6. Premade Eclipse installations for 32-bit Linux are no longer created. Embedded JRE in premade Eclipse installations has been updated from 8u162 (Oracle JRE) to 8u265-b01 (AdoptOpenJDK). Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.12/org.metaborg.spoofax.eclipse.updatesite-2.5.12-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.12 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.12 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.12 .","title":"Spoofax 2.5.12 (08-10-2020)"},{"location":"release/note/2.5.12/#spoofax-2512-08-10-2020","text":"Spoofax 2.5.12 contains an experimental gradual type system for Stratego, performance improvements to NaBL2 and Statix, and updates to Eclipse installations and their embedded JREs.","title":"Spoofax 2.5.12 (08-10-2020)"},{"location":"release/note/2.5.12/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.12/#stratego","text":"Stratego has two new reserved words: cast and is . Local variables can be reserved words if they start with ' , so you can use 'cast and 'is . Under the Stratego language options in your metaborg.yaml file you can turn on the gradual type system, if you use the incremental compiler. This option is gradual: static , and only tests the types statically. The default is gradual: none right now, meaning the gradual type system is not on by default. The is dynamic type check is not yet supported at runtime, you may get Java compilation errors when attempting to compile Stratego code with that. Dynamic type casts inserted by the gradual type system are also forthcoming, runtime support for this is not yet ready.","title":"Stratego"},{"location":"release/note/2.5.12/#nabl2","text":"NaBL2 supports a new resolution algorithm based on fexid-point environment computation instead of graph search, which can be enabled by adding strategy environments to the name-resolution signature section. It has much better performance characteristics, especially when dealing with mutually importing scopes and transitive imports. Compared the the search-based, the environment-based algorithm can get stuck on scope graphs with cycles involving scopes importing references that can be resolved via that same scope. Note that the environment-based algorithm may increase memory usage. The default remains the search-based algorithm. If a file was already analyzed in the editor, it is not reanalyzed on save anoymore.","title":"NaBL2"},{"location":"release/note/2.5.12/#statix","text":"Analysis times of large, multi-file Statix specifications has improved significantly. If a file was already analyzed in the editor, it is not reanalyzed on save anoymore.","title":"Statix"},{"location":"release/note/2.5.12/#eclipse","text":"Premade Eclipse installations have been updated from Eclipse Photon to Eclipse 2020-6. Premade Eclipse installations for 32-bit Linux are no longer created. Embedded JRE in premade Eclipse installations has been updated from 8u162 (Oracle JRE) to 8u265-b01 (AdoptOpenJDK).","title":"Eclipse"},{"location":"release/note/2.5.12/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.12/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.12/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.12/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.12/org.metaborg.spoofax.eclipse.updatesite-2.5.12-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.12/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.12 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.12/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.12/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.12","title":"Core API"},{"location":"release/note/2.5.12/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.12/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.12 .","title":"Maven artifacts"},{"location":"release/note/2.5.13/","text":"Spoofax 2.5.13 (20-11-2020) \u00b6 Spoofax 2.5.13 contains a couple of small improvements. Changes \u00b6 SDF3 \u00b6 prefer and avoid are now deprecated. Usages of the operators will be marked with a deprecation warning. Parser \u00b6 The JSGLR2 parser variants now report warnings on ambiguously parsed substrings. This includes ambiguities in lexical and layout syntax that do not result into amb nodes in the AST. SPT \u00b6 The run expectation now allows to call strategies with term arguments. It\\'s now also possible to test if a strategy failed. See the SPT documentation for more details. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.13/org.metaborg.spoofax.eclipse.updatesite-2.5.13-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.13 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.13 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.13 .","title":"Spoofax 2.5.13 (20-11-2020)"},{"location":"release/note/2.5.13/#spoofax-2513-20-11-2020","text":"Spoofax 2.5.13 contains a couple of small improvements.","title":"Spoofax 2.5.13 (20-11-2020)"},{"location":"release/note/2.5.13/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.13/#sdf3","text":"prefer and avoid are now deprecated. Usages of the operators will be marked with a deprecation warning.","title":"SDF3"},{"location":"release/note/2.5.13/#parser","text":"The JSGLR2 parser variants now report warnings on ambiguously parsed substrings. This includes ambiguities in lexical and layout syntax that do not result into amb nodes in the AST.","title":"Parser"},{"location":"release/note/2.5.13/#spt","text":"The run expectation now allows to call strategies with term arguments. It\\'s now also possible to test if a strategy failed. See the SPT documentation for more details.","title":"SPT"},{"location":"release/note/2.5.13/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.13/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.13/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.13/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.13/org.metaborg.spoofax.eclipse.updatesite-2.5.13-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.13/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.13 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.13/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.13/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.13","title":"Core API"},{"location":"release/note/2.5.13/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.13/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.13 .","title":"Maven artifacts"},{"location":"release/note/2.5.14/","text":"Spoofax 2.5.14 (16-12-2020) \u00b6 Spoofax 2.5.14 was released. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.14/org.metaborg.spoofax.eclipse.updatesite-2.5.14-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.14 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.14 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.14 .","title":"Spoofax 2.5.14 (16-12-2020)"},{"location":"release/note/2.5.14/#spoofax-2514-16-12-2020","text":"Spoofax 2.5.14 was released.","title":"Spoofax 2.5.14 (16-12-2020)"},{"location":"release/note/2.5.14/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.14/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.14/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.14/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.14/org.metaborg.spoofax.eclipse.updatesite-2.5.14-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.14/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.14 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.14/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.14/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.14","title":"Core API"},{"location":"release/note/2.5.14/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.14/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.14 .","title":"Maven artifacts"},{"location":"release/note/2.5.15/","text":"Spoofax 2.5.15 (11-05-2021) \u00b6 Spoofax 2.5.15 contains a couple of small improvements and bug fixes, and supports the old SDF2-based parse table generator on macOS Catalina (10.15) and above. See the corresponding migration guide for migrating from Spoofax 2.5.14 to Spoofax 2.5.15. Changes \u00b6 macOS \u00b6 On macOS, Spoofax temporarily requires Docker and coreutils when building Spoofax on macOS Catalina, Big Sur, or newer. (This is only when you build Spoofax yourself instead of downloading it for this website, it does not influence building Spoofax projects.) SDF3 \u00b6 Fixed tree indexes in layout constraints/declarations to make them 0-based. The generate namespaced grammar option will now generate the namespaced grammar in src-gen . This feature can also be set to generate the grammar automatically similar to other extractions of the grammar like Stratego signatures. See the documentation for more information. Sadly, due to a bug in the changes for automatic generation, a build in Eclipse of a language project with namespaced grammar will work, but the build of that project with Maven will not work. Statix \u00b6 Fixed origin tracking in Statix injection explication for new projects that caused the top-level term of an AST to be missing when a Stratego strategy is applied to an analyzed AST in an SPT test. Add a menu action to view the scope graph resulting from Statix analysis. Deprecate namespaces, occurrences and query sugar. Fix bug in evaluation of try construct. Improvements to memory usage and runtime of the solver. Improve rule overlap handling: consider variables already bound to the left more specific than concrete patterns, to keep with left-to-right specificity. Add configuration settings to control trace length and term depth in error messages. Stratego \u00b6 The previously advertised incremental compiler was considered too slow and attempts to make it faster made it less stable. It is currently not recommended for general use, while we develop a new version. The documentation on how to use contains a similar warning now. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.15/org.metaborg.spoofax.eclipse.updatesite-2.5.15-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.15 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.15 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.15 .","title":"Spoofax 2.5.15 (11-05-2021)"},{"location":"release/note/2.5.15/#spoofax-2515-11-05-2021","text":"Spoofax 2.5.15 contains a couple of small improvements and bug fixes, and supports the old SDF2-based parse table generator on macOS Catalina (10.15) and above. See the corresponding migration guide for migrating from Spoofax 2.5.14 to Spoofax 2.5.15.","title":"Spoofax 2.5.15 (11-05-2021)"},{"location":"release/note/2.5.15/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.15/#macos","text":"On macOS, Spoofax temporarily requires Docker and coreutils when building Spoofax on macOS Catalina, Big Sur, or newer. (This is only when you build Spoofax yourself instead of downloading it for this website, it does not influence building Spoofax projects.)","title":"macOS"},{"location":"release/note/2.5.15/#sdf3","text":"Fixed tree indexes in layout constraints/declarations to make them 0-based. The generate namespaced grammar option will now generate the namespaced grammar in src-gen . This feature can also be set to generate the grammar automatically similar to other extractions of the grammar like Stratego signatures. See the documentation for more information. Sadly, due to a bug in the changes for automatic generation, a build in Eclipse of a language project with namespaced grammar will work, but the build of that project with Maven will not work.","title":"SDF3"},{"location":"release/note/2.5.15/#statix","text":"Fixed origin tracking in Statix injection explication for new projects that caused the top-level term of an AST to be missing when a Stratego strategy is applied to an analyzed AST in an SPT test. Add a menu action to view the scope graph resulting from Statix analysis. Deprecate namespaces, occurrences and query sugar. Fix bug in evaluation of try construct. Improvements to memory usage and runtime of the solver. Improve rule overlap handling: consider variables already bound to the left more specific than concrete patterns, to keep with left-to-right specificity. Add configuration settings to control trace length and term depth in error messages.","title":"Statix"},{"location":"release/note/2.5.15/#stratego","text":"The previously advertised incremental compiler was considered too slow and attempts to make it faster made it less stable. It is currently not recommended for general use, while we develop a new version. The documentation on how to use contains a similar warning now.","title":"Stratego"},{"location":"release/note/2.5.15/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.15/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.15/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.15/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.15/org.metaborg.spoofax.eclipse.updatesite-2.5.15-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.15/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.15 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.15/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.15/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.15","title":"Core API"},{"location":"release/note/2.5.15/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.15/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.15 .","title":"Maven artifacts"},{"location":"release/note/2.5.16/","text":"Spoofax 2.5.16 (04-06-2021) \u00b6 Spoofax 2.5.16 contains a couple of small improvements and bug fixes. Changes \u00b6 SDF3 \u00b6 Fix a bug with the automatic generation of namespaced grammars, which was introduced in the previous release. Statix \u00b6 Added stc-get-ast-ref rule to the Stratego API, which can be used to query ref properties. The Stratego primitives now issue console warnings when invalid labels or properties are used. Fixed a bug where stx-get-scopegraph-data would return unification variables instead of their values. Changed the default data order to true , to make queries where only a label order is provided apply shadowing as expected. Added a menu option to execute tests with the concurrent solver. Fixed a completeness bug in the traditional solver when executing queries in dataWf or dataLeq predicates. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.16 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.16 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.16 .","title":"Spoofax 2.5.16 (04-06-2021)"},{"location":"release/note/2.5.16/#spoofax-2516-04-06-2021","text":"Spoofax 2.5.16 contains a couple of small improvements and bug fixes.","title":"Spoofax 2.5.16 (04-06-2021)"},{"location":"release/note/2.5.16/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.16/#sdf3","text":"Fix a bug with the automatic generation of namespaced grammars, which was introduced in the previous release.","title":"SDF3"},{"location":"release/note/2.5.16/#statix","text":"Added stc-get-ast-ref rule to the Stratego API, which can be used to query ref properties. The Stratego primitives now issue console warnings when invalid labels or properties are used. Fixed a bug where stx-get-scopegraph-data would return unification variables instead of their values. Changed the default data order to true , to make queries where only a label order is provided apply shadowing as expected. Added a menu option to execute tests with the concurrent solver. Fixed a completeness bug in the traditional solver when executing queries in dataWf or dataLeq predicates.","title":"Statix"},{"location":"release/note/2.5.16/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.16/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.16/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.16/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.16/org.metaborg.spoofax.eclipse.updatesite-2.5.16-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.16/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.16 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.16/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.16/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.16","title":"Core API"},{"location":"release/note/2.5.16/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.16/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.16 .","title":"Maven artifacts"},{"location":"release/note/2.5.2/","text":"Spoofax 2.5.2 (12-03-2019) \u00b6 Spoofax 2.5.2 is a minor bugfix and performance improvement release. Changes \u00b6 NaBL2 \u00b6 A bug introduced in 2.5.2 would remove the parse error in the editor as soon as analysis failed. This bug has been fixed. FlowSpec \u00b6 A whole host of bugs has been fixed in FlowSpec, mostly ones that lead to no clear error message. Much of the system has also been optimized for speed. Stratego \u00b6 Separate compilation for Stratego was added in this release. It is currently still experimental. It is documented as a separate item under the Stratego documentation, including instructions on how to opt-in to it and what its limitations are. The Stratego primitives all , some and one sometimes lost annotations and origins of list tails when an element in the list was transformed. This bug has been fixed. The Stratego editor used to give spurious errors on missing variable definitions if those were list variables that were bound in a concrete syntax pattern. This long-standing bug has been fixed in this release. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.2/org.metaborg.spoofax.eclipse.updatesite-2.5.2-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.2 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.2 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.2 .","title":"Spoofax 2.5.2 (12-03-2019)"},{"location":"release/note/2.5.2/#spoofax-252-12-03-2019","text":"Spoofax 2.5.2 is a minor bugfix and performance improvement release.","title":"Spoofax 2.5.2 (12-03-2019)"},{"location":"release/note/2.5.2/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.2/#nabl2","text":"A bug introduced in 2.5.2 would remove the parse error in the editor as soon as analysis failed. This bug has been fixed.","title":"NaBL2"},{"location":"release/note/2.5.2/#flowspec","text":"A whole host of bugs has been fixed in FlowSpec, mostly ones that lead to no clear error message. Much of the system has also been optimized for speed.","title":"FlowSpec"},{"location":"release/note/2.5.2/#stratego","text":"Separate compilation for Stratego was added in this release. It is currently still experimental. It is documented as a separate item under the Stratego documentation, including instructions on how to opt-in to it and what its limitations are. The Stratego primitives all , some and one sometimes lost annotations and origins of list tails when an element in the list was transformed. This bug has been fixed. The Stratego editor used to give spurious errors on missing variable definitions if those were list variables that were bound in a concrete syntax pattern. This long-standing bug has been fixed in this release.","title":"Stratego"},{"location":"release/note/2.5.2/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.2/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.2/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.2/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.2/org.metaborg.spoofax.eclipse.updatesite-2.5.2-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.2/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.2 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.2/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.2/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.2","title":"Core API"},{"location":"release/note/2.5.2/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.2/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.2 .","title":"Maven artifacts"},{"location":"release/note/2.5.3/","text":"Spoofax 2.5.3 (02-05-2019) \u00b6 Spoofax 2.5.3 is a minor release with bugfixes, performance improvements, and new small and/or experimental features. Changes \u00b6 Overall \u00b6 Added support for getting the selected term in Stratego builders/transformations. In the builder tuple (node, _, ast, path, projectPath) , the first term ( node ) is now the selected term when a builder is executed in the context of an editor with a selection. The term is selected by finding the outermost term that has an origin that fits in the selection. Fixed a bug that prevented source transformations from being run if context or analysis were missing. Changed constraint analyzer to support more multi-file scenarios. JSGLR2 \u00b6 Added an incremental variant of the JSGLR2 parser (experimental). NaBL2 \u00b6 Improved preformance of AST resolution lookups. Statix (experimental) \u00b6 Fixed bugs and improved performance. Eclipse \u00b6 Added a lifecycle mapping that adds a Spoofax nature to an imported spoofax-project. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.3/org.metaborg.spoofax.eclipse.updatesite-2.5.3-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.3 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.3 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.3 .","title":"Spoofax 2.5.3 (02-05-2019)"},{"location":"release/note/2.5.3/#spoofax-253-02-05-2019","text":"Spoofax 2.5.3 is a minor release with bugfixes, performance improvements, and new small and/or experimental features.","title":"Spoofax 2.5.3 (02-05-2019)"},{"location":"release/note/2.5.3/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.3/#overall","text":"Added support for getting the selected term in Stratego builders/transformations. In the builder tuple (node, _, ast, path, projectPath) , the first term ( node ) is now the selected term when a builder is executed in the context of an editor with a selection. The term is selected by finding the outermost term that has an origin that fits in the selection. Fixed a bug that prevented source transformations from being run if context or analysis were missing. Changed constraint analyzer to support more multi-file scenarios.","title":"Overall"},{"location":"release/note/2.5.3/#jsglr2","text":"Added an incremental variant of the JSGLR2 parser (experimental).","title":"JSGLR2"},{"location":"release/note/2.5.3/#nabl2","text":"Improved preformance of AST resolution lookups.","title":"NaBL2"},{"location":"release/note/2.5.3/#statix-experimental","text":"Fixed bugs and improved performance.","title":"Statix (experimental)"},{"location":"release/note/2.5.3/#eclipse","text":"Added a lifecycle mapping that adds a Spoofax nature to an imported spoofax-project.","title":"Eclipse"},{"location":"release/note/2.5.3/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.3/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.3/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.3/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.3/org.metaborg.spoofax.eclipse.updatesite-2.5.3-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.3/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.3 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.3/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.3/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.3","title":"Core API"},{"location":"release/note/2.5.3/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.3/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.3 .","title":"Maven artifacts"},{"location":"release/note/2.5.4/","text":"Spoofax 2.5.4 (08-05-2019) \u00b6 Spoofax 2.5.4 is a minor bugfix release. Changes \u00b6 Maven \u00b6 Fixed failing SPT tests failing the build immediately. All SPT files are processed and a summary of how many tests failed is shown at the end. Fixed class loading errors at the end of Maven builds. Fixed application icon from showing up when building languages on some platforms. Statix \u00b6 Fix Statix analysis crash when detailed logging is enabled. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.4/org.metaborg.spoofax.eclipse.updatesite-2.5.4-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.4 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.4 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.4 .","title":"Spoofax 2.5.4 (08-05-2019)"},{"location":"release/note/2.5.4/#spoofax-254-08-05-2019","text":"Spoofax 2.5.4 is a minor bugfix release.","title":"Spoofax 2.5.4 (08-05-2019)"},{"location":"release/note/2.5.4/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.4/#maven","text":"Fixed failing SPT tests failing the build immediately. All SPT files are processed and a summary of how many tests failed is shown at the end. Fixed class loading errors at the end of Maven builds. Fixed application icon from showing up when building languages on some platforms.","title":"Maven"},{"location":"release/note/2.5.4/#statix","text":"Fix Statix analysis crash when detailed logging is enabled.","title":"Statix"},{"location":"release/note/2.5.4/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.4/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.4/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.4/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.4/org.metaborg.spoofax.eclipse.updatesite-2.5.4-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.4/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.4 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.4/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.4/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.4","title":"Core API"},{"location":"release/note/2.5.4/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.4/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.4 .","title":"Maven artifacts"},{"location":"release/note/2.5.5/","text":"Spoofax 2.5.5 (23-05-2019) \u00b6 Spoofax 2.5.5 is a minor bugfix release. There are a few incompatiable changes in Statix, which are described in the migration guide <2.5.5-migration-guide> . Changes \u00b6 Overall \u00b6 Do not throw away error messages in unchanged files if other files changed, when using constraint analyzer. JSGLR \u00b6 Add missing location information on sublists. Statix \u00b6 Improve speed of normalization. Add AST properties and editor reference resolution. Regular expression and label order are direct parameters to queries. It is not possible anymore to pass an arbitary predicate there. Special path constraints are removed in favour of concrete path terms that can be matched as terms. Functional constraints can only have a single output. Namespace based resolution short-hands must contain a occurrence literal, and explicit resolution policies. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.5/org.metaborg.spoofax.eclipse.updatesite-2.5.5-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.5 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.5 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.5 .","title":"Spoofax 2.5.5 (23-05-2019)"},{"location":"release/note/2.5.5/#spoofax-255-23-05-2019","text":"Spoofax 2.5.5 is a minor bugfix release. There are a few incompatiable changes in Statix, which are described in the migration guide <2.5.5-migration-guide> .","title":"Spoofax 2.5.5 (23-05-2019)"},{"location":"release/note/2.5.5/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.5/#overall","text":"Do not throw away error messages in unchanged files if other files changed, when using constraint analyzer.","title":"Overall"},{"location":"release/note/2.5.5/#jsglr","text":"Add missing location information on sublists.","title":"JSGLR"},{"location":"release/note/2.5.5/#statix","text":"Improve speed of normalization. Add AST properties and editor reference resolution. Regular expression and label order are direct parameters to queries. It is not possible anymore to pass an arbitary predicate there. Special path constraints are removed in favour of concrete path terms that can be matched as terms. Functional constraints can only have a single output. Namespace based resolution short-hands must contain a occurrence literal, and explicit resolution policies.","title":"Statix"},{"location":"release/note/2.5.5/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.5/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.5/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.5/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.5/org.metaborg.spoofax.eclipse.updatesite-2.5.5-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.5/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.5 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.5/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.5/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.5","title":"Core API"},{"location":"release/note/2.5.5/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.5/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.5 .","title":"Maven artifacts"},{"location":"release/note/2.5.6/","text":"Spoofax 2.5.6 (24-05-2019) \u00b6 Spoofax 2.5.6 is a minor bugfix release. Changes \u00b6 Overall \u00b6 Statix \u00b6 Fix a crash in single-file analysis. Fix several bugs in scope extension checking. Fix bug in rule application that dropped cause. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.6/org.metaborg.spoofax.eclipse.updatesite-2.5.6-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.6 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.6 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.6 .","title":"Spoofax 2.5.6 (24-05-2019)"},{"location":"release/note/2.5.6/#spoofax-256-24-05-2019","text":"Spoofax 2.5.6 is a minor bugfix release.","title":"Spoofax 2.5.6 (24-05-2019)"},{"location":"release/note/2.5.6/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.6/#overall","text":"","title":"Overall"},{"location":"release/note/2.5.6/#statix","text":"Fix a crash in single-file analysis. Fix several bugs in scope extension checking. Fix bug in rule application that dropped cause.","title":"Statix"},{"location":"release/note/2.5.6/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.6/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.6/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.6/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.6/org.metaborg.spoofax.eclipse.updatesite-2.5.6-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.6/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.6 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.6/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.6/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.6","title":"Core API"},{"location":"release/note/2.5.6/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.6/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.6 .","title":"Maven artifacts"},{"location":"release/note/2.5.7/","text":"Spoofax 2.5.7 (26-06-2019) \u00b6 Spoofax 2.5.7 includes minor bugfixes and improvements to the experimental Stratego separate compiler. Changes \u00b6 FlowSpec \u00b6 Bugfix: Names with namespaces were broken in an earlier version during performance optimization. The error would like: java.lang.AssertionError: Unrecognised Namespace: Namespace(\"Var\") . Stratego \u00b6 Stratego separate compilation is now switched to the new system. It no longer has any limitations that were previously mentioned. Do note that separate compilation will give the same stricter error messages that the editor does: You need to import anything you use, you cannot use something that another module imports that imports your module. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.7/org.metaborg.spoofax.eclipse.updatesite-2.5.7-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.7 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.7 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.7 .","title":"Spoofax 2.5.7 (26-06-2019)"},{"location":"release/note/2.5.7/#spoofax-257-26-06-2019","text":"Spoofax 2.5.7 includes minor bugfixes and improvements to the experimental Stratego separate compiler.","title":"Spoofax 2.5.7 (26-06-2019)"},{"location":"release/note/2.5.7/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.7/#flowspec","text":"Bugfix: Names with namespaces were broken in an earlier version during performance optimization. The error would like: java.lang.AssertionError: Unrecognised Namespace: Namespace(\"Var\") .","title":"FlowSpec"},{"location":"release/note/2.5.7/#stratego","text":"Stratego separate compilation is now switched to the new system. It no longer has any limitations that were previously mentioned. Do note that separate compilation will give the same stricter error messages that the editor does: You need to import anything you use, you cannot use something that another module imports that imports your module.","title":"Stratego"},{"location":"release/note/2.5.7/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.7/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.7/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.7/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.7/org.metaborg.spoofax.eclipse.updatesite-2.5.7-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.7/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.7 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.7/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.7/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.7","title":"Core API"},{"location":"release/note/2.5.7/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.7/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.7 .","title":"Maven artifacts"},{"location":"release/note/2.5.8/","text":"Spoofax 2.5.8 (28-04-2020) \u00b6 Spoofax 2.5.8 includes several bugfixes and improvements. Changes \u00b6 SDF \u00b6 The Java version of sdf2table is now slightly faster and takes up less peak memory due to improvements in writing away the parsetable to file. The old (Aster based) version of make-permissive (to add error recovery to your grammar) used to be called in a way that create a small memory leak, which would compound over time with subsequent builds. This is has now been fixed. The old version of make-permissive is only in effect if you use sdf2table: c in your metaborg.yaml file. Parser \u00b6 Add two experimental variants to the JSGLR2 parser: recovery and recovery-incremental . Add Unicode support to the JSGLR1 and JSGLR2 parsers. The meta-languages themselves do not support Unicode yet, because they are bootstrapped with and old version of SDF3. However, other languages built with Spoofax can use Unicode. Add logging to the JSGLR2 parser. Configure by setting language.sdf.jsglr2-logging to all , none , minimal , parsing or recovery in metaborg.yaml . Programmatic API \u00b6 TermFactory for building Stratego terms now supports a builder for lists that creates an arraylist-like structure instead of the standard linkedlist-like structure. This is typically more efficient for building stratego list terms in Java. Add org.spoofax.terms.util.TermUtils class with functions for working with terms. This replaces the equivalent (now deprecated) functions in org.spoofax.interpreter.core.Tools . NaBL2 \u00b6 Improve error message location when scopes are used as term indices. Dropped support for polymorphism, which was unsound. Small improvements to solver performance. Add support for external calls for language with Stratego JAR compilation. Statix \u00b6 Ability to automatically generate <statix-signature-generator> {.interpreted-text role=\"ref\"} Statix signatures from SDF3 specifications. Add support for importing other modules in Statix specifications. Add support for custom messages, and a try construct for warnings and notes. Add support for adding multiple values to AST properties. Improve disunification support in the solver. Extend reserved keywords to fix parsing problems. Several smaller bugfixes. Overall \u00b6 Fixed several issues with files not being released properly, causing file I/O errors on Windows. Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.8/org.metaborg.spoofax.eclipse.updatesite-2.5.8-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.8 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.8 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.8 .","title":"Spoofax 2.5.8 (28-04-2020)"},{"location":"release/note/2.5.8/#spoofax-258-28-04-2020","text":"Spoofax 2.5.8 includes several bugfixes and improvements.","title":"Spoofax 2.5.8 (28-04-2020)"},{"location":"release/note/2.5.8/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.8/#sdf","text":"The Java version of sdf2table is now slightly faster and takes up less peak memory due to improvements in writing away the parsetable to file. The old (Aster based) version of make-permissive (to add error recovery to your grammar) used to be called in a way that create a small memory leak, which would compound over time with subsequent builds. This is has now been fixed. The old version of make-permissive is only in effect if you use sdf2table: c in your metaborg.yaml file.","title":"SDF"},{"location":"release/note/2.5.8/#parser","text":"Add two experimental variants to the JSGLR2 parser: recovery and recovery-incremental . Add Unicode support to the JSGLR1 and JSGLR2 parsers. The meta-languages themselves do not support Unicode yet, because they are bootstrapped with and old version of SDF3. However, other languages built with Spoofax can use Unicode. Add logging to the JSGLR2 parser. Configure by setting language.sdf.jsglr2-logging to all , none , minimal , parsing or recovery in metaborg.yaml .","title":"Parser"},{"location":"release/note/2.5.8/#programmatic-api","text":"TermFactory for building Stratego terms now supports a builder for lists that creates an arraylist-like structure instead of the standard linkedlist-like structure. This is typically more efficient for building stratego list terms in Java. Add org.spoofax.terms.util.TermUtils class with functions for working with terms. This replaces the equivalent (now deprecated) functions in org.spoofax.interpreter.core.Tools .","title":"Programmatic API"},{"location":"release/note/2.5.8/#nabl2","text":"Improve error message location when scopes are used as term indices. Dropped support for polymorphism, which was unsound. Small improvements to solver performance. Add support for external calls for language with Stratego JAR compilation.","title":"NaBL2"},{"location":"release/note/2.5.8/#statix","text":"Ability to automatically generate <statix-signature-generator> {.interpreted-text role=\"ref\"} Statix signatures from SDF3 specifications. Add support for importing other modules in Statix specifications. Add support for custom messages, and a try construct for warnings and notes. Add support for adding multiple values to AST properties. Improve disunification support in the solver. Extend reserved keywords to fix parsing problems. Several smaller bugfixes.","title":"Statix"},{"location":"release/note/2.5.8/#overall","text":"Fixed several issues with files not being released properly, causing file I/O errors on Windows.","title":"Overall"},{"location":"release/note/2.5.8/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.8/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.8/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.8/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.8/org.metaborg.spoofax.eclipse.updatesite-2.5.8-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.8/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.8 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.8/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.8/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.8","title":"Core API"},{"location":"release/note/2.5.8/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.8/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.8 .","title":"Maven artifacts"},{"location":"release/note/2.5.9/","text":"Spoofax 2.5.9 (08-05-2020) \u00b6 Spoofax 2.5.9 includes dependency upgrades. Changes \u00b6 Overall \u00b6 The following dependencies of Spoofax Core have been updated to the latest version: com.netflix.rxjava:rxjava:0.20.7 -> io.reactivex.rxjava3:rxjava:3.0.2 New transitive dependency: org.reactivestreams:reactive-streams:1.0.3 org.apache.commons:commons-configuration2:2.2 -> org.apache.commons:commons-configuration2:2.7 New transitive dependency: org.apache.commons:commons-text:1.8 com.virtlink.commons:commons-configuration2-jackson:0.7.0 -> com.virtlink.commons:commons-configuration2-jackson:0.10.0 com.fasterxml.jackson.core:jackson-core:2.9.5 -> com.fasterxml.jackson.core:jackson-core:2.11.0 com.fasterxml.jackson.core:jackson-databind:2.9.5 -> com.fasterxml.jackson.core:jackson-databind:2.11.0 com.fasterxml.jackson.core:jackson-annotations:2.9.5 -> com.fasterxml.jackson.core:jackson-annotations:2.11.0 com.fasterxml.jackson.core:jackson-dataformat-yaml:2.9.5 -> com.fasterxml.jackson.core:jackson-dataformat-yaml:2.11.0 org.yaml:snakeyaml:1.18 -> org.yaml:snakeyaml:1.26 The following dependencies of Spoofax-Meta Core have been updated: org.apache.commons:commons-compress:1.16.1 -> org.apache.commons:commons-compress:1.20 Downloads \u00b6 Eclipse plugin \u00b6 Premade Eclipse installations \u00b6 With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit Update site \u00b6 Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.9/org.metaborg.spoofax.eclipse.updatesite-2.5.9-assembly.zip-unzip/ Eclipse update site archive IntelliJ plugin \u00b6 IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.9 IntelliJ update site archive Command-line utilities \u00b6 Sunshine JAR SPT testrunner JAR Core API \u00b6 Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.9 StrategoXT \u00b6 Stratego/XT distribution Stratego/XT JAR Maven artifacts \u00b6 Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.9 .","title":"Spoofax 2.5.9 (08-05-2020)"},{"location":"release/note/2.5.9/#spoofax-259-08-05-2020","text":"Spoofax 2.5.9 includes dependency upgrades.","title":"Spoofax 2.5.9 (08-05-2020)"},{"location":"release/note/2.5.9/#changes","text":"","title":"Changes"},{"location":"release/note/2.5.9/#overall","text":"The following dependencies of Spoofax Core have been updated to the latest version: com.netflix.rxjava:rxjava:0.20.7 -> io.reactivex.rxjava3:rxjava:3.0.2 New transitive dependency: org.reactivestreams:reactive-streams:1.0.3 org.apache.commons:commons-configuration2:2.2 -> org.apache.commons:commons-configuration2:2.7 New transitive dependency: org.apache.commons:commons-text:1.8 com.virtlink.commons:commons-configuration2-jackson:0.7.0 -> com.virtlink.commons:commons-configuration2-jackson:0.10.0 com.fasterxml.jackson.core:jackson-core:2.9.5 -> com.fasterxml.jackson.core:jackson-core:2.11.0 com.fasterxml.jackson.core:jackson-databind:2.9.5 -> com.fasterxml.jackson.core:jackson-databind:2.11.0 com.fasterxml.jackson.core:jackson-annotations:2.9.5 -> com.fasterxml.jackson.core:jackson-annotations:2.11.0 com.fasterxml.jackson.core:jackson-dataformat-yaml:2.9.5 -> com.fasterxml.jackson.core:jackson-dataformat-yaml:2.11.0 org.yaml:snakeyaml:1.18 -> org.yaml:snakeyaml:1.26 The following dependencies of Spoofax-Meta Core have been updated: org.apache.commons:commons-compress:1.16.1 -> org.apache.commons:commons-compress:1.20","title":"Overall"},{"location":"release/note/2.5.9/#downloads","text":"","title":"Downloads"},{"location":"release/note/2.5.9/#eclipse-plugin","text":"","title":"Eclipse plugin"},{"location":"release/note/2.5.9/#premade-eclipse-installations","text":"With embedded JRE: macOS 64-bit with embedded JVM Linux 64-bit with embedded JVM Windows 64-bit with embedded JVM Windows 32-bit with embedded JVM Without embedded JRE: macOS 64-bit Linux 64-bit Windows 64-bit Windows 32-bit","title":"Premade Eclipse installations"},{"location":"release/note/2.5.9/#update-site","text":"Eclipse update site: https://artifacts.metaborg.org/content/unzip/releases-unzipped/org/metaborg/org.metaborg.spoofax.eclipse.updatesite/2.5.9/org.metaborg.spoofax.eclipse.updatesite-2.5.9-assembly.zip-unzip/ Eclipse update site archive","title":"Update site"},{"location":"release/note/2.5.9/#intellij-plugin","text":"IntelliJ update site: https://artifacts.metaborg.org/service/local/artifact/maven/redirect?r=releases&g=org.metaborg&a=org.metaborg.intellij.dist&p=zip&v=2.5.9 IntelliJ update site archive","title":"IntelliJ plugin"},{"location":"release/note/2.5.9/#command-line-utilities","text":"Sunshine JAR SPT testrunner JAR","title":"Command-line utilities"},{"location":"release/note/2.5.9/#core-api","text":"Spoofax Core Uber JAR Spoofax Core uber Maven artifact: org.metaborg:org.metaborg.spoofax.core.uber:2.5.9","title":"Core API"},{"location":"release/note/2.5.9/#strategoxt","text":"Stratego/XT distribution Stratego/XT JAR","title":"StrategoXT"},{"location":"release/note/2.5.9/#maven-artifacts","text":"Maven artifacts can be found on our artifact server . The Maven version used for this release is 2.5.9 .","title":"Maven artifacts"},{"location":"release/note/vnext/","text":"Spoofax vNext \u00b6 These are the release notes for the upcoming version of Spoofax. Update the current documentation vnext.rst file instead Until we have migrated to this new documentation, update the vnext.rst in the current documentation repository . See the corresponding migration guide for migrating from Spoofax vPrev to Spoofax vNext. Changes \u00b6","title":"Spoofax vNext"},{"location":"release/note/vnext/#spoofax-vnext","text":"These are the release notes for the upcoming version of Spoofax. Update the current documentation vnext.rst file instead Until we have migrated to this new documentation, update the vnext.rst in the current documentation repository . See the corresponding migration guide for migrating from Spoofax vPrev to Spoofax vNext.","title":"Spoofax vNext"},{"location":"release/note/vnext/#changes","text":"","title":"Changes"},{"location":"support/","text":"Support \u00b6 Spoofax is an open source project. We welcome contributions from the community. Spoofax is developed by the TU Delft Programming Languages group . We do our best to make Spoofax usable by everyone and to help you when you encounter issues. However, our resources are limited, so please be patient. I have a question \u00b6 The search functionality of this documentation should bring you right to a relevant How-to or Reference page. If this doesn't answer your question, please join us in the SLDE/#spoofax-users Slack channel and ask your question there. To get access, drop us a line. Please do not make an issue with your question on the Github repository. I found a bug \u00b6 Search the Issues to ensure the bug has not been reported before. If the bug is new, open a new issue with a clear title and description . Please indicate: what you did (i.e., how to reproduce), what you expected to happen, what actually happened, what version of Spoofax and Eclipse you are using, and any logs, exceptions, and stack traces relevant to the bug. Try to include as much relevant information as you have. For example, a code sample or executable test case are very helpful. Getting the Spoofax and Eclipse version In Eclipse, go to the Spoofax (meta) menu, and click Report issue . Copy the displayed version information from this dialog into your issue. I wrote a patch with a cosmetic change \u00b6 Please do not submit pull requests for cosmetic changes, such as whitespace and formatting changes. I wrote a patch with a bug fix \u00b6 Thank you! Please open a GitHub pull request with the patch. I wrote a patch that adds a new feature or changes an existing one \u00b6 Please open an issue first , so we can discuss the change. I want to contribute to the documentation or test suite \u00b6 Thank you! Please open a GitHub pull request with the patch. Thank you for your contributions! \u2014 The Spoofax Team","title":"Support"},{"location":"support/#support","text":"Spoofax is an open source project. We welcome contributions from the community. Spoofax is developed by the TU Delft Programming Languages group . We do our best to make Spoofax usable by everyone and to help you when you encounter issues. However, our resources are limited, so please be patient.","title":"Support"},{"location":"support/#i-have-a-question","text":"The search functionality of this documentation should bring you right to a relevant How-to or Reference page. If this doesn't answer your question, please join us in the SLDE/#spoofax-users Slack channel and ask your question there. To get access, drop us a line. Please do not make an issue with your question on the Github repository.","title":"&nbsp; I have a question"},{"location":"support/#i-found-a-bug","text":"Search the Issues to ensure the bug has not been reported before. If the bug is new, open a new issue with a clear title and description . Please indicate: what you did (i.e., how to reproduce), what you expected to happen, what actually happened, what version of Spoofax and Eclipse you are using, and any logs, exceptions, and stack traces relevant to the bug. Try to include as much relevant information as you have. For example, a code sample or executable test case are very helpful. Getting the Spoofax and Eclipse version In Eclipse, go to the Spoofax (meta) menu, and click Report issue . Copy the displayed version information from this dialog into your issue.","title":"&nbsp; I found a bug"},{"location":"support/#i-wrote-a-patch-with-a-cosmetic-change","text":"Please do not submit pull requests for cosmetic changes, such as whitespace and formatting changes.","title":"&nbsp; I wrote a patch with a cosmetic change"},{"location":"support/#i-wrote-a-patch-with-a-bug-fix","text":"Thank you! Please open a GitHub pull request with the patch.","title":"&nbsp; I wrote a patch with a bug fix"},{"location":"support/#i-wrote-a-patch-that-adds-a-new-feature-or-changes-an-existing-one","text":"Please open an issue first , so we can discuss the change.","title":"&nbsp; I wrote a patch that adds a new feature or changes an existing one"},{"location":"support/#i-want-to-contribute-to-the-documentation-or-test-suite","text":"Thank you! Please open a GitHub pull request with the patch. Thank you for your contributions! \u2014 The Spoofax Team","title":"&nbsp; I want to contribute to the documentation or test suite"},{"location":"support/contributions/","text":"Contributions \u00b6 Spoofax and its components have been developed by many researchers, student, and developers supported by universities and funding agencies. Universities \u00b6 The main research effort on Spoofax and it predecessors (SDF2, Stratego, XT) was conducted at the following universities: University of Amsterdam Oregon Graduate Institute Utrecht University University of Bergen Delft University of Technology Funding \u00b6 Funding was provided by the following funding agencies and companies: The Netherlands Organisation for Scientific Research (NWO) Philips Research Oracle Labs Capes, Coordenaca de Bolsas, Brasil Tooling \u00b6 The following tools were used to develop and maintain Spoofax: \u2014 We use the YourKit Java profiler to diagnose and fix performance problems in Spoofax, provided free-of-charge by YourKit . Contributors \u00b6 Many people have contributed to Spoofax as bachelor student, master student, PhD student, postdoc, professor, or as an external contributor. This is an attempt at listing at least the main contributors to the various main projects. SDF2 Eelco Visser Jeroen Scheerder Mark van den Brand Jurgen Vinju Stratego/XT Eelco Visser Martin Bravenboer Karina Olmos Karl Trygve Kalleberg Anya Bagge Joost Visser Merijn de Jonge Rob Vermaas Eelco Dolstra Spoofax 1 Eelco Visser Lennart Kats Oskar van Rest Karl Trygve Kalleberg Rob Vermaas Guido Wachsmuth Maartje de Jonge Ricky Lindeman Sebastian Erdweg Tobi Vollebregt Spoofax 2 Eelco Visser Gabri\u00ebl Konat Eduardo Souza Vlad Vergu Volker Lanting Daniel A. A. Pelsmaeker Andrew Tolmach Pierre N\u00e9ron Hendrik van Antwerpen Volker Lanting Martijn Dwars Tobi Vollebregt Nathan Bruning Maarten Sijm Jasper Denkers Phil Misteli Daco Harkes JSGLR2 Eelco Visser Jasper Denkers Karl Trygve Lennart Kats Maarten Sijm Maartje de Jonge Gabri\u00ebl Konat Eduardo Souza SDF3 Eelco Visser Eduardo Souza Gabri\u00ebl Konat Daniel A. A. Pelsmaeker Jasper Denkers NaBL2 & Statix Eelco Visser Hendrik van Antwerpen Guido Wachsmuth Gabri\u00ebl Konat Jeff Smits Aron Zwaan Daniel A. A. Pelsmaeker Phil Misteli Dynsem Eelco Visser Vlag Vergu Casper Back Poulsen Hendrik van Antwerpen Gabri\u00ebl Konat Flowspec Eelco Visser Jeff Smits Hendrik van Antwerpen SPT Eelco Visser Gabri\u00ebl Konat Lennart Kats Volker Lanting Daniel A. A. Pelsmaeker Phil Misteli Stratego Eelco Visser Jeff Smits Gabri\u00ebl Konat Maartje de Jonge Oskar van Rest Nathan Bruning Spoofax 3 Eelco Visser Gabri\u00ebl Konat Daniel A. A. Pelsmaeker Aron Zwaan PIE Eelco Visser Gabri\u00ebl Konat Ivo Wilms","title":"Contributions"},{"location":"support/contributions/#contributions","text":"Spoofax and its components have been developed by many researchers, student, and developers supported by universities and funding agencies.","title":"Contributions"},{"location":"support/contributions/#universities","text":"The main research effort on Spoofax and it predecessors (SDF2, Stratego, XT) was conducted at the following universities: University of Amsterdam Oregon Graduate Institute Utrecht University University of Bergen Delft University of Technology","title":"Universities"},{"location":"support/contributions/#funding","text":"Funding was provided by the following funding agencies and companies: The Netherlands Organisation for Scientific Research (NWO) Philips Research Oracle Labs Capes, Coordenaca de Bolsas, Brasil","title":"Funding"},{"location":"support/contributions/#tooling","text":"The following tools were used to develop and maintain Spoofax: \u2014 We use the YourKit Java profiler to diagnose and fix performance problems in Spoofax, provided free-of-charge by YourKit .","title":"Tooling"},{"location":"support/contributions/#contributors","text":"Many people have contributed to Spoofax as bachelor student, master student, PhD student, postdoc, professor, or as an external contributor. This is an attempt at listing at least the main contributors to the various main projects. SDF2 Eelco Visser Jeroen Scheerder Mark van den Brand Jurgen Vinju Stratego/XT Eelco Visser Martin Bravenboer Karina Olmos Karl Trygve Kalleberg Anya Bagge Joost Visser Merijn de Jonge Rob Vermaas Eelco Dolstra Spoofax 1 Eelco Visser Lennart Kats Oskar van Rest Karl Trygve Kalleberg Rob Vermaas Guido Wachsmuth Maartje de Jonge Ricky Lindeman Sebastian Erdweg Tobi Vollebregt Spoofax 2 Eelco Visser Gabri\u00ebl Konat Eduardo Souza Vlad Vergu Volker Lanting Daniel A. A. Pelsmaeker Andrew Tolmach Pierre N\u00e9ron Hendrik van Antwerpen Volker Lanting Martijn Dwars Tobi Vollebregt Nathan Bruning Maarten Sijm Jasper Denkers Phil Misteli Daco Harkes JSGLR2 Eelco Visser Jasper Denkers Karl Trygve Lennart Kats Maarten Sijm Maartje de Jonge Gabri\u00ebl Konat Eduardo Souza SDF3 Eelco Visser Eduardo Souza Gabri\u00ebl Konat Daniel A. A. Pelsmaeker Jasper Denkers NaBL2 & Statix Eelco Visser Hendrik van Antwerpen Guido Wachsmuth Gabri\u00ebl Konat Jeff Smits Aron Zwaan Daniel A. A. Pelsmaeker Phil Misteli Dynsem Eelco Visser Vlag Vergu Casper Back Poulsen Hendrik van Antwerpen Gabri\u00ebl Konat Flowspec Eelco Visser Jeff Smits Hendrik van Antwerpen SPT Eelco Visser Gabri\u00ebl Konat Lennart Kats Volker Lanting Daniel A. A. Pelsmaeker Phil Misteli Stratego Eelco Visser Jeff Smits Gabri\u00ebl Konat Maartje de Jonge Oskar van Rest Nathan Bruning Spoofax 3 Eelco Visser Gabri\u00ebl Konat Daniel A. A. Pelsmaeker Aron Zwaan PIE Eelco Visser Gabri\u00ebl Konat Ivo Wilms","title":"Contributors"},{"location":"tutorials/","text":"Tutorials \u00b6 This page lists tutorials that take you step-by-step through a project to learn a variety of concepts and aspects of Spoofax in a specific scope. For guides on achieving specific tasks, see the How To's section. For the Spoofax language reference, see the References section. No tutorials yet.","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"This page lists tutorials that take you step-by-step through a project to learn a variety of concepts and aspects of Spoofax in a specific scope. For guides on achieving specific tasks, see the How To's section. For the Spoofax language reference, see the References section. No tutorials yet.","title":"Tutorials"}]}